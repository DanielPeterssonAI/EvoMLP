{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using EvoMLP to predict fuel consumption\n",
    "\n",
    "A simple example with the *mpg* dataset from seaborn. It's a small dataset with about 400 rows of data which can be loaded using *seaborn.load_dataset(\"mpg\")*. \n",
    "\n",
    "There where some missing values which are filled in using linear regression, and the categeorical columns are one-hot encoded with drop first. The result is 9 features and 1 column to predict on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin_europe</th>\n",
       "      <th>origin_japan</th>\n",
       "      <th>origin_usa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0   \n",
       "1  15.0          8         350.0       165.0    3693          11.5   \n",
       "2  18.0          8         318.0       150.0    3436          11.0   \n",
       "3  16.0          8         304.0       150.0    3433          12.0   \n",
       "4  17.0          8         302.0       140.0    3449          10.5   \n",
       "\n",
       "   model_year  origin_europe  origin_japan  origin_usa  \n",
       "0          70              0             0           1  \n",
       "1          70              0             0           1  \n",
       "2          70              0             0           1  \n",
       "3          70              0             0           1  \n",
       "4          70              0             0           1  "
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error\n",
    "\n",
    "df = pickle.load(open(\"sns_mpg.p\", \"rb\"))\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's split the dataset into train, validation and test parts to be able to evaluate the model(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((318, 9), (40, 9), (40, 9), (318,), (40,), (40,))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X, y = df.drop([\"mpg\"], axis = 1).values, df[\"mpg\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)\n",
    "X_train.shape, X_val.shape, X_test.shape, y_train.shape, y_val.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Standardizing features using Sklearn's StandardScaler, z = (x - u) / s. (Removing the mean and scaling to variance)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_val = scaler.transform(X_val)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing the EvoMLPRegressor from evolutionary_algos.py and instantiating a regressor with 48 MLP nets with 16 hidden layers. There are some parameters to tune and this is just a basic example. The hidden layers will by default have a ReLU activation function and as we are using the regressor, the output is linear.\n",
    "\n",
    "The regressor is then fitted over 1000 epochs and validation data is used to be able to catch overfitting of the nets.\n",
    "\n",
    "Execution time on this laptop is about 0.6s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 10.225950767369405 - val_loss: 10.574316085557683 - sigma: 1.0149800099966677\n",
      "Epoch 1 - loss: 8.067201041900564 - val_loss: 6.231929645400766 - sigma: 0.9567163976175883\n",
      "Epoch 2 - loss: 5.134156479807699 - val_loss: 4.377657379558039 - sigma: 0.9018451270734162\n",
      "Epoch 7 - loss: 3.782244143093803 - val_loss: 3.694006353273108 - sigma: 0.6718475300332143\n",
      "Epoch 11 - loss: 3.31728208347896 - val_loss: 2.692008128877852 - sigma: 0.5315634133302428\n",
      "Epoch 22 - loss: 3.0465879835330982 - val_loss: 2.9249523960640675 - sigma: 0.2816295352492394\n",
      "Epoch 23 - loss: 2.9071803614104907 - val_loss: 2.341124239257693 - sigma: 0.26605403816112994\n",
      "Epoch 25 - loss: 2.8630426812432823 - val_loss: 2.457668605641282 - sigma: 0.23756843930684998\n",
      "Epoch 31 - loss: 2.5941978795490885 - val_loss: 2.3344173703077713 - sigma: 0.1700008717772989\n",
      "Epoch 32 - loss: 2.4864415728880997 - val_loss: 2.0395313854273383 - sigma: 0.160917010159469\n",
      "Epoch 34 - loss: 2.4626487186718857 - val_loss: 2.0127506082696485 - sigma: 0.1443024439436812\n",
      "Epoch 35 - loss: 2.4250200211769757 - val_loss: 2.084863190071336 - sigma: 0.1367120308707217\n",
      "Epoch 37 - loss: 2.3966674391407268 - val_loss: 1.8638869358933512 - sigma: 0.12282848611689678\n",
      "Epoch 38 - loss: 2.3794171461396294 - val_loss: 2.300396760484195 - sigma: 0.11648548301814013\n",
      "Epoch 39 - loss: 2.354476355129356 - val_loss: 1.8316593422025569 - sigma: 0.11051081788071251\n",
      "Epoch 40 - loss: 2.164149401930084 - val_loss: 1.9609896858288018 - sigma: 0.10488303846122021\n",
      "Epoch 45 - loss: 2.1560409486778034 - val_loss: 2.0713894668422137 - sigma: 0.08128010195183459\n",
      "Epoch 46 - loss: 2.1299127636213493 - val_loss: 1.9633807744443224 - sigma: 0.07734824570893825\n",
      "Epoch 50 - loss: 2.122374661224164 - val_loss: 1.8938827452583709 - sigma: 0.06377103502321489\n",
      "Epoch 51 - loss: 2.1143007556933116 - val_loss: 1.8078467395017888 - sigma: 0.06085351720159262\n",
      "Epoch 52 - loss: 2.069914674093977 - val_loss: 1.992634358173938 - sigma: 0.058104838830590345\n",
      "Epoch 56 - loss: 2.0605520124496337 - val_loss: 1.7657668675301192 - sigma: 0.04861025730750062\n",
      "Epoch 57 - loss: 2.0507265942943493 - val_loss: 1.8909383951503123 - sigma: 0.046569250198638966\n",
      "Epoch 58 - loss: 2.041989104860468 - val_loss: 1.8547286888002426 - sigma: 0.04464603786887162\n",
      "Epoch 59 - loss: 2.040253706694476 - val_loss: 1.8562822958525254 - sigma: 0.04283376060158238\n",
      "Epoch 60 - loss: 2.0390087987128243 - val_loss: 1.8155882029984078 - sigma: 0.04112595821395644\n",
      "Epoch 61 - loss: 2.0152297128557546 - val_loss: 1.874633525898826 - sigma: 0.03951654678667769\n",
      "Epoch 68 - loss: 2.00633589743562 - val_loss: 1.8443260603867124 - sigma: 0.030564020955782083\n",
      "Epoch 71 - loss: 1.9985158914218206 - val_loss: 1.8012650280300015 - sigma: 0.027724218162353147\n",
      "Epoch 72 - loss: 1.9969242644947192 - val_loss: 1.8503341451299682 - sigma: 0.0268835893615572\n",
      "Epoch 73 - loss: 1.981693482019793 - val_loss: 1.8593857384251247 - sigma: 0.026090857027627458\n",
      "Epoch 74 - loss: 1.9811803247698856 - val_loss: 1.8593162484036263 - sigma: 0.025343232560161785\n",
      "Epoch 75 - loss: 1.9772241831577084 - val_loss: 1.8788245035259898 - sigma: 0.0246380897759388\n",
      "Epoch 76 - loss: 1.9771413514432394 - val_loss: 1.8414115452182152 - sigma: 0.023972955449150735\n",
      "Epoch 77 - loss: 1.975237395240514 - val_loss: 1.8332442255490164 - sigma: 0.023345500402607598\n",
      "Epoch 79 - loss: 1.9653339741001057 - val_loss: 1.7744847908919688 - sigma: 0.02219498183375366\n",
      "Epoch 80 - loss: 1.962508757753659 - val_loss: 1.8270673413870504 - sigma: 0.021667907105750568\n",
      "Epoch 81 - loss: 1.9613314036487217 - val_loss: 1.8454092242340643 - sigma: 0.02117047479788619\n",
      "Epoch 82 - loss: 1.9554128878014925 - val_loss: 1.7926224160176445 - sigma: 0.020700959483447218\n",
      "Epoch 83 - loss: 1.9520231867517233 - val_loss: 1.8085914324352068 - sigma: 0.020257736229801503\n",
      "Epoch 84 - loss: 1.9494734038250703 - val_loss: 1.8535850736688328 - sigma: 0.01983927474525772\n",
      "Epoch 86 - loss: 1.9484935848987668 - val_loss: 1.849453642661977 - sigma: 0.01907095636907513\n",
      "Epoch 89 - loss: 1.947727408657923 - val_loss: 1.8377128639643452 - sigma: 0.018070790279465426\n",
      "Epoch 91 - loss: 1.9409635564115837 - val_loss: 1.8351589788132248 - sigma: 0.017492299664344478\n",
      "Epoch 93 - loss: 1.93998430861885 - val_loss: 1.8097427018765462 - sigma: 0.016975176083426552\n",
      "Epoch 95 - loss: 1.9385850094199844 - val_loss: 1.8093334360379338 - sigma: 0.01651248734083062\n",
      "Epoch 96 - loss: 1.9340645188286198 - val_loss: 1.872823436420699 - sigma: 0.016299606588159467\n",
      "Epoch 97 - loss: 1.9326067005306578 - val_loss: 1.8315223220884989 - sigma: 0.01609808522091425\n",
      "Epoch 98 - loss: 1.9274668287374137 - val_loss: 1.8062939574111876 - sigma: 0.015907262666158238\n",
      "Epoch 100 - loss: 1.9208535986545758 - val_loss: 1.891515186006155 - sigma: 0.015555261827229097\n",
      "Epoch 101 - loss: 1.9205636143669922 - val_loss: 1.8436657441145639 - sigma: 0.015392945930114447\n",
      "Epoch 102 - loss: 1.9163452877777796 - val_loss: 1.8181569408619562 - sigma: 0.015239049521915\n",
      "Epoch 104 - loss: 1.9138590106230198 - val_loss: 1.8418386498690587 - sigma: 0.01495458628127652\n",
      "Epoch 107 - loss: 1.9136893557749264 - val_loss: 1.857750255352989 - sigma: 0.014579695965060568\n",
      "Epoch 108 - loss: 1.9092384114092875 - val_loss: 1.8179748101393343 - sigma: 0.014466981591096174\n",
      "Epoch 109 - loss: 1.9083823733337455 - val_loss: 1.8499721704728345 - sigma: 0.01435980494805003\n",
      "Epoch 110 - loss: 1.9078843066029885 - val_loss: 1.8285128766788852 - sigma: 0.014257844524954445\n",
      "Epoch 111 - loss: 1.9058395634119487 - val_loss: 1.8260323297609553 - sigma: 0.014160797535785368\n",
      "Epoch 113 - loss: 1.90404181742681 - val_loss: 1.8146948220983012 - sigma: 0.01398031985972263\n",
      "Epoch 114 - loss: 1.8988495798442429 - val_loss: 1.8214805386904174 - sigma: 0.013896367723933404\n",
      "Epoch 116 - loss: 1.8983910743796635 - val_loss: 1.8471225230967427 - sigma: 0.013739845121958907\n",
      "Epoch 118 - loss: 1.8963258537014338 - val_loss: 1.8225477308901687 - sigma: 0.013597067273514165\n",
      "Epoch 119 - loss: 1.8919058527766421 - val_loss: 1.799167074046231 - sigma: 0.013530342258038402\n",
      "Epoch 123 - loss: 1.8887324169522752 - val_loss: 1.8227138937781917 - sigma: 0.013290532158954487\n",
      "Epoch 125 - loss: 1.8881845623478533 - val_loss: 1.846793275544894 - sigma: 0.013184781427693892\n",
      "Epoch 126 - loss: 1.8861896558051374 - val_loss: 1.8336951449182493 - sigma: 0.013134979234808072\n",
      "Epoch 127 - loss: 1.8840543684003526 - val_loss: 1.8035833992364847 - sigma: 0.013087068861824992\n",
      "Epoch 128 - loss: 1.8819431229553176 - val_loss: 1.812937099816379 - sigma: 0.013040941130599787\n",
      "Epoch 129 - loss: 1.8817754592600469 - val_loss: 1.8103836994687097 - sigma: 0.012996493220912597\n",
      "Epoch 130 - loss: 1.8797895609676152 - val_loss: 1.8100269167198448 - sigma: 0.012953628300160834\n",
      "Epoch 131 - loss: 1.8791440819671956 - val_loss: 1.7759320819890103 - sigma: 0.012912255174619565\n",
      "Epoch 133 - loss: 1.8760584063412444 - val_loss: 1.8184752116669132 - sigma: 0.012833645777219278\n",
      "Epoch 134 - loss: 1.8730049832875528 - val_loss: 1.8156966563094106 - sigma: 0.012796252450978774\n",
      "Epoch 136 - loss: 1.8726286705994715 - val_loss: 1.7948709119016335 - sigma: 0.012724929601528148\n",
      "Epoch 137 - loss: 1.8672030732594573 - val_loss: 1.7852930218291867 - sigma: 0.012690868893223689\n",
      "Epoch 141 - loss: 1.8664333724622844 - val_loss: 1.7602267575868744 - sigma: 0.012563938129116351\n",
      "Epoch 142 - loss: 1.8663456863147214 - val_loss: 1.7844724567524537 - sigma: 0.012534275089817309\n",
      "Epoch 143 - loss: 1.863322658503556 - val_loss: 1.7777674538926456 - sigma: 0.012505346905775773\n",
      "Epoch 146 - loss: 1.8590164961235458 - val_loss: 1.7940963553334321 - sigma: 0.012422565430662611\n",
      "Epoch 147 - loss: 1.858048985977946 - val_loss: 1.7967837072847133 - sigma: 0.01239618222468415\n",
      "Epoch 148 - loss: 1.8572939391652172 - val_loss: 1.833886635138097 - sigma: 0.012370347796275548\n",
      "Epoch 149 - loss: 1.855341586230868 - val_loss: 1.7631453070747036 - sigma: 0.012345031170014972\n",
      "Epoch 151 - loss: 1.853185003572165 - val_loss: 1.7618925977927244 - sigma: 0.01229583633290291\n",
      "Epoch 152 - loss: 1.8521531499890351 - val_loss: 1.7751418018194731 - sigma: 0.012271904772916465\n",
      "Epoch 156 - loss: 1.8498866155373577 - val_loss: 1.8073064069463947 - sigma: 0.012180064746761149\n",
      "Epoch 158 - loss: 1.8459559484109826 - val_loss: 1.792663481821173 - sigma: 0.01213618644109028\n",
      "Epoch 160 - loss: 1.8455661293502925 - val_loss: 1.7887850093867166 - sigma: 0.012093476145481894\n",
      "Epoch 161 - loss: 1.8449464653928946 - val_loss: 1.772758260944432 - sigma: 0.012072519521506555\n",
      "Epoch 163 - loss: 1.8428378550128524 - val_loss: 1.7623144007838747 - sigma: 0.012031332235937933\n",
      "Epoch 164 - loss: 1.8421253622733202 - val_loss: 1.7771065072389263 - sigma: 0.012011075591993927\n",
      "Epoch 167 - loss: 1.8416975738056112 - val_loss: 1.77657123635957 - sigma: 0.011951513173047502\n",
      "Epoch 168 - loss: 1.8407032019622802 - val_loss: 1.7635493661980182 - sigma: 0.011932026404663959\n",
      "Epoch 170 - loss: 1.8395922166788023 - val_loss: 1.7658232718314502 - sigma: 0.011893546970264289\n",
      "Epoch 173 - loss: 1.839334100073522 - val_loss: 1.7629334313041318 - sigma: 0.011836938617983175\n",
      "Epoch 174 - loss: 1.8374011410867017 - val_loss: 1.756469358565333 - sigma: 0.01181833548856445\n",
      "Epoch 178 - loss: 1.8361971407733186 - val_loss: 1.7553736617468547 - sigma: 0.011745082848414063\n",
      "Epoch 180 - loss: 1.8353250318956473 - val_loss: 1.7543474361513394 - sigma: 0.01170907482434736\n",
      "Epoch 181 - loss: 1.8344615110766096 - val_loss: 1.7597229271581156 - sigma: 0.011691208172598914\n",
      "Epoch 185 - loss: 1.8339529298871078 - val_loss: 1.772979732235531 - sigma: 0.01162055997123741\n",
      "Epoch 186 - loss: 1.8336852940782593 - val_loss: 1.7725975004485666 - sigma: 0.011603084015060798\n",
      "Epoch 187 - loss: 1.831545746884658 - val_loss: 1.7739306592094715 - sigma: 0.011585675831355632\n",
      "Epoch 189 - loss: 1.8305512263543104 - val_loss: 1.7319177624040183 - sigma: 0.01155105096420901\n",
      "Epoch 190 - loss: 1.8304597374805742 - val_loss: 1.7587209738954432 - sigma: 0.011533828796198816\n",
      "Epoch 191 - loss: 1.8291529738404282 - val_loss: 1.753692275457324 - sigma: 0.01151666341188575\n",
      "Epoch 193 - loss: 1.8263601204086055 - val_loss: 1.7349714971185441 - sigma: 0.011482493685088939\n",
      "Epoch 195 - loss: 1.8246594953171393 - val_loss: 1.72919544268888 - sigma: 0.011448524483978038\n",
      "Epoch 198 - loss: 1.8230256147941326 - val_loss: 1.7205608019632208 - sigma: 0.011397913548829455\n",
      "Epoch 199 - loss: 1.8223636482604624 - val_loss: 1.7136727830572025 - sigma: 0.011381127949211567\n",
      "Epoch 201 - loss: 1.8221358053055303 - val_loss: 1.7568793702413104 - sigma: 0.011347674871709705\n",
      "Epoch 203 - loss: 1.8208353777704436 - val_loss: 1.730561767506348 - sigma: 0.011314370464722027\n",
      "Epoch 204 - loss: 1.8180034763094333 - val_loss: 1.739384935341444 - sigma: 0.011297770984638076\n",
      "Epoch 207 - loss: 1.8171863437021938 - val_loss: 1.7435811840413589 - sigma: 0.011248170521062415\n",
      "Epoch 208 - loss: 1.8148290927325175 - val_loss: 1.7257271100477916 - sigma: 0.011231699790229343\n",
      "Epoch 212 - loss: 1.809868719710365 - val_loss: 1.7395650514553984 - sigma: 0.01116610795425013\n",
      "Epoch 216 - loss: 1.8098590103731613 - val_loss: 1.7254287184443597 - sigma: 0.011100947553850165\n",
      "Epoch 218 - loss: 1.8090392490769744 - val_loss: 1.7340696815647973 - sigma: 0.011068516964878232\n",
      "Epoch 219 - loss: 1.808001755627995 - val_loss: 1.7278107979964843 - sigma: 0.011052337262740531\n",
      "Epoch 225 - loss: 1.8078192963945319 - val_loss: 1.752281924712662 - sigma: 0.010955730317052574\n",
      "Epoch 226 - loss: 1.805377387509425 - val_loss: 1.7541111525731377 - sigma: 0.010939704235713143\n",
      "Epoch 229 - loss: 1.803464464493747 - val_loss: 1.7239191167920758 - sigma: 0.010891748342801487\n",
      "Epoch 234 - loss: 1.8015882145217406 - val_loss: 1.7084799160598423 - sigma: 0.01081221429523358\n",
      "Epoch 236 - loss: 1.801548479709217 - val_loss: 1.7089879778024524 - sigma: 0.010780532892096488\n",
      "Epoch 238 - loss: 1.7987318702396315 - val_loss: 1.7079209398636253 - sigma: 0.010748924806159707\n",
      "Epoch 239 - loss: 1.797611414297989 - val_loss: 1.7330037189690273 - sigma: 0.010733147852029652\n",
      "Epoch 242 - loss: 1.793853404033836 - val_loss: 1.719068166781883 - sigma: 0.010685923598810999\n",
      "Epoch 248 - loss: 1.7934572460840508 - val_loss: 1.708348877780916 - sigma: 0.010591943629809612\n",
      "Epoch 249 - loss: 1.7920983202914222 - val_loss: 1.7050244533650663 - sigma: 0.010576339778288526\n",
      "Epoch 250 - loss: 1.7915237997393103 - val_loss: 1.7032709901857657 - sigma: 0.010560752670293076\n",
      "Epoch 254 - loss: 1.7913172669509356 - val_loss: 1.7096397350765262 - sigma: 0.010498570060474976\n",
      "Epoch 256 - loss: 1.7902685111572716 - val_loss: 1.7231242830143951 - sigma: 0.01046757722232001\n",
      "Epoch 260 - loss: 1.7900216475661879 - val_loss: 1.6899757525977226 - sigma: 0.010405785896536504\n",
      "Epoch 264 - loss: 1.7878082299620937 - val_loss: 1.711802753146933 - sigma: 0.01034425075843958\n",
      "Epoch 265 - loss: 1.7867035420825357 - val_loss: 1.7050845452676213 - sigma: 0.010328906634823295\n",
      "Epoch 270 - loss: 1.7866372769007473 - val_loss: 1.7218104626754749 - sigma: 0.010252421848394073\n",
      "Epoch 272 - loss: 1.786314532553289 - val_loss: 1.6757939898966536 - sigma: 0.010221937277149358\n",
      "Epoch 273 - loss: 1.7843965090921563 - val_loss: 1.7051017446987067 - sigma: 0.010206718281500377\n",
      "Epoch 278 - loss: 1.7835342793326276 - val_loss: 1.708006753635474 - sigma: 0.010130854939101252\n",
      "Epoch 279 - loss: 1.7835332568053262 - val_loss: 1.7102305856888862 - sigma: 0.010115728391319285\n",
      "Epoch 280 - loss: 1.7814003557561353 - val_loss: 1.694312282128143 - sigma: 0.01010061715215104\n",
      "Epoch 283 - loss: 1.781148445453053 - val_loss: 1.6710518633585187 - sigma: 0.010055375026183392\n",
      "Epoch 285 - loss: 1.7799331632899675 - val_loss: 1.6933788469220439 - sigma: 0.010025289686073488\n",
      "Epoch 290 - loss: 1.7796300005842 - val_loss: 1.6970604901581332 - sigma: 0.009950341241415252\n",
      "Epoch 291 - loss: 1.777928115250754 - val_loss: 1.7058482424654513 - sigma: 0.0099353967882752\n",
      "Epoch 293 - loss: 1.7764492240500878 - val_loss: 1.708587214780018 - sigma: 0.009905552949422953\n",
      "Epoch 295 - loss: 1.7756592466653442 - val_loss: 1.6913839244237674 - sigma: 0.009875769066142214\n",
      "Epoch 297 - loss: 1.7721270815972905 - val_loss: 1.6934413132143298 - sigma: 0.009846044981598362\n",
      "Epoch 306 - loss: 1.7717304798208284 - val_loss: 1.689433746160395 - sigma: 0.009713022615551367\n",
      "Epoch 308 - loss: 1.7713643331897522 - val_loss: 1.67262858991386 - sigma: 0.009683624799418296\n",
      "Epoch 309 - loss: 1.7702359612227805 - val_loss: 1.6765388094312434 - sigma: 0.00966894797595305\n",
      "Epoch 310 - loss: 1.7702017187490162 - val_loss: 1.6897979648958472 - sigma: 0.009654285853312223\n",
      "Epoch 312 - loss: 1.7692112990078563 - val_loss: 1.6810574714089064 - sigma: 0.00962500564455202\n",
      "Epoch 316 - loss: 1.7690629840346965 - val_loss: 1.6883529965265325 - sigma: 0.009566620917989028\n",
      "Epoch 317 - loss: 1.7684734891571205 - val_loss: 1.6612992896049097 - sigma: 0.00955206124505246\n",
      "Epoch 318 - loss: 1.7676345908832127 - val_loss: 1.6650976939125104 - sigma: 0.00953751614390083\n",
      "Epoch 323 - loss: 1.7671913487649846 - val_loss: 1.6890389805005586 - sigma: 0.009465008668403998\n",
      "Epoch 324 - loss: 1.765778476361636 - val_loss: 1.6740891928267216 - sigma: 0.009450550671110869\n",
      "Epoch 330 - loss: 1.7647824733716522 - val_loss: 1.6838488715856426 - sigma: 0.009364105891108037\n",
      "Epoch 333 - loss: 1.7646498669157762 - val_loss: 1.6607563310498121 - sigma: 0.009321077741265558\n",
      "Epoch 334 - loss: 1.76392007724184 - val_loss: 1.6642753480417896 - sigma: 0.009306763701651897\n",
      "Epoch 336 - loss: 1.762481680562772 - val_loss: 1.6440191563356497 - sigma: 0.009278178549337704\n",
      "Epoch 340 - loss: 1.7613920024318221 - val_loss: 1.6695048350373625 - sigma: 0.009221179541688598\n",
      "Epoch 342 - loss: 1.761350574259244 - val_loss: 1.670198435359017 - sigma: 0.009192765452969433\n",
      "Epoch 343 - loss: 1.7608268830424278 - val_loss: 1.6674383807769888 - sigma: 0.009178579711595412\n",
      "Epoch 349 - loss: 1.760138332300194 - val_loss: 1.661610935658905 - sigma: 0.009093762597082463\n",
      "Epoch 350 - loss: 1.7597304011665813 - val_loss: 1.6511478258546368 - sigma: 0.009079675833067962\n",
      "Epoch 355 - loss: 1.759200615537811 - val_loss: 1.6816738496305992 - sigma: 0.00900945296470638\n",
      "Epoch 358 - loss: 1.7591809173949455 - val_loss: 1.673776019621792 - sigma: 0.008967487494771722\n",
      "Epoch 359 - loss: 1.7580410752835052 - val_loss: 1.675609044010668 - sigma: 0.008953526961917974\n",
      "Epoch 363 - loss: 1.7574996686297697 - val_loss: 1.671407980705866 - sigma: 0.00889782424135367\n",
      "Epoch 364 - loss: 1.7564536945640268 - val_loss: 1.677781110891991 - sigma: 0.008883933343873879\n",
      "Epoch 368 - loss: 1.7561073052336194 - val_loss: 1.660616725607208 - sigma: 0.008828508465685175\n",
      "Epoch 369 - loss: 1.7553404733718625 - val_loss: 1.6580706926000346 - sigma: 0.008814686854476178\n",
      "Epoch 377 - loss: 1.7551037742935645 - val_loss: 1.651166747199482 - sigma: 0.008704610162884962\n",
      "Epoch 378 - loss: 1.7542256593912096 - val_loss: 1.6418767671362153 - sigma: 0.00869091239418176\n",
      "Epoch 382 - loss: 1.754157984379374 - val_loss: 1.6657033157602972 - sigma: 0.00863625809651737\n",
      "Epoch 387 - loss: 1.753722478358405 - val_loss: 1.6463928710225624 - sigma: 0.00856824694735937\n",
      "Epoch 394 - loss: 1.7537016408790556 - val_loss: 1.6583695386654056 - sigma: 0.008473600838902697\n",
      "Epoch 395 - loss: 1.7535698019817572 - val_loss: 1.6460730985142615 - sigma: 0.008460133969532263\n",
      "Epoch 397 - loss: 1.7528072554672012 - val_loss: 1.6467977544339614 - sigma: 0.008433240598281256\n",
      "Epoch 400 - loss: 1.7516035861138717 - val_loss: 1.641193927027238 - sigma: 0.008393001258379166\n",
      "Epoch 405 - loss: 1.7504854136477268 - val_loss: 1.6350513541636098 - sigma: 0.008326203376366451\n",
      "Epoch 407 - loss: 1.7490262451956666 - val_loss: 1.6310767259972867 - sigma: 0.00829957760116495\n",
      "Epoch 414 - loss: 1.7480392178531385 - val_loss: 1.6409443146828202 - sigma: 0.008206805630338838\n",
      "Epoch 416 - loss: 1.747545975322842 - val_loss: 1.644584891956192 - sigma: 0.008180418413285026\n",
      "Epoch 423 - loss: 1.7462315674942999 - val_loss: 1.6209470360390128 - sigma: 0.008088477647639786\n",
      "Epoch 427 - loss: 1.7454769093710973 - val_loss: 1.6266494266190805 - sigma: 0.008036228303417936\n",
      "Epoch 428 - loss: 1.7445695197678277 - val_loss: 1.6217481679585444 - sigma: 0.008023198590630422\n",
      "Epoch 433 - loss: 1.744043241840319 - val_loss: 1.6312859775343393 - sigma: 0.007958245114899754\n",
      "Epoch 438 - loss: 1.7440375370435384 - val_loss: 1.6236952532570825 - sigma: 0.007893615596442737\n",
      "Epoch 439 - loss: 1.7427559306714384 - val_loss: 1.632731338167829 - sigma: 0.007880728425285493\n",
      "Epoch 440 - loss: 1.7425072936043546 - val_loss: 1.6328151695618538 - sigma: 0.007867854134870792\n",
      "Epoch 441 - loss: 1.7421779239994988 - val_loss: 1.6335543542050572 - sigma: 0.007854992712323584\n",
      "Epoch 442 - loss: 1.740483990655949 - val_loss: 1.6159590956617556 - sigma: 0.00784214414478173\n",
      "Epoch 450 - loss: 1.7398757043015258 - val_loss: 1.627783119928972 - sigma: 0.00773981684543185\n",
      "Epoch 452 - loss: 1.7387963490260931 - val_loss: 1.6256318686753572 - sigma: 0.007714362674188749\n",
      "Epoch 455 - loss: 1.737825166010828 - val_loss: 1.6232212573461688 - sigma: 0.007676276743357878\n",
      "Epoch 465 - loss: 1.737537277108285 - val_loss: 1.6178551024710692 - sigma: 0.007550145681707893\n",
      "Epoch 466 - loss: 1.7364330841175506 - val_loss: 1.6207025116141676 - sigma: 0.007537601808964293\n",
      "Epoch 468 - loss: 1.7363864639515614 - val_loss: 1.6226826273814914 - sigma: 0.007512551663761906\n",
      "Epoch 473 - loss: 1.735572987987485 - val_loss: 1.6261835404881624 - sigma: 0.007450145051824645\n",
      "Epoch 475 - loss: 1.734862027674063 - val_loss: 1.604990343991687 - sigma: 0.007425269645366887\n",
      "Epoch 487 - loss: 1.7336097116445224 - val_loss: 1.5967272689736103 - sigma: 0.007277057461059614\n",
      "Epoch 494 - loss: 1.733174049540516 - val_loss: 1.607595260817666 - sigma: 0.007191418146059735\n",
      "Epoch 499 - loss: 1.7331712080776738 - val_loss: 1.6219623360700361 - sigma: 0.0071306131943516\n",
      "Epoch 500 - loss: 1.7328474660105657 - val_loss: 1.612592475285607 - sigma: 0.00711848864443692\n",
      "Epoch 501 - loss: 1.7324617761689773 - val_loss: 1.5901280749977704 - sigma: 0.007106376213012229\n",
      "Epoch 506 - loss: 1.7307148095480729 - val_loss: 1.6076953916469983 - sigma: 0.007045995409726308\n",
      "Epoch 509 - loss: 1.7303656439429618 - val_loss: 1.6056026785339128 - sigma: 0.007009911576299609\n",
      "Epoch 510 - loss: 1.7299642816753567 - val_loss: 1.6004200020938089 - sigma: 0.006997907667674838\n",
      "Epoch 513 - loss: 1.729183106801899 - val_loss: 1.6057377940538693 - sigma: 0.006961967881297932\n",
      "Epoch 515 - loss: 1.7284166076619716 - val_loss: 1.605919601563667 - sigma: 0.0069380678535250345\n",
      "Epoch 518 - loss: 1.7281417304649345 - val_loss: 1.6155423728044842 - sigma: 0.006902307317582637\n",
      "Epoch 521 - loss: 1.728054824399615 - val_loss: 1.6092693921839185 - sigma: 0.006866653902487462\n",
      "Epoch 525 - loss: 1.7277538954820202 - val_loss: 1.6037389384449359 - sigma: 0.006819282093652037\n",
      "Epoch 530 - loss: 1.7276414686332073 - val_loss: 1.594541580845314 - sigma: 0.006760333178277105\n",
      "Epoch 535 - loss: 1.7273637285493812 - val_loss: 1.6021268663345523 - sigma: 0.006701678271845542\n",
      "Epoch 537 - loss: 1.7267195061744056 - val_loss: 1.6051566202121077 - sigma: 0.006678298303062689\n",
      "Epoch 546 - loss: 1.725855079424207 - val_loss: 1.5998514823703038 - sigma: 0.006573665173685841\n",
      "Epoch 554 - loss: 1.725728589840386 - val_loss: 1.5947825283991839 - sigma: 0.006481445223932365\n",
      "Epoch 556 - loss: 1.725248124319345 - val_loss: 1.6137469410091214 - sigma: 0.006458505281073602\n",
      "Epoch 557 - loss: 1.7237976090248375 - val_loss: 1.584795559332735 - sigma: 0.00644705250313571\n",
      "Epoch 559 - loss: 1.7229342117959086 - val_loss: 1.5861176676337074 - sigma: 0.006424181276978998\n",
      "Epoch 563 - loss: 1.721494804132711 - val_loss: 1.5877739410920753 - sigma: 0.006378575823584559\n",
      "Epoch 570 - loss: 1.720835147562944 - val_loss: 1.5942126985955347 - sigma: 0.006299203918587921\n",
      "Epoch 575 - loss: 1.720610995621314 - val_loss: 1.5887675982586607 - sigma: 0.006242848903937483\n",
      "Epoch 579 - loss: 1.7202389826905289 - val_loss: 1.5811734058276432 - sigma: 0.006197967331308854\n",
      "Epoch 583 - loss: 1.7194333288836212 - val_loss: 1.569684393842062 - sigma: 0.006153264926396465\n",
      "Epoch 588 - loss: 1.7184179077501136 - val_loss: 1.5571849451302926 - sigma: 0.006097637785506371\n",
      "Epoch 589 - loss: 1.7184115875179657 - val_loss: 1.56775015741668 - sigma: 0.006086545694690587\n",
      "Epoch 592 - loss: 1.7173626269116056 - val_loss: 1.5589366508500588 - sigma: 0.006053335897210008\n",
      "Epoch 594 - loss: 1.7170137160921486 - val_loss: 1.5492322232260463 - sigma: 0.006031251317356927\n",
      "Epoch 597 - loss: 1.716283313356519 - val_loss: 1.5755001450549888 - sigma: 0.005998207154432306\n",
      "Epoch 600 - loss: 1.7159369426468623 - val_loss: 1.5634339815876372 - sigma: 0.005965261975446324\n",
      "Epoch 601 - loss: 1.7151299208127617 - val_loss: 1.5645970699138145 - sigma: 0.005954302194274766\n",
      "Epoch 606 - loss: 1.7148055374113247 - val_loss: 1.5617541953976937 - sigma: 0.00589966738415112\n",
      "Epoch 613 - loss: 1.7142904119862468 - val_loss: 1.5591755553905633 - sigma: 0.0058236361323041625\n",
      "Epoch 614 - loss: 1.714106156036034 - val_loss: 1.5581195744886895 - sigma: 0.005812817906186429\n",
      "Epoch 616 - loss: 1.713812692411972 - val_loss: 1.5667442222989079 - sigma: 0.0057912138815999735\n",
      "Epoch 619 - loss: 1.7130661082357144 - val_loss: 1.5507033959367273 - sigma: 0.005758888751893563\n",
      "Epoch 620 - loss: 1.7129623204480393 - val_loss: 1.5633526755031535 - sigma: 0.0057481352407933416\n",
      "Epoch 622 - loss: 1.712850770812338 - val_loss: 1.5461398467132974 - sigma: 0.005726660452258545\n",
      "Epoch 623 - loss: 1.7123961593942865 - val_loss: 1.5564261183692412 - sigma: 0.005715939153349177\n",
      "Epoch 624 - loss: 1.7122181985232852 - val_loss: 1.5533630026616865 - sigma: 0.005705228570379861\n",
      "Epoch 626 - loss: 1.711804824555936 - val_loss: 1.5473662913879498 - sigma: 0.005683839509429731\n",
      "Epoch 630 - loss: 1.7116077532012022 - val_loss: 1.5671443678017365 - sigma: 0.00564118950826099\n",
      "Epoch 632 - loss: 1.7109401400955333 - val_loss: 1.5545296458804732 - sigma: 0.005619928397442318\n",
      "Epoch 635 - loss: 1.7107717603107988 - val_loss: 1.541822828413149 - sigma: 0.005588116354173918\n",
      "Epoch 639 - loss: 1.7092654511593472 - val_loss: 1.5440041397782305 - sigma: 0.0055458484808609924\n",
      "Epoch 644 - loss: 1.7085441989416714 - val_loss: 1.540181328404227 - sigma: 0.005493250842131873\n",
      "Epoch 651 - loss: 1.7080784536500853 - val_loss: 1.5506311749602126 - sigma: 0.005420054572066698\n",
      "Epoch 652 - loss: 1.708016529127765 - val_loss: 1.547855826847568 - sigma: 0.005409639725785674\n",
      "Epoch 654 - loss: 1.7078866266520003 - val_loss: 1.5480077323931467 - sigma: 0.005388841251740972\n",
      "Epoch 656 - loss: 1.707170453727011 - val_loss: 1.5507746393273574 - sigma: 0.005368084333075127\n",
      "Epoch 659 - loss: 1.7069540198669433 - val_loss: 1.5457056252151598 - sigma: 0.00533702668983399\n",
      "Epoch 660 - loss: 1.7067093675041536 - val_loss: 1.5505031188566845 - sigma: 0.0053266948299350945\n",
      "Epoch 663 - loss: 1.7062238843765196 - val_loss: 1.543290889621845 - sigma: 0.0052957611691367285\n",
      "Epoch 667 - loss: 1.7057524633555137 - val_loss: 1.5460189988508741 - sigma: 0.005254660380837812\n",
      "Epoch 669 - loss: 1.7048707079218093 - val_loss: 1.5518474355437615 - sigma: 0.005234171555730851\n",
      "Epoch 675 - loss: 1.704745779656183 - val_loss: 1.5390014502337874 - sigma: 0.0051729503736062735\n",
      "Epoch 676 - loss: 1.7036527127135008 - val_loss: 1.5374723783142534 - sigma: 0.0051627825080127875\n",
      "Epoch 680 - loss: 1.7036252239069065 - val_loss: 1.5528042753884088 - sigma: 0.005122212571946104\n",
      "Epoch 682 - loss: 1.702938120640953 - val_loss: 1.547306542917759 - sigma: 0.005101988377737817\n",
      "Epoch 685 - loss: 1.7027275738436995 - val_loss: 1.5433313132674251 - sigma: 0.0050717278261274285\n",
      "Epoch 687 - loss: 1.7025230186888411 - val_loss: 1.5519780625366821 - sigma: 0.005051604500508568\n",
      "Epoch 689 - loss: 1.7024908018800864 - val_loss: 1.5558757657058941 - sigma: 0.005031521381321111\n",
      "Epoch 694 - loss: 1.7010730599196198 - val_loss: 1.5463341275907665 - sigma: 0.0049814889597027175\n",
      "Epoch 697 - loss: 1.7007662376753927 - val_loss: 1.5456608361644617 - sigma: 0.004951589364640895\n",
      "Epoch 701 - loss: 1.7007085251087615 - val_loss: 1.5484369891101992 - sigma: 0.004911862513853027\n",
      "Epoch 702 - loss: 1.7002307789611724 - val_loss: 1.5563987069917204 - sigma: 0.004901955605618867\n",
      "Epoch 705 - loss: 1.6993470429036468 - val_loss: 1.5499437393183169 - sigma: 0.0048722942530768365\n",
      "Epoch 709 - loss: 1.6987516316272218 - val_loss: 1.5481091011204087 - sigma: 0.004832883949219302\n",
      "Epoch 715 - loss: 1.6985583889053024 - val_loss: 1.5526765592997644 - sigma: 0.0047740632839815885\n",
      "Epoch 716 - loss: 1.6983068764541245 - val_loss: 1.5487069817288004 - sigma: 0.004764294106100646\n",
      "Epoch 717 - loss: 1.6978170464208548 - val_loss: 1.5464560837350938 - sigma: 0.004754534692514623\n",
      "Epoch 719 - loss: 1.6976600357230212 - val_loss: 1.5486104765727418 - sigma: 0.0047350451191994335\n",
      "Epoch 722 - loss: 1.6967491361281262 - val_loss: 1.5497691831281901 - sigma: 0.004705883747770003\n",
      "Epoch 727 - loss: 1.695980630002816 - val_loss: 1.544465080978185 - sigma: 0.004657475450624594\n",
      "Epoch 734 - loss: 1.6954621247912993 - val_loss: 1.5471830291758768 - sigma: 0.004590109179497882\n",
      "Epoch 737 - loss: 1.6949596763316008 - val_loss: 1.5454687673219842 - sigma: 0.004561381964327552\n",
      "Epoch 740 - loss: 1.694854577644252 - val_loss: 1.5505772191081015 - sigma: 0.004532740801659441\n",
      "Epoch 743 - loss: 1.6940294036734638 - val_loss: 1.5510113659142164 - sigma: 0.004504185433722888\n",
      "Epoch 745 - loss: 1.6938487652439778 - val_loss: 1.549252187541293 - sigma: 0.0044851960585603964\n",
      "Epoch 747 - loss: 1.693465929141468 - val_loss: 1.5333865367334139 - sigma: 0.0044662446241947856\n",
      "Epoch 749 - loss: 1.6925816817894888 - val_loss: 1.5356197360183215 - sigma: 0.004447331054820293\n",
      "Epoch 756 - loss: 1.6917750071374724 - val_loss: 1.5391820276606119 - sigma: 0.004381430657918774\n",
      "Epoch 760 - loss: 1.6913786437977847 - val_loss: 1.5422936775952478 - sigma: 0.0043439798867637575\n",
      "Epoch 762 - loss: 1.691090199868604 - val_loss: 1.541304696866371 - sigma: 0.004325310602497591\n",
      "Epoch 766 - loss: 1.690816988631255 - val_loss: 1.5374958424516492 - sigma: 0.004288083863201831\n",
      "Epoch 767 - loss: 1.690678014933466 - val_loss: 1.5379607736289245 - sigma: 0.004278800421832934\n",
      "Epoch 768 - loss: 1.6905667193727443 - val_loss: 1.5468028772601077 - sigma: 0.004269526259265232\n",
      "Epoch 770 - loss: 1.6904124122281332 - val_loss: 1.532612747511583 - sigma: 0.004251005733446028\n",
      "Epoch 771 - loss: 1.68979991987109 - val_loss: 1.536361466866365 - sigma: 0.004241759351674\n",
      "Epoch 774 - loss: 1.6892618195556572 - val_loss: 1.5317931247601906 - sigma: 0.004214075619979316\n",
      "Epoch 777 - loss: 1.689213098170017 - val_loss: 1.5324136415821599 - sigma: 0.004186474815027406\n",
      "Epoch 779 - loss: 1.6886051356320362 - val_loss: 1.535040219601282 - sigma: 0.004168120226104469\n",
      "Epoch 782 - loss: 1.6884572709466399 - val_loss: 1.5331546607694033 - sigma: 0.0041406570807415565\n",
      "Epoch 783 - loss: 1.6882214737388148 - val_loss: 1.5331124700230858 - sigma: 0.004131520992466293\n",
      "Epoch 784 - loss: 1.6880989017085637 - val_loss: 1.533529462889138 - sigma: 0.004122394035712783\n",
      "Epoch 787 - loss: 1.6880344931305362 - val_loss: 1.5333888577647627 - sigma: 0.004095067863358802\n",
      "Epoch 789 - loss: 1.687755397002647 - val_loss: 1.5349590338305727 - sigma: 0.004076895905647115\n",
      "Epoch 791 - loss: 1.6876119679643908 - val_loss: 1.5370931293615715 - sigma: 0.004058760255531153\n",
      "Epoch 794 - loss: 1.6868063108711375 - val_loss: 1.5404283458048482 - sigma: 0.004031624698451844\n",
      "Epoch 799 - loss: 1.6862373904394121 - val_loss: 1.5300887668066319 - sigma: 0.003986579282344431\n",
      "Epoch 806 - loss: 1.6859191980279664 - val_loss: 1.5353051010802363 - sigma: 0.0039238928857254305\n",
      "Epoch 812 - loss: 1.6858276434037958 - val_loss: 1.5291172086332998 - sigma: 0.00387050983770419\n",
      "Epoch 813 - loss: 1.6856696918303158 - val_loss: 1.5345124688512892 - sigma: 0.003861643761643354\n",
      "Epoch 815 - loss: 1.6850599675826936 - val_loss: 1.532238153774761 - sigma: 0.0038439381855979726\n",
      "Epoch 820 - loss: 1.6846764547493618 - val_loss: 1.5354398205133162 - sigma: 0.00379982885987867\n",
      "Epoch 825 - loss: 1.684484703523416 - val_loss: 1.5382721282570102 - sigma: 0.0037559395303391908\n",
      "Epoch 826 - loss: 1.6844008425951118 - val_loss: 1.5405187056557486 - sigma: 0.003747187967319659\n",
      "Epoch 829 - loss: 1.6840278356544887 - val_loss: 1.5360689971802313 - sigma: 0.0037209857264307103\n",
      "Epoch 830 - loss: 1.683710125091209 - val_loss: 1.5342194352474774 - sigma: 0.003712269099744009\n",
      "Epoch 833 - loss: 1.6835270156773348 - val_loss: 1.533020789929013 - sigma: 0.003686171458479902\n",
      "Epoch 834 - loss: 1.6833623923880534 - val_loss: 1.5393136892051498 - sigma: 0.0036774896286598177\n",
      "Epoch 839 - loss: 1.6828259203619014 - val_loss: 1.5254664847037709 - sigma: 0.0036342104685815942\n",
      "Epoch 840 - loss: 1.6827262504061806 - val_loss: 1.5359187205449119 - sigma: 0.0036255805737795705\n",
      "Epoch 841 - loss: 1.6825875427226211 - val_loss: 1.522136961215267 - sigma: 0.003616959304558841\n",
      "Epoch 843 - loss: 1.6825137982397573 - val_loss: 1.5250983436035095 - sigma: 0.003599742608384795\n",
      "Epoch 845 - loss: 1.6818059618297494 - val_loss: 1.5317575362338753 - sigma: 0.00358256031119265\n",
      "Epoch 849 - loss: 1.6814896531469063 - val_loss: 1.5281907925360545 - sigma: 0.003548298638974533\n",
      "Epoch 857 - loss: 1.6813003689070198 - val_loss: 1.5334180990751467 - sigma: 0.003480185067420944\n",
      "Epoch 858 - loss: 1.6808771779133425 - val_loss: 1.5288396487047209 - sigma: 0.0034717091210330463\n",
      "Epoch 863 - loss: 1.6808318021571855 - val_loss: 1.5298305784561586 - sigma: 0.003429456295518352\n",
      "Epoch 865 - loss: 1.6799965452448895 - val_loss: 1.5314077852615182 - sigma: 0.003412614230606249\n",
      "Epoch 881 - loss: 1.6797986992825782 - val_loss: 1.5339414231991264 - sigma: 0.0032790834974254814\n",
      "Epoch 885 - loss: 1.6797411073592179 - val_loss: 1.5333168881371528 - sigma: 0.0032460333078817745\n",
      "Epoch 889 - loss: 1.6795350139437257 - val_loss: 1.5280261702502405 - sigma: 0.0032131150550469088\n",
      "Epoch 890 - loss: 1.6790563419514732 - val_loss: 1.529495713893412 - sigma: 0.0032049060451808786\n",
      "Epoch 892 - loss: 1.6786475945185657 - val_loss: 1.5362902241036522 - sigma: 0.003188512631968201\n",
      "Epoch 896 - loss: 1.678615280541188 - val_loss: 1.5317592315415922 - sigma: 0.003155824002284523\n",
      "Epoch 897 - loss: 1.6783601232116754 - val_loss: 1.528330070970646 - sigma: 0.003147672254835275\n",
      "Epoch 900 - loss: 1.6780702064261763 - val_loss: 1.53333528137034 - sigma: 0.003123265865958874\n",
      "Epoch 903 - loss: 1.6780144959744963 - val_loss: 1.5190283584341002 - sigma: 0.0030989325865300975\n",
      "Epoch 905 - loss: 1.6775285282263566 - val_loss: 1.5267259650118483 - sigma: 0.0030827509084290303\n",
      "Epoch 906 - loss: 1.6774369394598778 - val_loss: 1.5268440186601049 - sigma: 0.0030746721975492674\n",
      "Epoch 908 - loss: 1.6772812155126784 - val_loss: 1.5249609476404398 - sigma: 0.003058538991737714\n",
      "Epoch 912 - loss: 1.6770976068365333 - val_loss: 1.5343242650162787 - sigma: 0.0030263692182108375\n",
      "Epoch 915 - loss: 1.6770630535310158 - val_loss: 1.5260496759114295 - sigma: 0.003002326193126098\n",
      "Epoch 916 - loss: 1.676818789491223 - val_loss: 1.5328272037952277 - sigma: 0.0029943278667626805\n",
      "Epoch 917 - loss: 1.6767596766228987 - val_loss: 1.5290649207674265 - sigma: 0.002986337534727797\n",
      "Epoch 918 - loss: 1.676721156747732 - val_loss: 1.5321854207635976 - sigma: 0.002978355189031112\n",
      "Epoch 920 - loss: 1.67663043314398 - val_loss: 1.5334967046966395 - sigma: 0.002962414424730938\n",
      "Epoch 921 - loss: 1.676498870228032 - val_loss: 1.5286151911803296 - sigma: 0.0029544559901866826\n",
      "Epoch 922 - loss: 1.676408737844192 - val_loss: 1.5355872989908543 - sigma: 0.0029465055100990796\n",
      "Epoch 924 - loss: 1.6759644006293661 - val_loss: 1.5265580335009479 - sigma: 0.0029306283814998554\n",
      "Epoch 926 - loss: 1.6757082766283666 - val_loss: 1.5327097447610472 - sigma: 0.0029147829754247335\n",
      "Epoch 929 - loss: 1.6755130271864958 - val_loss: 1.5341133997317231 - sigma: 0.0028910742074320216\n",
      "Epoch 934 - loss: 1.6752570841505918 - val_loss: 1.5249629973235208 - sigma: 0.002851717310630367\n",
      "Epoch 936 - loss: 1.6752046514851604 - val_loss: 1.5304354314837059 - sigma: 0.0028360295689800023\n",
      "Epoch 937 - loss: 1.6751891108851469 - val_loss: 1.5359924234044144 - sigma: 0.002828197456120129\n",
      "Epoch 939 - loss: 1.674610572610723 - val_loss: 1.534549300731245 - sigma: 0.0028125567071704202\n",
      "Epoch 944 - loss: 1.6744794270490138 - val_loss: 1.5270312184013708 - sigma: 0.0027735914180350586\n",
      "Epoch 945 - loss: 1.6744024968651232 - val_loss: 1.524760759794938 - sigma: 0.0027658217121174587\n",
      "Epoch 948 - loss: 1.6740511592977563 - val_loss: 1.5341531489550704 - sigma: 0.0027425591582588057\n",
      "Epoch 952 - loss: 1.673969494267263 - val_loss: 1.5263823027287635 - sigma: 0.00271165077959426\n",
      "Epoch 953 - loss: 1.6738529014304 - val_loss: 1.533255078523645 - sigma: 0.0027039429833551013\n",
      "Epoch 954 - loss: 1.6733906021511566 - val_loss: 1.5299323658310722 - sigma: 0.0026962428910595696\n",
      "Epoch 958 - loss: 1.6733570566393183 - val_loss: 1.530649431967097 - sigma: 0.0026655194074272307\n",
      "Epoch 959 - loss: 1.6732508073032355 - val_loss: 1.5276006233016235 - sigma: 0.0026578577195022404\n",
      "Epoch 960 - loss: 1.6728144557610996 - val_loss: 1.5252928379891475 - sigma: 0.002650203689435608\n",
      "Epoch 964 - loss: 1.67253660900851 - val_loss: 1.5351384515692492 - sigma: 0.0026196639947867455\n",
      "Epoch 966 - loss: 1.6724332810553977 - val_loss: 1.527084364755898 - sigma: 0.0026044398959706866\n",
      "Epoch 971 - loss: 1.6721492158343016 - val_loss: 1.526962192644594 - sigma: 0.0025665125937615347\n",
      "Epoch 973 - loss: 1.6720077909836253 - val_loss: 1.5308217397997492 - sigma: 0.0025513946915155586\n",
      "Epoch 977 - loss: 1.6719559438334772 - val_loss: 1.5267647425119368 - sigma: 0.0025212494434393024\n",
      "Epoch 978 - loss: 1.6718312239617263 - val_loss: 1.530104493104323 - sigma: 0.0025137319533673572\n",
      "Epoch 979 - loss: 1.6718098772474446 - val_loss: 1.525565143007332 - sigma: 0.002506221977027991\n",
      "Epoch 980 - loss: 1.6717952390506345 - val_loss: 1.5289206867372225 - sigma: 0.0024987195069112265\n",
      "Epoch 981 - loss: 1.6715940108812126 - val_loss: 1.525198751316017 - sigma: 0.0024912245355145947\n",
      "Epoch 984 - loss: 1.6711918991577261 - val_loss: 1.5305551613431791 - sigma: 0.0024687845387332177\n",
      "Epoch 985 - loss: 1.6711803002048757 - val_loss: 1.5307525978393415 - sigma: 0.0024613194873422672\n",
      "Epoch 987 - loss: 1.670870724685922 - val_loss: 1.5289358008051184 - sigma: 0.002446411761063102\n",
      "Epoch 990 - loss: 1.6707065064622355 - val_loss: 1.533288072769315 - sigma: 0.002424106001149102\n",
      "Epoch 994 - loss: 1.6703554499899158 - val_loss: 1.531339593165494 - sigma: 0.00239446889088118\n"
     ]
    }
   ],
   "source": [
    "from evolutionary_algos import EvoMLPRegressor\n",
    "\n",
    "regressor = EvoMLPRegressor(n = 48, hidden_layer_sizes = [16], random_state = 42)\n",
    "\n",
    "regressor.fit(scaled_X_train, y_train, epochs = 1000, validation_data = (scaled_X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Predicting on unseen data rows using the predict method is straight forward."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([27.65176361, 32.65071127, 26.76185299, 39.28299608, 14.84064125,\n",
       "       19.78872588, 15.06953527, 33.53340665, 13.81477573, 25.24662098,\n",
       "       26.99690256, 25.19215497, 35.05025175, 12.07671727, 25.47121052,\n",
       "       35.11311494, 19.24614639, 31.92118958, 13.95566904, 24.44267407,\n",
       "       19.30422033, 20.00243896, 26.43967488, 22.29465697, 35.92210808,\n",
       "       23.30247859, 22.15552395, 26.74220408, 29.78533881, 25.84358678,\n",
       "       17.86096007, 18.60295555, 12.15377146, 21.02185334, 12.71546834,\n",
       "       39.64879995, 23.07451285, 47.53040331, 24.01083667, 14.61555135])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_pred = regressor.predict(scaled_X_test)\n",
    "y_pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The regressor has a .training_loss_history and a .validation_loss_history property which can be useful while tuning a model. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test data: 1.7093769775391805\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(f\"Loss on test data: {mean_absolute_error(y_test, y_pred)}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(regressor.training_loss_history, label = \"Training loss\")\n",
    "ax.plot(regressor.validation_loss_history, label = \"Validation loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a5361d98b3513d72cb4620654a80a0bb40d1ea53adf5703cbac5d5174886d6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('EvoMLP-1oh6nhp_')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
