{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import mean_absolute_error, mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>mpg</th>\n",
       "      <th>cylinders</th>\n",
       "      <th>displacement</th>\n",
       "      <th>horsepower</th>\n",
       "      <th>weight</th>\n",
       "      <th>acceleration</th>\n",
       "      <th>model_year</th>\n",
       "      <th>origin_europe</th>\n",
       "      <th>origin_japan</th>\n",
       "      <th>origin_usa</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>307.0</td>\n",
       "      <td>130.0</td>\n",
       "      <td>3504</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>15.0</td>\n",
       "      <td>8</td>\n",
       "      <td>350.0</td>\n",
       "      <td>165.0</td>\n",
       "      <td>3693</td>\n",
       "      <td>11.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>18.0</td>\n",
       "      <td>8</td>\n",
       "      <td>318.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3436</td>\n",
       "      <td>11.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>16.0</td>\n",
       "      <td>8</td>\n",
       "      <td>304.0</td>\n",
       "      <td>150.0</td>\n",
       "      <td>3433</td>\n",
       "      <td>12.0</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>17.0</td>\n",
       "      <td>8</td>\n",
       "      <td>302.0</td>\n",
       "      <td>140.0</td>\n",
       "      <td>3449</td>\n",
       "      <td>10.5</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    mpg  cylinders  displacement  horsepower  weight  acceleration  \\\n",
       "0  18.0          8         307.0       130.0    3504          12.0   \n",
       "1  15.0          8         350.0       165.0    3693          11.5   \n",
       "2  18.0          8         318.0       150.0    3436          11.0   \n",
       "3  16.0          8         304.0       150.0    3433          12.0   \n",
       "4  17.0          8         302.0       140.0    3449          10.5   \n",
       "\n",
       "   model_year  origin_europe  origin_japan  origin_usa  \n",
       "0          70              0             0           1  \n",
       "1          70              0             0           1  \n",
       "2          70              0             0           1  \n",
       "3          70              0             0           1  \n",
       "4          70              0             0           1  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = sns.load_dataset(\"mpg\")\n",
    "\n",
    "X_train, y_train = df[~df[\"horsepower\"].isna()][[\"displacement\", \"acceleration\"]], df[~df[\"horsepower\"].isna()][\"horsepower\"]\n",
    "X_pred = df[df[\"horsepower\"].isna()][[\"displacement\", \"acceleration\"]]\n",
    "\n",
    "linreg = LinearRegression()\n",
    "linreg.fit(X_train, y_train)\n",
    "y_pred = linreg.predict(X_pred)\n",
    "y_pred = np.round(y_pred)\n",
    "df.loc[X_pred.index, \"horsepower\"] = y_pred\n",
    "df = pd.get_dummies(df.drop(\"name\", axis = 1), columns = [\"origin\"])\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = df.drop([\"mpg\"], axis = 1).values, df[\"mpg\"].values\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state = 42)\n",
    "X_val, X_test, y_val, y_test = train_test_split(X_test, y_test, test_size = 0.5, random_state = 42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = StandardScaler()\n",
    "#scaler = MinMaxScaler()\n",
    "\n",
    "scaled_X_train = scaler.fit_transform(X_train)\n",
    "scaled_X_val = scaler.transform(X_val)\n",
    "scaled_X_test = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from evolutionary_algos import EvoMLPRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 11.47729651839145 - val_loss: 10.797554693616892\n",
      "Epoch 1 - loss: 9.650244530343594 - val_loss: 9.363033717148546\n",
      "Epoch 2 - loss: 9.207527364413783 - val_loss: 9.197177534405714\n",
      "Epoch 3 - loss: 8.084616554140132 - val_loss: 9.734644144265024\n",
      "Epoch 4 - loss: 6.305441418301813 - val_loss: 7.456975627026308\n",
      "Epoch 7 - loss: 5.932342757460538 - val_loss: 5.585084707810168\n",
      "Epoch 8 - loss: 5.85655696656713 - val_loss: 6.780661323690616\n",
      "Epoch 12 - loss: 5.611324589434035 - val_loss: 5.130731185559079\n",
      "Epoch 14 - loss: 5.562353665810678 - val_loss: 5.986210784743693\n",
      "Epoch 19 - loss: 5.531463831026204 - val_loss: 5.350248957892359\n",
      "Epoch 20 - loss: 4.481168999406992 - val_loss: 4.099212342625239\n",
      "Epoch 28 - loss: 4.367844381101649 - val_loss: 4.711231555082381\n",
      "Epoch 34 - loss: 4.22281663849945 - val_loss: 4.046843643120317\n",
      "Epoch 40 - loss: 3.8188455118952254 - val_loss: 3.855389954521052\n",
      "Epoch 41 - loss: 3.7134910566832846 - val_loss: 3.21303272263294\n",
      "Epoch 49 - loss: 3.447950792676604 - val_loss: 2.9522845791964567\n",
      "Epoch 54 - loss: 3.314085756849849 - val_loss: 2.9787603128150257\n",
      "Epoch 55 - loss: 2.935216177301868 - val_loss: 2.9209825025301424\n",
      "Epoch 56 - loss: 2.7487198372687325 - val_loss: 3.0041097466158324\n",
      "Epoch 63 - loss: 2.6970101986953323 - val_loss: 2.5717208011682686\n",
      "Epoch 82 - loss: 2.627313861704512 - val_loss: 2.416094657561955\n",
      "Epoch 83 - loss: 2.622350124823152 - val_loss: 2.312449202972414\n",
      "Epoch 85 - loss: 2.5684340719527468 - val_loss: 2.83457744566691\n",
      "Epoch 86 - loss: 2.3660183769709264 - val_loss: 2.3015872923665874\n",
      "Epoch 124 - loss: 2.340196900523317 - val_loss: 2.344652823844588\n",
      "Epoch 132 - loss: 2.3111279140484275 - val_loss: 2.3101980061857352\n",
      "Epoch 133 - loss: 2.3077443515985556 - val_loss: 2.509340846065393\n",
      "Epoch 139 - loss: 2.255158643691195 - val_loss: 2.483003806396721\n",
      "Epoch 148 - loss: 2.2549270492526254 - val_loss: 2.35686632794268\n",
      "Epoch 153 - loss: 2.2431469518178146 - val_loss: 2.386828065363834\n",
      "Epoch 156 - loss: 2.228531974858211 - val_loss: 2.1179884796252493\n",
      "Epoch 157 - loss: 2.2257237034888817 - val_loss: 2.181062868690101\n",
      "Epoch 161 - loss: 2.2150985904004807 - val_loss: 2.227233183518607\n",
      "Epoch 162 - loss: 2.2094828277300476 - val_loss: 2.3130716893416388\n",
      "Epoch 164 - loss: 2.2054267592276986 - val_loss: 2.124545368635797\n",
      "Epoch 170 - loss: 2.191789565985467 - val_loss: 2.189675237779764\n",
      "Epoch 172 - loss: 2.1853657758589744 - val_loss: 2.261970274120481\n",
      "Epoch 173 - loss: 2.182243685483636 - val_loss: 2.3054556842513203\n",
      "Epoch 177 - loss: 2.167307218205805 - val_loss: 2.282806227887169\n",
      "Epoch 183 - loss: 2.161260981519489 - val_loss: 2.1474890916485956\n",
      "Epoch 186 - loss: 2.1466054092335978 - val_loss: 2.268413026217249\n",
      "Epoch 198 - loss: 2.1305551884362477 - val_loss: 2.1530274245105288\n",
      "Epoch 218 - loss: 2.126679934468281 - val_loss: 2.28751306469639\n",
      "Epoch 221 - loss: 2.1141853367413175 - val_loss: 2.1214797837280686\n",
      "Epoch 269 - loss: 2.1105578571704857 - val_loss: 2.133399498085246\n",
      "Epoch 272 - loss: 2.10822863232598 - val_loss: 2.1542834240767776\n",
      "Epoch 276 - loss: 2.1046189994088658 - val_loss: 2.1593133188697733\n",
      "Epoch 280 - loss: 2.104116364058734 - val_loss: 2.137714852885826\n",
      "Epoch 284 - loss: 2.103426618170212 - val_loss: 2.1197980520973125\n",
      "Epoch 292 - loss: 2.095093782865977 - val_loss: 2.1458841407673477\n",
      "Epoch 320 - loss: 2.0942427215783113 - val_loss: 2.1638763715543634\n",
      "Epoch 332 - loss: 2.094167442750416 - val_loss: 2.145109384297508\n",
      "Epoch 338 - loss: 2.088432370664648 - val_loss: 2.137716963782637\n",
      "Epoch 339 - loss: 2.0875962976017868 - val_loss: 2.0889878379795492\n",
      "Epoch 345 - loss: 2.0796649957770996 - val_loss: 2.1545761752986388\n",
      "Epoch 368 - loss: 2.0780228238883915 - val_loss: 2.1042868806544703\n",
      "Epoch 377 - loss: 2.07170479652908 - val_loss: 2.128996078911552\n",
      "Epoch 387 - loss: 2.0711752520729547 - val_loss: 2.0713859953742935\n",
      "Epoch 389 - loss: 2.065728385436387 - val_loss: 2.0170137864689854\n",
      "Epoch 391 - loss: 2.059235937419037 - val_loss: 2.0868831470910223\n",
      "Epoch 416 - loss: 2.058349048261029 - val_loss: 2.035311733571967\n",
      "Epoch 422 - loss: 2.0554732537146 - val_loss: 2.0798731497844924\n",
      "Epoch 438 - loss: 2.0534098961909666 - val_loss: 2.0625993595166294\n",
      "Epoch 448 - loss: 2.0513427940519326 - val_loss: 2.1054734017307632\n",
      "Epoch 473 - loss: 2.050804818147956 - val_loss: 2.0700151000376423\n",
      "Epoch 502 - loss: 2.0444193192517255 - val_loss: 2.021226303832186\n",
      "Epoch 569 - loss: 2.043963612090327 - val_loss: 2.0987763564613013\n",
      "Epoch 580 - loss: 2.0425577223929507 - val_loss: 2.039423482155167\n",
      "Epoch 630 - loss: 2.0376992110848744 - val_loss: 2.051361079533769\n",
      "Epoch 648 - loss: 2.0371348274280763 - val_loss: 2.021388791466726\n",
      "Epoch 728 - loss: 2.0359330195632466 - val_loss: 2.0622673988855516\n",
      "Epoch 743 - loss: 2.0354779333189157 - val_loss: 2.0426618044578957\n",
      "Epoch 744 - loss: 2.03214556875867 - val_loss: 2.016820393294656\n",
      "Epoch 827 - loss: 2.03169982314793 - val_loss: 1.990108525068082\n",
      "Epoch 833 - loss: 2.026611910325764 - val_loss: 2.05719366118657\n",
      "Epoch 841 - loss: 2.0237640267789416 - val_loss: 1.9927547843145839\n",
      "Epoch 873 - loss: 2.0220560502011784 - val_loss: 1.995509550651929\n",
      "Epoch 878 - loss: 2.0217683639622326 - val_loss: 2.0478935011460218\n",
      "Epoch 882 - loss: 2.0213114243064263 - val_loss: 2.0334986713338865\n",
      "Epoch 892 - loss: 2.020315464161399 - val_loss: 2.0129241208907565\n",
      "Epoch 924 - loss: 2.020140614745418 - val_loss: 2.0323101180218286\n",
      "Epoch 929 - loss: 2.017238719042212 - val_loss: 1.9951548031979196\n",
      "Epoch 942 - loss: 2.015725473411091 - val_loss: 2.068413045254713\n",
      "Epoch 1008 - loss: 2.0147995612343506 - val_loss: 2.053575422523734\n",
      "Epoch 1021 - loss: 2.012938029940344 - val_loss: 2.072094916936809\n",
      "Epoch 1036 - loss: 2.0113845418088467 - val_loss: 2.010721351724123\n",
      "Epoch 1070 - loss: 2.005242572239835 - val_loss: 2.0591531341426985\n",
      "Epoch 1139 - loss: 1.9991521327981567 - val_loss: 2.0644116832052317\n",
      "Epoch 1325 - loss: 1.9967805774842657 - val_loss: 2.0559860797371785\n",
      "Epoch 1383 - loss: 1.9952428282196335 - val_loss: 2.0296259478021286\n",
      "Epoch 1391 - loss: 1.9947411671766329 - val_loss: 2.044350979084075\n",
      "Epoch 1407 - loss: 1.9946881096862952 - val_loss: 2.09588043630209\n",
      "Epoch 1470 - loss: 1.99410105823827 - val_loss: 2.042886713068731\n",
      "Epoch 1481 - loss: 1.9932526880598191 - val_loss: 2.0296578681232886\n",
      "Epoch 1495 - loss: 1.9891352502331292 - val_loss: 2.0624245361621902\n",
      "Epoch 1530 - loss: 1.9886119847413 - val_loss: 2.02971354878756\n",
      "Epoch 1545 - loss: 1.9824737938354862 - val_loss: 2.0210952670894256\n",
      "Epoch 1662 - loss: 1.9824128617853942 - val_loss: 2.008554436876346\n",
      "Epoch 1667 - loss: 1.9824064604778036 - val_loss: 2.001522612951517\n",
      "Epoch 1675 - loss: 1.9822324451360456 - val_loss: 1.9258128501730538\n",
      "Epoch 1711 - loss: 1.9778552412865573 - val_loss: 1.9578012516826078\n",
      "Epoch 1714 - loss: 1.976767406154924 - val_loss: 2.001146743240593\n",
      "Epoch 1815 - loss: 1.9762839704774329 - val_loss: 2.0351537768826367\n",
      "Epoch 1858 - loss: 1.974069444179925 - val_loss: 1.984986090833781\n",
      "Epoch 1892 - loss: 1.9730695280314332 - val_loss: 2.027680755085646\n",
      "Epoch 1902 - loss: 1.9705073442644228 - val_loss: 1.9767824274155266\n",
      "Epoch 1922 - loss: 1.9678917440873023 - val_loss: 2.009896374154515\n",
      "Epoch 1944 - loss: 1.9649092200294949 - val_loss: 1.9669752030211243\n",
      "Epoch 2095 - loss: 1.963856651734928 - val_loss: 1.9735803384592248\n",
      "Epoch 2251 - loss: 1.963590566776056 - val_loss: 2.026433179054238\n",
      "Epoch 2345 - loss: 1.9625646685984148 - val_loss: 1.9920340370233842\n",
      "Epoch 2413 - loss: 1.9599533224561114 - val_loss: 1.9611486200393309\n",
      "Epoch 2553 - loss: 1.959123103787599 - val_loss: 2.0146113519739766\n",
      "Epoch 2836 - loss: 1.9572469914635922 - val_loss: 1.9782071156875698\n",
      "Epoch 3068 - loss: 1.9541227913100245 - val_loss: 1.9961257019482255\n",
      "Epoch 3214 - loss: 1.9540118430061155 - val_loss: 1.964558304466791\n",
      "Epoch 3229 - loss: 1.9531923953298163 - val_loss: 1.9666826235773005\n",
      "Epoch 3290 - loss: 1.9526996591376384 - val_loss: 1.9828633448378743\n",
      "Epoch 3325 - loss: 1.9515957751559383 - val_loss: 1.9912659774464772\n",
      "Epoch 3359 - loss: 1.9508154876527886 - val_loss: 1.9605304429638282\n",
      "Epoch 3433 - loss: 1.9502715196681457 - val_loss: 1.9443822619515854\n",
      "Epoch 3449 - loss: 1.9482232447900292 - val_loss: 1.957058098461712\n",
      "Epoch 3668 - loss: 1.947933521853441 - val_loss: 1.986271762052271\n",
      "Epoch 3690 - loss: 1.9461276760166817 - val_loss: 1.9999704992696514\n",
      "Epoch 3793 - loss: 1.9453019440158716 - val_loss: 1.9445666062535298\n",
      "Epoch 3867 - loss: 1.9429783032288774 - val_loss: 1.9634184202092306\n",
      "Epoch 3893 - loss: 1.9417155680231224 - val_loss: 1.948372311597204\n",
      "Epoch 3945 - loss: 1.9398243991118818 - val_loss: 1.9445934266136249\n",
      "Epoch 3960 - loss: 1.9382502235560726 - val_loss: 1.970933609069657\n",
      "Epoch 3992 - loss: 1.936470182901345 - val_loss: 1.96913189731156\n",
      "Epoch 4072 - loss: 1.9358857062050814 - val_loss: 1.9213667134951287\n",
      "Epoch 4079 - loss: 1.9345669616710368 - val_loss: 1.9913309197884004\n",
      "Epoch 4116 - loss: 1.93449955936019 - val_loss: 1.9548841813768127\n",
      "Epoch 4146 - loss: 1.9341383471966491 - val_loss: 1.989458621890401\n",
      "Epoch 4191 - loss: 1.9332262781887382 - val_loss: 2.012429745958157\n",
      "Epoch 4200 - loss: 1.9331377543533994 - val_loss: 1.9967956956034918\n",
      "Epoch 4204 - loss: 1.9328677180861913 - val_loss: 2.020365339516443\n",
      "Epoch 4205 - loss: 1.931650497782548 - val_loss: 1.9873470096910826\n",
      "Epoch 4265 - loss: 1.9272060934589905 - val_loss: 1.9791904732419496\n",
      "Epoch 4400 - loss: 1.9269655996310795 - val_loss: 1.904697765120845\n",
      "Epoch 4412 - loss: 1.9261260530036763 - val_loss: 1.9381700434831086\n",
      "Epoch 4437 - loss: 1.9235758678138597 - val_loss: 1.9417239654962715\n",
      "Epoch 4487 - loss: 1.921648119517875 - val_loss: 1.9134632025177418\n",
      "Epoch 4661 - loss: 1.920213089561292 - val_loss: 1.8809946404978413\n",
      "Epoch 4675 - loss: 1.9190515024339498 - val_loss: 1.9150784099828038\n",
      "Epoch 4829 - loss: 1.918422856575432 - val_loss: 1.862519918298655\n",
      "Epoch 4895 - loss: 1.9173674753914378 - val_loss: 1.9309113550843364\n",
      "Epoch 4905 - loss: 1.9167411841992061 - val_loss: 1.8712679054065675\n",
      "Epoch 5156 - loss: 1.9167287374897184 - val_loss: 1.8963129978090358\n",
      "Epoch 5180 - loss: 1.9162549700700289 - val_loss: 1.9037980733349216\n",
      "Epoch 5263 - loss: 1.9151782361638443 - val_loss: 1.8975622395772802\n",
      "Epoch 5289 - loss: 1.9139690288068165 - val_loss: 1.9323829816230649\n",
      "Epoch 5344 - loss: 1.9132653715038108 - val_loss: 1.8775390914267198\n",
      "Epoch 5355 - loss: 1.9106684383815062 - val_loss: 1.8923225119120475\n",
      "Epoch 5425 - loss: 1.9069645898911252 - val_loss: 1.9009614483971133\n",
      "Epoch 5546 - loss: 1.906794776807285 - val_loss: 1.8485448458409715\n",
      "Epoch 5693 - loss: 1.9066445460772028 - val_loss: 1.892648900018116\n",
      "Epoch 5720 - loss: 1.9053655414363353 - val_loss: 1.8739320062032092\n",
      "Epoch 5727 - loss: 1.9043370996895694 - val_loss: 1.891188384105424\n",
      "Epoch 5827 - loss: 1.9039869329876569 - val_loss: 1.8733448542866271\n",
      "Epoch 5849 - loss: 1.9023992436146047 - val_loss: 1.8719718482245642\n",
      "Epoch 5851 - loss: 1.9013211586212884 - val_loss: 1.8645030462122776\n",
      "Epoch 5860 - loss: 1.9007009718637464 - val_loss: 1.8748078971079039\n",
      "Epoch 5907 - loss: 1.8996073993875373 - val_loss: 1.8788086244879552\n",
      "Epoch 6140 - loss: 1.8982041186442502 - val_loss: 1.8854810574664846\n",
      "Epoch 6191 - loss: 1.8968528263172049 - val_loss: 1.9091460574870411\n",
      "Epoch 6228 - loss: 1.8960509203841136 - val_loss: 1.8382934750586746\n",
      "Epoch 6307 - loss: 1.8958838794522046 - val_loss: 1.9068706972180067\n",
      "Epoch 6457 - loss: 1.8947779809310854 - val_loss: 1.882166665819271\n",
      "Epoch 6559 - loss: 1.8926168731241024 - val_loss: 1.9089956789768245\n",
      "Epoch 6716 - loss: 1.8923529887688664 - val_loss: 1.8557396715703818\n",
      "Epoch 6758 - loss: 1.8915305332998869 - val_loss: 1.90855566449062\n",
      "Epoch 6773 - loss: 1.890117198286923 - val_loss: 1.8531795050716702\n",
      "Epoch 6823 - loss: 1.8888733952337202 - val_loss: 1.873377668874888\n",
      "Epoch 6923 - loss: 1.88886598467231 - val_loss: 1.8802956674997156\n",
      "Epoch 6973 - loss: 1.888057345012866 - val_loss: 1.8935097160674144\n",
      "Epoch 7052 - loss: 1.8869046708523352 - val_loss: 1.9285002326464018\n",
      "Epoch 7067 - loss: 1.886094644206102 - val_loss: 1.9011578482989548\n",
      "Epoch 7441 - loss: 1.8858873106154217 - val_loss: 1.910001467017274\n",
      "Epoch 7482 - loss: 1.8854270412986587 - val_loss: 1.8836513381123723\n",
      "Epoch 7503 - loss: 1.8847336525907574 - val_loss: 1.860896281303707\n",
      "Epoch 7567 - loss: 1.8827915854865631 - val_loss: 1.8723804084410767\n",
      "Epoch 7775 - loss: 1.8823759993027178 - val_loss: 1.904076975894992\n",
      "Epoch 7777 - loss: 1.881264701717936 - val_loss: 1.8866516536080482\n",
      "Epoch 7937 - loss: 1.880525467742838 - val_loss: 1.8596893622465682\n",
      "Epoch 7977 - loss: 1.8804404623646611 - val_loss: 1.9015512597824604\n",
      "Epoch 7989 - loss: 1.880352113197779 - val_loss: 1.8935574699212725\n",
      "Epoch 7990 - loss: 1.8796815934312874 - val_loss: 1.8438132939888985\n",
      "Epoch 8007 - loss: 1.8781177191767897 - val_loss: 1.871292798686698\n",
      "Epoch 8025 - loss: 1.8765931360216535 - val_loss: 1.8446152828633877\n",
      "Epoch 8136 - loss: 1.8751825286805919 - val_loss: 1.8718757134635353\n",
      "Epoch 8355 - loss: 1.8750558222692522 - val_loss: 1.8728621555589633\n",
      "Epoch 8368 - loss: 1.874724986350261 - val_loss: 1.8648513360791281\n",
      "Epoch 8377 - loss: 1.8732982830272225 - val_loss: 1.8663623300875838\n",
      "Epoch 8398 - loss: 1.872101902177703 - val_loss: 1.8767511980861005\n",
      "Epoch 8444 - loss: 1.8719633343339204 - val_loss: 1.8768079940106432\n",
      "Epoch 8639 - loss: 1.871578404617454 - val_loss: 1.88141664327555\n",
      "Epoch 8680 - loss: 1.8713822068478214 - val_loss: 1.8885001667325512\n",
      "Epoch 8686 - loss: 1.8713690204569042 - val_loss: 1.8598594949616836\n",
      "Epoch 8693 - loss: 1.8681820403634941 - val_loss: 1.8828828264960022\n",
      "Epoch 8810 - loss: 1.8659399312172935 - val_loss: 1.8866027089586375\n",
      "Epoch 8858 - loss: 1.865580958495336 - val_loss: 1.860220104984954\n",
      "Epoch 9010 - loss: 1.8650145867589576 - val_loss: 1.8604705614694486\n",
      "Epoch 9031 - loss: 1.8646202867871007 - val_loss: 1.8914817185705082\n",
      "Epoch 9033 - loss: 1.8635153525164494 - val_loss: 1.8646685635903695\n",
      "Epoch 9093 - loss: 1.8629856110801883 - val_loss: 1.8717867745816636\n",
      "Epoch 9147 - loss: 1.8614114647834918 - val_loss: 1.8368364490031301\n",
      "Epoch 9320 - loss: 1.8610577080850603 - val_loss: 1.878235977371088\n",
      "Epoch 9337 - loss: 1.8608203204810476 - val_loss: 1.8630516818263494\n",
      "Epoch 9357 - loss: 1.8585189170701986 - val_loss: 1.8715828447742855\n",
      "Epoch 9367 - loss: 1.8583197293630618 - val_loss: 1.8391640243959657\n",
      "Epoch 9391 - loss: 1.857064806278643 - val_loss: 1.792633443956165\n",
      "Epoch 9494 - loss: 1.8551714465466178 - val_loss: 1.8721228602880704\n",
      "Epoch 9549 - loss: 1.8540198535380372 - val_loss: 1.8478247649675559\n",
      "Epoch 9612 - loss: 1.85287852413521 - val_loss: 1.8179258325113257\n",
      "Epoch 9654 - loss: 1.8526489591062416 - val_loss: 1.812057970802588\n",
      "Epoch 9657 - loss: 1.8514426623351437 - val_loss: 1.7912869120221342\n",
      "Epoch 9669 - loss: 1.850080447375379 - val_loss: 1.79032881213752\n",
      "Epoch 9745 - loss: 1.8451678407163044 - val_loss: 1.7959486226667465\n",
      "Epoch 9835 - loss: 1.8434586364938121 - val_loss: 1.7793658689160428\n",
      "Epoch 9863 - loss: 1.8426416406622073 - val_loss: 1.8091367283491109\n",
      "Epoch 9892 - loss: 1.8406048728984337 - val_loss: 1.808652671360456\n",
      "Epoch 9933 - loss: 1.8362566398345372 - val_loss: 1.8249495980421053\n"
     ]
    }
   ],
   "source": [
    "regressor = EvoMLPRegressor(n = 240, hidden_layers = [16], activation = \"relu\", random_state = 42, lr_target = 0.02, lr_final_decay = 0.03)\n",
    "regressor.fit(scaled_X_train, y_train, epochs = 10000, validation_data = (scaled_X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss on test data: 1.5177418357046766\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "y_pred = regressor.predict(scaled_X_test)\n",
    "print(f\"Loss on test data: {mean_absolute_error(y_test, y_pred)}\")\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "\n",
    "ax.plot(regressor.training_loss_history)\n",
    "ax.plot(regressor.validation_loss_history)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoMLPRegressor2:\n",
    "    def __init__(\n",
    "        self, \n",
    "        n = 24, \n",
    "        hidden_layer_sizes = False, \n",
    "        activation = \"relu\", \n",
    "        lr_target = 0.04, \n",
    "        lr_initial_decay = 60, \n",
    "        lr_final_decay = 0.03, \n",
    "        random_state = None\n",
    "    ):\n",
    "        self.n = int(round(n / 8) * 8)\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.random_state = random_state\n",
    "        self.activation = activation\n",
    "        self.lr_target = lr_target\n",
    "        self.lr_initial_decay = lr_initial_decay\n",
    "        self.lr_final_decay = lr_final_decay\n",
    "        self.hidden_layer_sizes = hidden_layer_sizes\n",
    "\n",
    "    def fit(\n",
    "        self, \n",
    "        X_train, \n",
    "        y_train, \n",
    "        epochs = 100, \n",
    "        validation_data = False, \n",
    "        verbose = 0\n",
    "    ):\n",
    "\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        elif self.activation == \"relu\":\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "        \n",
    "        # Add bias column to X\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "        \n",
    "        if self.hidden_layer_sizes:\n",
    "            layers = [X_train.shape[1]] + self.hidden_layer_sizes + [1]\n",
    "        else:\n",
    "            layers = [X_train.shape[1]] + [1]\n",
    "        \n",
    "        n = self.n\n",
    "        ndiv4 = n // 4\n",
    "        number_of_layers_minus_one = len(layers) - 1\n",
    "\n",
    "        lr_target = self.lr_target\n",
    "        lr_initial_decay = self.lr_initial_decay\n",
    "        lr_final_decay = self.lr_final_decay\n",
    "        \n",
    "        y_preds = np.zeros((n, y_train.shape[0]))\n",
    "        nets_loss = np.zeros(n)\n",
    "        sorted_indices = np.arange(-(ndiv4), n, 1)\n",
    "\n",
    "        best_net_index = -1\n",
    "        weights = []\n",
    "\n",
    "        for i in range(number_of_layers_minus_one):\n",
    "            weights += [np.random.normal(0, 2, (n, layers[i], layers[i + 1]))]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            forward_pass = X_train.T\n",
    "            \n",
    "            # Hidden layers\n",
    "            for j in range(number_of_layers_minus_one - 1):\n",
    "                forward_pass = activation_function(weights[j][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass)\n",
    "\n",
    "            # Output layer\n",
    "            forward_pass = weights[-1][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass\n",
    "            \n",
    "            # Fill in in the predictions from the new nets\n",
    "            y_preds[sorted_indices[ndiv4:]] = forward_pass.reshape(*forward_pass.shape[::2])\n",
    "            nets_loss[sorted_indices[ndiv4:]] = np.mean(np.abs(y_preds[sorted_indices[ndiv4:]] - y_train), axis = 1)\n",
    "            \n",
    "            sorted_indices = np.argsort(nets_loss)\n",
    "\n",
    "            mutation_sigma = math.exp(-epoch / (epochs / (lr_initial_decay * math.log10(epochs + 1)))) + lr_final_decay * math.exp(-(epoch + 1) * (1 / (epochs))) + lr_target + (-0.035 * 10 * lr_final_decay)\n",
    "\n",
    "            for j in range(number_of_layers_minus_one):\n",
    "                weights[j][sorted_indices[0 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[1 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[2 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[3 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[4 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[5 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "\n",
    "            if best_net_index != sorted_indices[0]:\n",
    "                best_net_index = sorted_indices[0]\n",
    "                self.training_loss_history += [nets_loss[best_net_index]]\n",
    "\n",
    "                self.best_net_weights = []\n",
    "                for j in range(number_of_layers_minus_one):\n",
    "                    self.best_net_weights += [weights[j][best_net_index]]\n",
    "                \n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]} - sigma: {mutation_sigma}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        pass\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - sigma: {mutation_sigma}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(len(self.best_net_weights) - 1):\n",
    "            forward_pass = activation_function(self.best_net_weights[j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.best_net_weights[-1].T @ forward_pass\n",
    "        return forward_pass.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 7.737057559160331 - val_loss: 8.01487563511419 - sigma: 1.0149980000999967\n",
      "Epoch 1 - loss: 7.510331828760596 - val_loss: 8.96839793201772 - sigma: 1.0070278290745487\n",
      "Epoch 2 - loss: 6.594710600989204 - val_loss: 7.045889093706046 - sigma: 0.9991211500033316\n",
      "Epoch 4 - loss: 5.829510982995591 - val_loss: 4.908348086119199 - sigma: 0.9834962481020115\n",
      "Epoch 5 - loss: 5.281573633891502 - val_loss: 4.607294730076003 - sigma: 0.9757770255070041\n",
      "Epoch 10 - loss: 5.112338465601076 - val_loss: 4.75531962254462 - sigma: 0.9380935567139655\n",
      "Epoch 11 - loss: 4.900885893928005 - val_loss: 5.699607906812145 - sigma: 0.9307360161999603\n",
      "Epoch 20 - loss: 4.733581993006013 - val_loss: 4.3269828801831824 - sigma: 0.8671003527852732\n",
      "Epoch 25 - loss: 4.634677148123932 - val_loss: 4.266749034164244 - sigma: 0.8336770428590088\n",
      "Epoch 31 - loss: 4.5969835099279885 - val_loss: 4.909666069304842 - sigma: 0.7952939444594703\n",
      "Epoch 32 - loss: 3.944425215611959 - val_loss: 3.7948153652362238 - sigma: 0.7890739259673801\n",
      "Epoch 48 - loss: 3.9133378960722878 - val_loss: 4.154128150864471 - sigma: 0.6960308272442084\n",
      "Epoch 53 - loss: 3.576034330568115 - val_loss: 3.5443791210184576 - sigma: 0.6693131604878269\n",
      "Epoch 59 - loss: 3.5050014662963993 - val_loss: 3.3427085597688526 - sigma: 0.6386306758328736\n",
      "Epoch 68 - loss: 3.500043362815546 - val_loss: 3.322992274904968 - sigma: 0.5952809621222458\n",
      "Epoch 71 - loss: 3.4039651042044183 - val_loss: 2.491508484568967 - sigma: 0.5815106441586431\n",
      "Epoch 73 - loss: 3.339316584573895 - val_loss: 2.528769340426236 - sigma: 0.5725122567845045\n",
      "Epoch 81 - loss: 3.1005799573615893 - val_loss: 2.8486087236709055 - sigma: 0.5379239036222608\n",
      "Epoch 82 - loss: 3.0222521788835826 - val_loss: 2.542217523511023 - sigma: 0.5337538713632486\n",
      "Epoch 87 - loss: 2.7900851172313295 - val_loss: 2.5776637065524675 - sigma: 0.5133966277288644\n",
      "Epoch 105 - loss: 2.5806639865239847 - val_loss: 2.238137868444637 - sigma: 0.4464957060048679\n",
      "Epoch 133 - loss: 2.5597229587754673 - val_loss: 2.278871638104951 - sigma: 0.35980255651121784\n",
      "Epoch 135 - loss: 2.389991881293036 - val_loss: 2.1923572773333344 - sigma: 0.3543213850304547\n",
      "Epoch 144 - loss: 2.3897667030799057 - val_loss: 2.260097754156181 - sigma: 0.33071226880997523\n",
      "Epoch 152 - loss: 2.317548370624144 - val_loss: 1.9666911210402696 - sigma: 0.311105868399648\n",
      "Epoch 155 - loss: 2.2213342726991 - val_loss: 1.8921802383551054 - sigma: 0.3040707431375028\n",
      "Epoch 156 - loss: 2.2100679536270738 - val_loss: 1.9522223125746123 - sigma: 0.30176294220880995\n",
      "Epoch 159 - loss: 2.1817222830251892 - val_loss: 1.783611105281245 - sigma: 0.29494919575106615\n",
      "Epoch 196 - loss: 2.169362223837118 - val_loss: 1.8299758890132254 - sigma: 0.2230679959142723\n",
      "Epoch 200 - loss: 2.1613567689524613 - val_loss: 1.5951171797937653 - sigma: 0.21649502406552745\n",
      "Epoch 208 - loss: 2.068152715947494 - val_loss: 1.6537737557148877 - sigma: 0.20396285983480494\n",
      "Epoch 222 - loss: 2.044263435569728 - val_loss: 1.6074317847039978 - sigma: 0.18386972227028853\n",
      "Epoch 223 - loss: 2.024180012318003 - val_loss: 1.6661517875961995 - sigma: 0.18251866912333697\n",
      "Epoch 254 - loss: 2.02377669352096 - val_loss: 1.635468611827782 - sigma: 0.14556666860206052\n",
      "Epoch 255 - loss: 2.0085838582452245 - val_loss: 1.7799742943459491 - sigma: 0.14452032907801102\n",
      "Epoch 257 - loss: 1.962146309200951 - val_loss: 1.7028262034536954 - sigma: 0.14245254993937212\n",
      "Epoch 287 - loss: 1.9562957358383428 - val_loss: 1.7390890579786131 - sigma: 0.11509038841931071\n",
      "Epoch 289 - loss: 1.931939727714663 - val_loss: 1.7276493624111864 - sigma: 0.11348877019211048\n",
      "Epoch 310 - loss: 1.9309500645686655 - val_loss: 1.6945486336462114 - sigma: 0.09812854345091848\n",
      "Epoch 316 - loss: 1.9165329655655967 - val_loss: 1.6412045430696125 - sigma: 0.09419225063205743\n",
      "Epoch 325 - loss: 1.9160439403016423 - val_loss: 1.6794945588854169 - sigma: 0.08862999471794081\n",
      "Epoch 335 - loss: 1.9154285142759147 - val_loss: 1.715858418609282 - sigma: 0.08290032346238575\n",
      "Epoch 338 - loss: 1.9151043442402618 - val_loss: 1.695851250560402 - sigma: 0.0812686259125888\n",
      "Epoch 339 - loss: 1.8975119758828687 - val_loss: 1.6226257756067581 - sigma: 0.08073334103296806\n",
      "Epoch 348 - loss: 1.8883557796038821 - val_loss: 1.6014799510952826 - sigma: 0.076103021003416\n",
      "Epoch 349 - loss: 1.8794306295807504 - val_loss: 1.6818340777023528 - sigma: 0.07560874450643315\n",
      "Epoch 353 - loss: 1.8711497901415206 - val_loss: 1.6484165456671753 - sigma: 0.07367056000257519\n",
      "Epoch 364 - loss: 1.8709951491100194 - val_loss: 1.7016132771960328 - sigma: 0.06864833038113881\n",
      "Epoch 369 - loss: 1.8542200052780788 - val_loss: 1.5969567157900237 - sigma: 0.06650697977903106\n",
      "Epoch 379 - loss: 1.8532092121596824 - val_loss: 1.620474049799117 - sigma: 0.06247177499019128\n",
      "Epoch 383 - loss: 1.8495023063112266 - val_loss: 1.5373841497164922 - sigma: 0.060945524214806014\n",
      "Epoch 388 - loss: 1.8301210663355791 - val_loss: 1.5586324682254091 - sigma: 0.059104791225377935\n",
      "Epoch 400 - loss: 1.8291661303612423 - val_loss: 1.56181293327245 - sigma: 0.05497465514999553\n",
      "Epoch 402 - loss: 1.8225493729806312 - val_loss: 1.6198322935930194 - sigma: 0.05432382285282105\n",
      "Epoch 415 - loss: 1.8223668986128168 - val_loss: 1.5079992703500291 - sigma: 0.05033659676582362\n",
      "Epoch 419 - loss: 1.8180591627145681 - val_loss: 1.5916522153091464 - sigma: 0.049190376908413415\n",
      "Epoch 425 - loss: 1.815112798595744 - val_loss: 1.5854996381350268 - sigma: 0.047537930694687244\n",
      "Epoch 427 - loss: 1.8069068970582192 - val_loss: 1.5124145125054045 - sigma: 0.04700438851506455\n",
      "Epoch 431 - loss: 1.80643860493466 - val_loss: 1.600951403130408 - sigma: 0.04596239699044403\n",
      "Epoch 440 - loss: 1.8042574419820447 - val_loss: 1.5614014305027901 - sigma: 0.043735469354622254\n",
      "Epoch 442 - loss: 1.801706977396539 - val_loss: 1.6365160262978826 - sigma: 0.04326183283781306\n",
      "Epoch 446 - loss: 1.8009564748841769 - val_loss: 1.5822342581660238 - sigma: 0.04233681537960348\n",
      "Epoch 448 - loss: 1.7985136813132374 - val_loss: 1.6036816882933678 - sigma: 0.04188519958218007\n",
      "Epoch 449 - loss: 1.7959964684149754 - val_loss: 1.5260567686259408 - sigma: 0.041662064459019564\n",
      "Epoch 450 - loss: 1.7897790431038323 - val_loss: 1.541225167689562 - sigma: 0.04144069227008339\n",
      "Epoch 453 - loss: 1.7886037677040871 - val_loss: 1.5767295825604093 - sigma: 0.0407870134087519\n",
      "Epoch 466 - loss: 1.7881156039651802 - val_loss: 1.5935752037694941 - sigma: 0.03812736984521823\n",
      "Epoch 470 - loss: 1.7799617272441306 - val_loss: 1.5998823060853364 - sigma: 0.03736262978624135\n",
      "Epoch 471 - loss: 1.7788361702144242 - val_loss: 1.5193446642370234 - sigma: 0.037175200638682904\n",
      "Epoch 479 - loss: 1.778344175516668 - val_loss: 1.5657429480468328 - sigma: 0.03572801416143811\n",
      "Epoch 481 - loss: 1.7769696905731005 - val_loss: 1.5367208295094446 - sigma: 0.0353803113211551\n",
      "Epoch 488 - loss: 1.776825681020484 - val_loss: 1.5169343573864171 - sigma: 0.03420577806026951\n",
      "Epoch 495 - loss: 1.7762591682365418 - val_loss: 1.6172350423289665 - sigma: 0.03309449457625903\n",
      "Epoch 496 - loss: 1.7677636297408945 - val_loss: 1.5477715483894872 - sigma: 0.03294069982117824\n",
      "Epoch 511 - loss: 1.764155199066572 - val_loss: 1.5406417356026376 - sigma: 0.030773773799720676\n",
      "Epoch 515 - loss: 1.7631301692945747 - val_loss: 1.5631614813816401 - sigma: 0.03023796132644481\n",
      "Epoch 520 - loss: 1.7623379886435442 - val_loss: 1.5702578816189212 - sigma: 0.029591531804466227\n",
      "Epoch 521 - loss: 1.7599697674289743 - val_loss: 1.5707670745730236 - sigma: 0.029465275352712005\n",
      "Epoch 523 - loss: 1.7582424785520676 - val_loss: 1.5680380624872654 - sigma: 0.02921572784238806\n",
      "Epoch 524 - loss: 1.7540262817852363 - val_loss: 1.568092526037136 - sigma: 0.029092421055269616\n",
      "Epoch 540 - loss: 1.7489375157054743 - val_loss: 1.5329715321570727 - sigma: 0.027246007133691914\n",
      "Epoch 554 - loss: 1.7466804798687938 - val_loss: 1.5439244638771963 - sigma: 0.025810352844625656\n",
      "Epoch 559 - loss: 1.7466118995258675 - val_loss: 1.5732274452400254 - sigma: 0.025334672153322392\n",
      "Epoch 560 - loss: 1.7460382955355183 - val_loss: 1.5145546090223692 - sigma: 0.025241753661376487\n",
      "Epoch 561 - loss: 1.7429501189026126 - val_loss: 1.536144655089292 - sigma: 0.025149560681300358\n",
      "Epoch 564 - loss: 1.7412653516934458 - val_loss: 1.5111246035355992 - sigma: 0.024877277247032863\n",
      "Epoch 566 - loss: 1.7403108871538218 - val_loss: 1.5596057899602873 - sigma: 0.024699277780073296\n",
      "Epoch 569 - loss: 1.7396055453184023 - val_loss: 1.5302686238702254 - sigma: 0.024437451305255806\n",
      "Epoch 573 - loss: 1.7393698658274808 - val_loss: 1.5512218169693028 - sigma: 0.02409777647487233\n",
      "Epoch 574 - loss: 1.7392059827583353 - val_loss: 1.5453293854716126 - sigma: 0.02401450561463144\n",
      "Epoch 575 - loss: 1.7360639915294636 - val_loss: 1.5461656170336284 - sigma: 0.02393188341309349\n",
      "Epoch 578 - loss: 1.7356307004763434 - val_loss: 1.4842884415079953 - sigma: 0.023687857294851837\n",
      "Epoch 580 - loss: 1.733394097594087 - val_loss: 1.531364737458036 - sigma: 0.023528322870284585\n",
      "Epoch 582 - loss: 1.7323770872388176 - val_loss: 1.5344653404991213 - sigma: 0.023371261553036073\n",
      "Epoch 583 - loss: 1.7321788844471568 - val_loss: 1.4995319330322783 - sigma: 0.023293646021634716\n",
      "Epoch 585 - loss: 1.7307069732431342 - val_loss: 1.4988818901141765 - sigma: 0.02314022097897237\n",
      "Epoch 587 - loss: 1.729484308196671 - val_loss: 1.5035500928587004 - sigma: 0.022989172099892742\n",
      "Epoch 591 - loss: 1.7292713630433494 - val_loss: 1.5176656628941327 - sigma: 0.022694052612297427\n",
      "Epoch 594 - loss: 1.7269979180929804 - val_loss: 1.549267945555839 - sigma: 0.02247867432486727\n",
      "Epoch 595 - loss: 1.7261084011039858 - val_loss: 1.5241544248999628 - sigma: 0.022407993046415567\n",
      "Epoch 599 - loss: 1.7256138049228813 - val_loss: 1.4974522381125817 - sigma: 0.02213070816507868\n",
      "Epoch 600 - loss: 1.7252281225940003 - val_loss: 1.5049624806218547 - sigma: 0.022062725422381545\n",
      "Epoch 601 - loss: 1.724325842090445 - val_loss: 1.50536603648241 - sigma: 0.021995269558627538\n",
      "Epoch 605 - loss: 1.7212226395192536 - val_loss: 1.529089001442488 - sigma: 0.021730631457457183\n",
      "Epoch 610 - loss: 1.7209219437122742 - val_loss: 1.4904969757553892 - sigma: 0.02141119478184924\n",
      "Epoch 611 - loss: 1.7199607487005004 - val_loss: 1.5221338084047045 - sigma: 0.021348782315361414\n",
      "Epoch 619 - loss: 1.7186583627643632 - val_loss: 1.4796556161802574 - sigma: 0.02086653395040644\n",
      "Epoch 620 - loss: 1.7158999045675916 - val_loss: 1.5134542090655612 - sigma: 0.020808328261861984\n",
      "Epoch 625 - loss: 1.71515198747595 - val_loss: 1.529685196467292 - sigma: 0.020523963769732645\n",
      "Epoch 626 - loss: 1.7143722301115631 - val_loss: 1.5052122895631141 - sigma: 0.020468399023724708\n",
      "Epoch 635 - loss: 1.7109481045092245 - val_loss: 1.523894185224122 - sigma: 0.019987171512608426\n",
      "Epoch 638 - loss: 1.709635325560792 - val_loss: 1.4890915134404665 - sigma: 0.01983404790039226\n",
      "Epoch 642 - loss: 1.7084412020501578 - val_loss: 1.5101548111652456 - sigma: 0.01963531227257067\n",
      "Epoch 644 - loss: 1.708126013148235 - val_loss: 1.4958804829043626 - sigma: 0.01953821606505455\n",
      "Epoch 647 - loss: 1.7063783746594392 - val_loss: 1.487920867260031 - sigma: 0.019395343912549337\n",
      "Epoch 648 - loss: 1.7056535783829323 - val_loss: 1.4954507205972942 - sigma: 0.019348447368319324\n",
      "Epoch 651 - loss: 1.7053337496013474 - val_loss: 1.5066963500596897 - sigma: 0.019209899912973165\n",
      "Epoch 653 - loss: 1.7039813528828567 - val_loss: 1.5104444669792003 - sigma: 0.019119291788631164\n",
      "Epoch 654 - loss: 1.7035525059225662 - val_loss: 1.512511819099711 - sigma: 0.019074506407720758\n",
      "Epoch 659 - loss: 1.7034175030273493 - val_loss: 1.5390030887588053 - sigma: 0.018855657109099845\n",
      "Epoch 661 - loss: 1.7030352871179042 - val_loss: 1.5126746416388865 - sigma: 0.018770436984309136\n",
      "Epoch 662 - loss: 1.7027370293018715 - val_loss: 1.5060442544373918 - sigma: 0.018728313464388448\n",
      "Epoch 665 - loss: 1.6986954618837022 - val_loss: 1.515514526257623 - sigma: 0.018603858221227593\n",
      "Epoch 672 - loss: 1.697485592739296 - val_loss: 1.4745071137189136 - sigma: 0.018324313815233327\n",
      "Epoch 676 - loss: 1.697290098137932 - val_loss: 1.5467000594370002 - sigma: 0.018171145251980422\n",
      "Epoch 678 - loss: 1.6971687032882612 - val_loss: 1.505830284380705 - sigma: 0.018096291862433223\n",
      "Epoch 680 - loss: 1.696914605866057 - val_loss: 1.500870381561127 - sigma: 0.01802256802768696\n",
      "Epoch 683 - loss: 1.6963527281378095 - val_loss: 1.5062803665701303 - sigma: 0.017914061074090756\n",
      "Epoch 685 - loss: 1.6960377392976342 - val_loss: 1.4962518841428367 - sigma: 0.017843083357999417\n",
      "Epoch 690 - loss: 1.694768723192563 - val_loss: 1.4970308134022114 - sigma: 0.017670274921783982\n",
      "Epoch 692 - loss: 1.6946055246791376 - val_loss: 1.5462866001352316 - sigma: 0.017602961945010757\n",
      "Epoch 693 - loss: 1.6935896330095497 - val_loss: 1.5517177539672249 - sigma: 0.017569685194664543\n",
      "Epoch 701 - loss: 1.6930721473147987 - val_loss: 1.5093720120489333 - sigma: 0.017312322675423243\n",
      "Epoch 706 - loss: 1.6924343824043935 - val_loss: 1.5255421758549175 - sigma: 0.017159170183584156\n",
      "Epoch 710 - loss: 1.6921429716510925 - val_loss: 1.5159901256441022 - sigma: 0.017040722958798882\n",
      "Epoch 712 - loss: 1.6912993704713353 - val_loss: 1.5087316095691894 - sigma: 0.01698281829330007\n",
      "Epoch 715 - loss: 1.6911187987011744 - val_loss: 1.5056471507954572 - sigma: 0.016897570887760026\n",
      "Epoch 716 - loss: 1.6910086926877446 - val_loss: 1.4959125415659908 - sigma: 0.01686957749896799\n",
      "Epoch 718 - loss: 1.6892953947799025 - val_loss: 1.5027335011570655 - sigma: 0.01681421428570798\n",
      "Epoch 720 - loss: 1.688752117965804 - val_loss: 1.4964450449790232 - sigma: 0.01675967149644629\n",
      "Epoch 726 - loss: 1.6872410032039442 - val_loss: 1.492973752084 - sigma: 0.01660083659396027\n",
      "Epoch 730 - loss: 1.6855014804749444 - val_loss: 1.4965044507480392 - sigma: 0.01649881569172556\n",
      "Epoch 737 - loss: 1.6850275521972724 - val_loss: 1.4911765537654027 - sigma: 0.016327397423938843\n",
      "Epoch 745 - loss: 1.6843041620108823 - val_loss: 1.498011525506294 - sigma: 0.01614203819903553\n",
      "Epoch 747 - loss: 1.6837649771267666 - val_loss: 1.4920005617295353 - sigma: 0.016097378203253944\n",
      "Epoch 748 - loss: 1.681540472942499 - val_loss: 1.5079982900458226 - sigma: 0.01607529286743138\n",
      "Epoch 754 - loss: 1.6791283125052514 - val_loss: 1.4929808766053057 - sigma: 0.01594612519722912\n",
      "Epoch 764 - loss: 1.678606480864288 - val_loss: 1.500918551998014 - sigma: 0.015743025385551883\n",
      "Epoch 769 - loss: 1.6775826027279963 - val_loss: 1.5160292422241866 - sigma: 0.01564687393450285\n",
      "Epoch 772 - loss: 1.6754737558426829 - val_loss: 1.5163538386021127 - sigma: 0.01559082934033247\n",
      "Epoch 781 - loss: 1.6751816723720512 - val_loss: 1.517187290520158 - sigma: 0.015429776744118833\n",
      "Epoch 788 - loss: 1.6751153101864882 - val_loss: 1.518215649335565 - sigma: 0.015311495716697146\n",
      "Epoch 789 - loss: 1.673824659525422 - val_loss: 1.4987809250455726 - sigma: 0.0152950749618383\n",
      "Epoch 792 - loss: 1.6730284484682472 - val_loss: 1.4862616413946363 - sigma: 0.015246506813461893\n",
      "Epoch 794 - loss: 1.6709544350717294 - val_loss: 1.4939973090314047 - sigma: 0.015214697317275044\n",
      "Epoch 808 - loss: 1.6705294308923886 - val_loss: 1.4701509921404001 - sigma: 0.015004157964978183\n",
      "Epoch 809 - loss: 1.6704688490092803 - val_loss: 1.4720752987921668 - sigma: 0.01498989557118968\n",
      "Epoch 812 - loss: 1.6694187017891924 - val_loss: 1.47687013479936 - sigma: 0.014947700037330874\n",
      "Epoch 821 - loss: 1.6670726572556684 - val_loss: 1.4746935167092805 - sigma: 0.014826258111662318\n",
      "Epoch 823 - loss: 1.6658823686210285 - val_loss: 1.4741197424318322 - sigma: 0.014800280652049308\n",
      "Epoch 826 - loss: 1.6656593905956667 - val_loss: 1.4793084704265598 - sigma: 0.014761977574642085\n",
      "Epoch 837 - loss: 1.6648564422001901 - val_loss: 1.4755563045228646 - sigma: 0.01462805878613478\n",
      "Epoch 840 - loss: 1.6639624243059428 - val_loss: 1.4712615488980236 - sigma: 0.014593236542095411\n",
      "Epoch 841 - loss: 1.6638918585488978 - val_loss: 1.4884127314153652 - sigma: 0.014581784753575534\n",
      "Epoch 847 - loss: 1.6638787532476058 - val_loss: 1.4652812075569257 - sigma: 0.01451466528191434\n",
      "Epoch 849 - loss: 1.6613674413203223 - val_loss: 1.4943227004134976 - sigma: 0.014492883987236761\n",
      "Epoch 850 - loss: 1.6608113638961675 - val_loss: 1.4844786658766667 - sigma: 0.014482101680211556\n",
      "Epoch 861 - loss: 1.6599061821371497 - val_loss: 1.45658521156845 - sigma: 0.014368090049411591\n",
      "Epoch 864 - loss: 1.6590088575766557 - val_loss: 1.4701899690935916 - sigma: 0.01433840052909932\n",
      "Epoch 869 - loss: 1.658745026598854 - val_loss: 1.468070183980583 - sigma: 0.01429018950480297\n",
      "Epoch 871 - loss: 1.6583408888454687 - val_loss: 1.4645911235178795 - sigma: 0.014271338438476227\n",
      "Epoch 875 - loss: 1.656802681247746 - val_loss: 1.487833347679242 - sigma: 0.014234357754117367\n",
      "Epoch 880 - loss: 1.6560878373418726 - val_loss: 1.4688294347534327 - sigma: 0.014189445704991593\n",
      "Epoch 883 - loss: 1.6551900904112988 - val_loss: 1.4944735957646045 - sigma: 0.014163177158683758\n",
      "Epoch 893 - loss: 1.6547883288297416 - val_loss: 1.4863762541809689 - sigma: 0.014079120353200946\n",
      "Epoch 895 - loss: 1.6542563581703758 - val_loss: 1.4687462115035568 - sigma: 0.014062930768756731\n",
      "Epoch 907 - loss: 1.653908454651806 - val_loss: 1.4807946562955216 - sigma: 0.01396987714557421\n",
      "Epoch 910 - loss: 1.6536840698381803 - val_loss: 1.4666105076879397 - sigma: 0.013947659368104477\n",
      "Epoch 911 - loss: 1.652618598831171 - val_loss: 1.4969281588986878 - sigma: 0.013940342491942091\n",
      "Epoch 913 - loss: 1.6525872650074274 - val_loss: 1.4539666359178514 - sigma: 0.013925840201085926\n",
      "Epoch 922 - loss: 1.651650229853003 - val_loss: 1.493132894303257 - sigma: 0.013862681329786191\n",
      "Epoch 923 - loss: 1.6495360111683635 - val_loss: 1.483063919665104 - sigma: 0.013855869283002939\n",
      "Epoch 938 - loss: 1.6486961399152722 - val_loss: 1.459877666076584 - sigma: 0.013758307474979385\n",
      "Epoch 939 - loss: 1.6474687017426506 - val_loss: 1.4627018788810724 - sigma: 0.01375209769807363\n",
      "Epoch 950 - loss: 1.6472452380525084 - val_loss: 1.4765604590015475 - sigma: 0.013686050176412485\n",
      "Epoch 956 - loss: 1.6472178763803897 - val_loss: 1.4647265425981295 - sigma: 0.01365168948833404\n",
      "Epoch 959 - loss: 1.6446076810455104 - val_loss: 1.4741869996179933 - sigma: 0.01363492705507415\n",
      "Epoch 971 - loss: 1.6445535627443575 - val_loss: 1.4639973459484055 - sigma: 0.013570513122507752\n",
      "Epoch 976 - loss: 1.6436611708524551 - val_loss: 1.4664988436687891 - sigma: 0.013544854505305138\n",
      "Epoch 978 - loss: 1.642186927336671 - val_loss: 1.471697564543544 - sigma: 0.013534775890698089\n",
      "Epoch 988 - loss: 1.6407644762373395 - val_loss: 1.4820167176594343 - sigma: 0.013485897578766446\n",
      "Epoch 1002 - loss: 1.6405499830068357 - val_loss: 1.455665465840914 - sigma: 0.013421429357378658\n",
      "Epoch 1011 - loss: 1.639991832179252 - val_loss: 1.4645758960888735 - sigma: 0.013382221863025531\n",
      "Epoch 1018 - loss: 1.6399484470554786 - val_loss: 1.4650189941120284 - sigma: 0.013352844480333538\n",
      "Epoch 1020 - loss: 1.6391298279696103 - val_loss: 1.4625684992131345 - sigma: 0.013344622135473569\n",
      "Epoch 1026 - loss: 1.6381640180615287 - val_loss: 1.4526841589851736 - sigma: 0.013320393871363806\n",
      "Epoch 1038 - loss: 1.6367085123709648 - val_loss: 1.4667012045509449 - sigma: 0.013273810919836279\n",
      "Epoch 1047 - loss: 1.6358050257805403 - val_loss: 1.4491385150619513 - sigma: 0.013240400588486438\n",
      "Epoch 1055 - loss: 1.6357776239952984 - val_loss: 1.4572017354358138 - sigma: 0.013211720152151834\n",
      "Epoch 1057 - loss: 1.6354687703602113 - val_loss: 1.4448173930971722 - sigma: 0.013204692356481055\n",
      "Epoch 1064 - loss: 1.6349174336784555 - val_loss: 1.4385017680639107 - sigma: 0.013180523725082006\n",
      "Epoch 1068 - loss: 1.6348313695157073 - val_loss: 1.4369500839844442 - sigma: 0.013167002401374776\n",
      "Epoch 1080 - loss: 1.6345202114060375 - val_loss: 1.446857002260374 - sigma: 0.013127627073688765\n",
      "Epoch 1081 - loss: 1.6344749653282091 - val_loss: 1.4428597868137902 - sigma: 0.013124422754826696\n",
      "Epoch 1083 - loss: 1.6324791035685413 - val_loss: 1.4774562443796224 - sigma: 0.013118048255491307\n",
      "Epoch 1097 - loss: 1.632251315634737 - val_loss: 1.436984417601956 - sigma: 0.013074646120320643\n",
      "Epoch 1109 - loss: 1.6298663804778593 - val_loss: 1.4784923534104926 - sigma: 0.013039023252374804\n",
      "Epoch 1122 - loss: 1.6287378112899495 - val_loss: 1.4633021205205043 - sigma: 0.013001916838570477\n",
      "Epoch 1125 - loss: 1.627314915107103 - val_loss: 1.4419358331876424 - sigma: 0.012993557582967252\n",
      "Epoch 1152 - loss: 1.6259204278858943 - val_loss: 1.4472672632898864 - sigma: 0.012921401170493839\n",
      "Epoch 1173 - loss: 1.625751091998917 - val_loss: 1.433303947187612 - sigma: 0.012868638337399102\n",
      "Epoch 1177 - loss: 1.6254987973233914 - val_loss: 1.4268000489693171 - sigma: 0.012858878882901088\n",
      "Epoch 1179 - loss: 1.624512441506993 - val_loss: 1.4353435129965348 - sigma: 0.012854031650413334\n",
      "Epoch 1184 - loss: 1.6244101553740034 - val_loss: 1.4234187252213246 - sigma: 0.012842005696349195\n",
      "Epoch 1195 - loss: 1.6242218630019158 - val_loss: 1.4187404132173878 - sigma: 0.012815990999609032\n",
      "Epoch 1198 - loss: 1.6223175206753941 - val_loss: 1.4383416280490777 - sigma: 0.012808996615301283\n",
      "Epoch 1212 - loss: 1.6222639824947689 - val_loss: 1.4339092631513737 - sigma: 0.012776886428717304\n",
      "Epoch 1215 - loss: 1.6217616233555672 - val_loss: 1.4355680640027888 - sigma: 0.012770113635224031\n",
      "Epoch 1220 - loss: 1.6202447005202856 - val_loss: 1.4265952357091125 - sigma: 0.012758905668697777\n",
      "Epoch 1230 - loss: 1.6201237523832812 - val_loss: 1.4048543225255956 - sigma: 0.012736776432063458\n",
      "Epoch 1244 - loss: 1.6191828532444685 - val_loss: 1.434722415541519 - sigma: 0.012706392355138976\n",
      "Epoch 1249 - loss: 1.6190110503316042 - val_loss: 1.4328575275118831 - sigma: 0.012695697673237914\n",
      "Epoch 1256 - loss: 1.6184007482307452 - val_loss: 1.4047448136475371 - sigma: 0.012680854904991162\n",
      "Epoch 1257 - loss: 1.6178297446908263 - val_loss: 1.4306730264530905 - sigma: 0.012678746471687407\n",
      "Epoch 1261 - loss: 1.6163086206503932 - val_loss: 1.4221537574975187 - sigma: 0.012670341755139576\n",
      "Epoch 1262 - loss: 1.616153277023389 - val_loss: 1.4263685148341374 - sigma: 0.012668247721935191\n",
      "Epoch 1283 - loss: 1.615694494368349 - val_loss: 1.4268718674694312 - sigma: 0.0126248922231484\n",
      "Epoch 1285 - loss: 1.6145586205906934 - val_loss: 1.4190262170492918 - sigma: 0.012620821208617126\n",
      "Epoch 1294 - loss: 1.6140408726973063 - val_loss: 1.4404009175251555 - sigma: 0.012602617048188788\n",
      "Epoch 1311 - loss: 1.6133044907030234 - val_loss: 1.427961940645758 - sigma: 0.012568712671761828\n",
      "Epoch 1323 - loss: 1.612579356432937 - val_loss: 1.4322026068268963 - sigma: 0.01254512553872606\n",
      "Epoch 1330 - loss: 1.612077331417186 - val_loss: 1.4283368656596038 - sigma: 0.012531487273355004\n",
      "Epoch 1340 - loss: 1.6112695190931072 - val_loss: 1.3990102487590959 - sigma: 0.012512148145837244\n",
      "Epoch 1346 - loss: 1.6112664611537215 - val_loss: 1.401179156736039 - sigma: 0.012500621697791555\n",
      "Epoch 1363 - loss: 1.610399000338955 - val_loss: 1.414027227519942 - sigma: 0.012468253677320759\n",
      "Epoch 1371 - loss: 1.6098839755599927 - val_loss: 1.4111007864151615 - sigma: 0.01245315973558996\n",
      "Epoch 1379 - loss: 1.6094855098254441 - val_loss: 1.4045038029304615 - sigma: 0.01243814760477551\n",
      "Epoch 1384 - loss: 1.6083987826358994 - val_loss: 1.4017926683738384 - sigma: 0.012428804605906235\n",
      "Epoch 1386 - loss: 1.6079203668707542 - val_loss: 1.412185303672586 - sigma: 0.012425075643698593\n",
      "Epoch 1409 - loss: 1.6069637966897714 - val_loss: 1.4038051464604056 - sigma: 0.012382508938792563\n",
      "Epoch 1422 - loss: 1.6067803163982874 - val_loss: 1.4018289710676513 - sigma: 0.012358686193466749\n",
      "Epoch 1428 - loss: 1.6059051704066305 - val_loss: 1.4190017089185651 - sigma: 0.012347743609047222\n",
      "Epoch 1440 - loss: 1.6058999798465017 - val_loss: 1.4050442163888965 - sigma: 0.012325951534768743\n",
      "Epoch 1442 - loss: 1.6046198130338918 - val_loss: 1.3806084004410308 - sigma: 0.01232233108658142\n",
      "Epoch 1453 - loss: 1.6035807038868923 - val_loss: 1.404070706057879 - sigma: 0.01230247465893809\n",
      "Epoch 1457 - loss: 1.6030629164421053 - val_loss: 1.4102099459860358 - sigma: 0.012295276838168485\n",
      "Epoch 1459 - loss: 1.6026396890172983 - val_loss: 1.4175292781425566 - sigma: 0.012291682310750436\n",
      "Epoch 1464 - loss: 1.6022023010717028 - val_loss: 1.388204025050072 - sigma: 0.01228270849284428\n",
      "Epoch 1465 - loss: 1.6011671285913116 - val_loss: 1.405755337619219 - sigma: 0.012280915837779382\n",
      "Epoch 1488 - loss: 1.6008545181749092 - val_loss: 1.405056900118404 - sigma: 0.012239867945778566\n",
      "Epoch 1491 - loss: 1.6004320422365133 - val_loss: 1.4018663035656793 - sigma: 0.012234538421742323\n",
      "Epoch 1493 - loss: 1.6000596586896187 - val_loss: 1.386154263129073 - sigma: 0.01223098838556042\n",
      "Epoch 1517 - loss: 1.5995191428256712 - val_loss: 1.4077873732341184 - sigma: 0.012188564135484336\n",
      "Epoch 1524 - loss: 1.5991187746558317 - val_loss: 1.3721261181582634 - sigma: 0.012176248068617722\n",
      "Epoch 1532 - loss: 1.5988078111168187 - val_loss: 1.3871030915718805 - sigma: 0.012162202289922012\n",
      "Epoch 1541 - loss: 1.5977502107410635 - val_loss: 1.3971764183776416 - sigma: 0.012146437146038238\n",
      "Epoch 1547 - loss: 1.5966744463012874 - val_loss: 1.3843632397659698 - sigma: 0.012135947616713645\n",
      "Epoch 1572 - loss: 1.596358672111158 - val_loss: 1.3769998088296211 - sigma: 0.012092407172163281\n",
      "Epoch 1576 - loss: 1.5955416977957733 - val_loss: 1.4009007102637416 - sigma: 0.012085464195471051\n",
      "Epoch 1593 - loss: 1.5952857313262232 - val_loss: 1.403144468546826 - sigma: 0.012056023955848897\n",
      "Epoch 1622 - loss: 1.5938347316601513 - val_loss: 1.3986283019858845 - sigma: 0.01200603711628209\n",
      "Epoch 1629 - loss: 1.5937823952107624 - val_loss: 1.3748954056760472 - sigma: 0.01199401260248868\n",
      "Epoch 1633 - loss: 1.5937028424533741 - val_loss: 1.38380698764927 - sigma: 0.011987148297539425\n",
      "Epoch 1644 - loss: 1.5915169268302463 - val_loss: 1.3924132029686649 - sigma: 0.011968296458139396\n",
      "Epoch 1653 - loss: 1.5913598914583413 - val_loss: 1.3733040092466975 - sigma: 0.011952898744705764\n",
      "Epoch 1674 - loss: 1.5911105253247162 - val_loss: 1.3839582151365435 - sigma: 0.011917059437609156\n",
      "Epoch 1684 - loss: 1.5903941818182363 - val_loss: 1.3996405600198631 - sigma: 0.01190003495058716\n",
      "Epoch 1723 - loss: 1.5895801832616054 - val_loss: 1.3904912359092563 - sigma: 0.011833880834813721\n",
      "Epoch 1726 - loss: 1.5884808672574586 - val_loss: 1.3813423727497214 - sigma: 0.011828807267484245\n",
      "Epoch 1729 - loss: 1.5881152420526419 - val_loss: 1.382000212123493 - sigma: 0.011823735794952604\n",
      "Epoch 1775 - loss: 1.5878670115438884 - val_loss: 1.3748019962930833 - sigma: 0.011746226041635706\n",
      "Epoch 1810 - loss: 1.5876370116492957 - val_loss: 1.3981718214460415 - sigma: 0.011687552841918148\n",
      "Epoch 1812 - loss: 1.5875740010346473 - val_loss: 1.375844783391989 - sigma: 0.011684207602087401\n",
      "Epoch 1820 - loss: 1.5873384741452614 - val_loss: 1.3715612058858497 - sigma: 0.011670834590656284\n",
      "Epoch 1825 - loss: 1.5866331149586437 - val_loss: 1.3833668205017324 - sigma: 0.011662482872900703\n",
      "Epoch 1853 - loss: 1.5859117231046418 - val_loss: 1.3878842997037455 - sigma: 0.011615802883228629\n",
      "Epoch 1869 - loss: 1.5851928202557881 - val_loss: 1.3814435250064676 - sigma: 0.011589195619351204\n",
      "Epoch 1883 - loss: 1.5850100847157131 - val_loss: 1.3768406542145517 - sigma: 0.011565953443584498\n",
      "Epoch 1897 - loss: 1.5842235891281546 - val_loss: 1.378288114930649 - sigma: 0.011542747339131796\n",
      "Epoch 1898 - loss: 1.5840388443701963 - val_loss: 1.3859256452831556 - sigma: 0.011541091128977317\n",
      "Epoch 1904 - loss: 1.583495393369179 - val_loss: 1.4013362134224592 - sigma: 0.011531157678530515\n",
      "Epoch 1905 - loss: 1.582932193015664 - val_loss: 1.3995272162390635 - sigma: 0.011529502737194495\n",
      "Epoch 1922 - loss: 1.582559443017516 - val_loss: 1.3720973415217634 - sigma: 0.011501396269512692\n",
      "Epoch 1926 - loss: 1.5821176698174575 - val_loss: 1.3757602694458324 - sigma: 0.011494790501291706\n",
      "Epoch 1964 - loss: 1.5812411241267763 - val_loss: 1.392290895517245 - sigma: 0.011432176694915015\n",
      "Epoch 1971 - loss: 1.580603785491747 - val_loss: 1.3885178169693837 - sigma: 0.011420670128209976\n",
      "Epoch 1982 - loss: 1.5794488104089563 - val_loss: 1.384030470118163 - sigma: 0.011402605524610759\n",
      "Epoch 1993 - loss: 1.577974368250282 - val_loss: 1.3826956992555914 - sigma: 0.011384561774956793\n",
      "Epoch 2018 - loss: 1.576850958041556 - val_loss: 1.3573592285704201 - sigma: 0.011343630256164297\n",
      "Epoch 2054 - loss: 1.576610126430274 - val_loss: 1.3596245132404172 - sigma: 0.011284874937201218\n",
      "Epoch 2064 - loss: 1.5764977713274753 - val_loss: 1.3989373100645908 - sigma: 0.011268592658906345\n",
      "Epoch 2067 - loss: 1.5756463303422457 - val_loss: 1.3539974031498747 - sigma: 0.01126371123428181\n",
      "Epoch 2074 - loss: 1.57562928548838 - val_loss: 1.3674155437393423 - sigma: 0.011252327080928158\n",
      "Epoch 2077 - loss: 1.5753707985025813 - val_loss: 1.3656143832147631 - sigma: 0.011247450656633419\n",
      "Epoch 2081 - loss: 1.5750232761895548 - val_loss: 1.388021743159889 - sigma: 0.011240951086441629\n",
      "Epoch 2115 - loss: 1.5745367660291316 - val_loss: 1.376336131096717 - sigma: 0.011185811802598962\n",
      "Epoch 2123 - loss: 1.5740229184909769 - val_loss: 1.3687486230776138 - sigma: 0.011172865587259008\n",
      "Epoch 2126 - loss: 1.5734300425222865 - val_loss: 1.371486351446919 - sigma: 0.011168013470486854\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb Cell 9'\u001b[0m in \u001b[0;36m<cell line: 2>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000008?line=0'>1</a>\u001b[0m regressor \u001b[39m=\u001b[39m EvoMLPRegressor2(n \u001b[39m=\u001b[39m \u001b[39m240\u001b[39m, hidden_layer_sizes \u001b[39m=\u001b[39m [\u001b[39m16\u001b[39m], activation \u001b[39m=\u001b[39m \u001b[39m\"\u001b[39m\u001b[39mrelu\u001b[39m\u001b[39m\"\u001b[39m, random_state \u001b[39m=\u001b[39m \u001b[39m42\u001b[39m, lr_target \u001b[39m=\u001b[39m \u001b[39m0.002\u001b[39m, lr_initial_decay \u001b[39m=\u001b[39m \u001b[39m20\u001b[39m, lr_final_decay \u001b[39m=\u001b[39m \u001b[39m0.02\u001b[39m)\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000008?line=1'>2</a>\u001b[0m regressor\u001b[39m.\u001b[39;49mfit(scaled_X_train, y_train, epochs \u001b[39m=\u001b[39;49m \u001b[39m10000\u001b[39;49m, validation_data \u001b[39m=\u001b[39;49m (scaled_X_val, y_val), verbose \u001b[39m=\u001b[39;49m \u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb Cell 8'\u001b[0m in \u001b[0;36mEvoMLPRegressor2.fit\u001b[0;34m(self, X_train, y_train, epochs, validation_data, verbose)\u001b[0m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000007?line=70'>71</a>\u001b[0m \u001b[39m# Hidden layers\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000007?line=71'>72</a>\u001b[0m \u001b[39mfor\u001b[39;00m j \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(number_of_layers_minus_one \u001b[39m-\u001b[39m \u001b[39m1\u001b[39m):\n\u001b[0;32m---> <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000007?line=72'>73</a>\u001b[0m     forward_pass \u001b[39m=\u001b[39m activation_function(weights[j][sorted_indices[ndiv4:]]\u001b[39m.\u001b[39;49mtranspose(\u001b[39m0\u001b[39;49m, \u001b[39m2\u001b[39;49m, \u001b[39m1\u001b[39;49m) \u001b[39m@\u001b[39;49m forward_pass)\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000007?line=74'>75</a>\u001b[0m \u001b[39m# Output layer\u001b[39;00m\n\u001b[1;32m     <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000007?line=75'>76</a>\u001b[0m forward_pass \u001b[39m=\u001b[39m weights[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m][sorted_indices[ndiv4:]]\u001b[39m.\u001b[39mtranspose(\u001b[39m0\u001b[39m, \u001b[39m2\u001b[39m, \u001b[39m1\u001b[39m) \u001b[39m@\u001b[39m forward_pass\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "regressor = EvoMLPRegressor2(n = 240, hidden_layer_sizes = [16], activation = \"relu\", random_state = 42, lr_target = 0.002, lr_initial_decay = 20, lr_final_decay = 0.02)\n",
    "regressor.fit(scaled_X_train, y_train, epochs = 10000, validation_data = (scaled_X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EvoMLPRegressor3:\n",
    "\n",
    "    '''THIS IS THE ONE TO USE'''\n",
    "\n",
    "    def __init__(self, n = 24, hidden_layers = False, activation = \"relu\", lr_decay = 20, random_state = None):\n",
    "\n",
    "        self.n = int(round(n / 8) * 8)\n",
    "        self.validation_loss_history = []\n",
    "        self.training_loss_history = []\n",
    "        self.random_state = random_state\n",
    "        self.activation = activation\n",
    "        self.lr_decay = lr_decay\n",
    "        \n",
    "        if hidden_layers:\n",
    "            self.layers = hidden_layers + [1]\n",
    "        else:\n",
    "            self.layers = [1]\n",
    "\n",
    "    def fit(self, X_train, y_train, epochs = 100, validation_data = False, verbose = 0):\n",
    "\n",
    "        if self.random_state != None:\n",
    "            np.random.seed(self.random_state)\n",
    "\n",
    "        if validation_data:\n",
    "            X_val, y_val = validation_data\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        elif self.activation == \"relu\":\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        X_train = np.c_[np.ones(X_train.shape[0]), X_train]\n",
    "\n",
    "        n = self.n\n",
    "        ndiv4 = n // 4\n",
    "\n",
    "        lr_decay = self.lr_decay\n",
    "\n",
    "        layers = [X_train.shape[1]] + self.layers\n",
    "\n",
    "        number_of_layers_minus_one = len(layers) - 1\n",
    "\n",
    "        y_preds = np.zeros((n, y_train.shape[0]))\n",
    "\n",
    "        nets_loss = np.zeros(n)\n",
    "        sorted_indices = np.arange(-(ndiv4), n, 1)\n",
    "        \n",
    "        best_net_index = -1\n",
    "        \n",
    "        weights = []\n",
    "\n",
    "        for i in range(number_of_layers_minus_one):\n",
    "            weights += [np.random.normal(0, 2, (n, layers[i], layers[i + 1]))]\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            forward_pass = X_train.T\n",
    "            \n",
    "            for j in range(number_of_layers_minus_one - 1):\n",
    "                forward_pass = activation_function(weights[j][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass)\n",
    "\n",
    "            forward_pass = weights[-1][sorted_indices[ndiv4:]].transpose(0, 2, 1) @ forward_pass\n",
    "            \n",
    "            y_preds[sorted_indices[ndiv4:]] = forward_pass.reshape(*forward_pass.shape[::2])\n",
    "\n",
    "            nets_loss[sorted_indices[ndiv4:]] = np.mean(np.abs(y_preds[sorted_indices[ndiv4:]] - y_train), axis = 1)\n",
    "\n",
    "            sorted_indices = np.argsort(nets_loss)\n",
    "\n",
    "            mutation_sigma = math.exp(-epoch / (epochs / (lr_decay * math.log10(epochs + 1)))) + 0.02 * math.exp(-(epoch + 1) * (1 / (epochs + 1))) - 0.005\n",
    "            \n",
    "            for j in range(number_of_layers_minus_one):\n",
    "                weights[j][sorted_indices[0 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[1 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[2 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[3 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[4 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "                weights[j][sorted_indices[5 + ndiv4::6]] = (weights[j][sorted_indices[0: ndiv4: 2]] + weights[j][sorted_indices[1: ndiv4: 2]]) / 2 + np.random.normal(0, mutation_sigma, (ndiv4 // 2, layers[j], layers[j + 1]))\n",
    "\n",
    "            if best_net_index != sorted_indices[0]:\n",
    "                best_net_index = sorted_indices[0]\n",
    "                self.training_loss_history += [nets_loss[best_net_index]]\n",
    "\n",
    "                self.best_net_weights = []\n",
    "                for j in range(number_of_layers_minus_one):\n",
    "                    self.best_net_weights += [weights[j][best_net_index]]\n",
    "                \n",
    "                if validation_data:\n",
    "                    self.validation_loss_history += [np.mean(np.abs(y_val - self.predict(X_val)))]\n",
    "                    if verbose == 1:\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]} - val_loss: {self.validation_loss_history[-1]} - mutation_sigma: {mutation_sigma}\")\n",
    "                else:\n",
    "                    if verbose == 1:\n",
    "                        pass\n",
    "                        print(f\"Epoch {epoch} - loss: {self.training_loss_history[-1]}\")\n",
    "\n",
    "\n",
    "    def predict(self, X):\n",
    "        X = np.c_[np.ones(X.shape[0]), X]\n",
    "\n",
    "        if self.activation == \"sigmoid\":\n",
    "            activation_function = lambda x: 1 / (1 + np.exp(-x))\n",
    "        elif self.activation == \"leaky_relu\":\n",
    "            activation_function = lambda x: np.maximum(0.1 * x, x)\n",
    "        else:\n",
    "            activation_function = lambda x: np.maximum(0, x)\n",
    "\n",
    "        forward_pass = X.T\n",
    "        for j in range(len(self.best_net_weights) - 1):\n",
    "            forward_pass = activation_function(self.best_net_weights[j].T @ forward_pass)\n",
    "\n",
    "        forward_pass = self.best_net_weights[-1].T @ forward_pass\n",
    "        return forward_pass.ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0 - loss: 7.737057559160331 - val_loss: 8.01487563511419 - mutation_sigma: 1.0149980002999568\n",
      "Epoch 1 - loss: 7.510331828695556 - val_loss: 8.968397931066926 - mutation_sigma: 1.0070278294744286\n",
      "Epoch 2 - loss: 6.5947106040460755 - val_loss: 7.0458890972361505 - mutation_sigma: 0.9991211506030916\n",
      "Epoch 4 - loss: 5.829510974233065 - val_loss: 4.908348079817188 - mutation_sigma: 0.9834962491014118\n",
      "Epoch 5 - loss: 5.281573644652448 - val_loss: 4.607294742401278 - mutation_sigma: 0.9757770267061644\n",
      "Epoch 10 - loss: 5.112338479928962 - val_loss: 4.755319630232976 - mutation_sigma: 0.9380935589113272\n",
      "Epoch 11 - loss: 4.900885896319837 - val_loss: 5.699607911615603 - mutation_sigma: 0.9307360185968425\n",
      "Epoch 20 - loss: 4.733582000861924 - val_loss: 4.326982884667847 - mutation_sigma: 0.8671003569760438\n",
      "Epoch 25 - loss: 4.634677176188427 - val_loss: 4.266749060398398 - mutation_sigma: 0.8336770480449884\n",
      "Epoch 31 - loss: 4.596983524063516 - val_loss: 4.909666057905956 - mutation_sigma: 0.7952939508383862\n",
      "Epoch 32 - loss: 3.9444251938768815 - val_loss: 3.7948153497907677 - mutation_sigma: 0.7890739325449794\n",
      "Epoch 48 - loss: 3.913337989896924 - val_loss: 4.15412824898813 - mutation_sigma: 0.6960308369953331\n",
      "Epoch 53 - loss: 3.5760343636423015 - val_loss: 3.5443791654741332 - mutation_sigma: 0.6693131712285929\n",
      "Epoch 59 - loss: 3.505001437926961 - val_loss: 3.342708554591469 - mutation_sigma: 0.6386306877599001\n",
      "Epoch 68 - loss: 3.5000433360136887 - val_loss: 3.3229922460243655 - mutation_sigma: 0.5952809758259879\n",
      "Epoch 71 - loss: 3.4039652344390032 - val_loss: 2.491508669678962 - mutation_sigma: 0.581510658453911\n",
      "Epoch 73 - loss: 3.339316649783786 - val_loss: 2.528769459576549 - mutation_sigma: 0.5725122714739252\n",
      "Epoch 81 - loss: 3.1005799648925514 - val_loss: 2.8486087312841484 - mutation_sigma: 0.537923919886711\n",
      "Epoch 82 - loss: 3.0222522288176834 - val_loss: 2.542217669668907 - mutation_sigma: 0.5337538878243995\n",
      "Epoch 87 - loss: 2.7900852483096266 - val_loss: 2.5776638482262344 - mutation_sigma: 0.5133966451729272\n",
      "Epoch 105 - loss: 2.580664007832874 - val_loss: 2.2381378983115447 - mutation_sigma: 0.4464957269792484\n",
      "Epoch 133 - loss: 2.559722965143909 - val_loss: 2.278871614253666 - mutation_sigma: 0.35980258295186685\n",
      "Epoch 135 - loss: 2.3899918244105205 - val_loss: 2.1923572268733507 - mutation_sigma: 0.354321411860374\n",
      "Epoch 144 - loss: 2.389766900525124 - val_loss: 2.2600978971897896 - mutation_sigma: 0.3307122973896719\n",
      "Epoch 152 - loss: 2.3175483599393893 - val_loss: 1.9666911107254528 - mutation_sigma: 0.3111058985320412\n",
      "Epoch 155 - loss: 2.2213342418936124 - val_loss: 1.892180138493059 - mutation_sigma: 0.30407077385151215\n",
      "Epoch 156 - loss: 2.210068066118549 - val_loss: 1.9522225317515347 - mutation_sigma: 0.30176297311661315\n",
      "Epoch 159 - loss: 2.1817222405188232 - val_loss: 1.7836109597102072 - mutation_sigma: 0.29494922724001665\n",
      "Epoch 196 - loss: 2.1693622563992427 - val_loss: 1.8299758409572484 - mutation_sigma: 0.223068034541863\n",
      "Epoch 200 - loss: 2.161356898359588 - val_loss: 1.595117302756535 - mutation_sigma: 0.21649506346167388\n",
      "Epoch 208 - loss: 2.0681527193182765 - val_loss: 1.6537737262392518 - mutation_sigma: 0.20396290076620066\n",
      "Epoch 222 - loss: 2.0442635165005583 - val_loss: 1.6074319218259039 - mutation_sigma: 0.18386976588240353\n",
      "Epoch 223 - loss: 2.0241800940185986 - val_loss: 1.6661519453282914 - mutation_sigma: 0.18251871292664165\n",
      "Epoch 254 - loss: 2.023776672506467 - val_loss: 1.63546868534046 - mutation_sigma: 0.14556671831309415\n",
      "Epoch 255 - loss: 2.008583950839811 - val_loss: 1.7799745180328777 - mutation_sigma: 0.14452037897899978\n",
      "Epoch 257 - loss: 1.96214626183952 - val_loss: 1.7028261526232193 - mutation_sigma: 0.14245260022015568\n",
      "Epoch 287 - loss: 1.9562957160859948 - val_loss: 1.7390892167771905 - mutation_sigma: 0.11509044437857555\n",
      "Epoch 289 - loss: 1.9319396965167446 - val_loss: 1.7276494760785315 - mutation_sigma: 0.11348882652871345\n",
      "Epoch 310 - loss: 1.9309500390857222 - val_loss: 1.6945487480502677 - mutation_sigma: 0.0981286037403341\n",
      "Epoch 316 - loss: 1.9165329332248093 - val_loss: 1.6412047328107782 - mutation_sigma: 0.09419231204775427\n",
      "Epoch 325 - loss: 1.9160439059471277 - val_loss: 1.6794946658731753 - mutation_sigma: 0.08863005782048594\n",
      "Epoch 335 - loss: 1.9154285492142349 - val_loss: 1.7158586732435797 - mutation_sigma: 0.08290038843558935\n",
      "Epoch 338 - loss: 1.9151044388582628 - val_loss: 1.6958512782525275 - mutation_sigma: 0.08126869144624821\n",
      "Epoch 339 - loss: 1.8975118971703278 - val_loss: 1.6226258886088811 - mutation_sigma: 0.08073340675337005\n",
      "Epoch 348 - loss: 1.888355758967431 - val_loss: 1.6014800276604366 - mutation_sigma: 0.076103088402792\n",
      "Epoch 349 - loss: 1.8794305645636966 - val_loss: 1.6818342037238376 - mutation_sigma: 0.07560881209217199\n",
      "Epoch 353 - loss: 1.8711496975960402 - val_loss: 1.64841652285107 - mutation_sigma: 0.07367062833338604\n",
      "Epoch 364 - loss: 1.870995206111572 - val_loss: 1.7016134081101768 - mutation_sigma: 0.06864840075777043\n",
      "Epoch 369 - loss: 1.854219956480461 - val_loss: 1.5969568338314086 - mutation_sigma: 0.06650705108406649\n",
      "Epoch 379 - loss: 1.8532092059376457 - val_loss: 1.6204742064508082 - mutation_sigma: 0.062471848149197894\n",
      "Epoch 383 - loss: 1.8495022176064686 - val_loss: 1.537384180940168 - mutation_sigma: 0.06094559811434317\n",
      "Epoch 388 - loss: 1.8301210329013573 - val_loss: 1.5586326251503047 - mutation_sigma: 0.05910486604972901\n",
      "Epoch 400 - loss: 1.8291662439109284 - val_loss: 1.561813038150841 - mutation_sigma: 0.05497473219005386\n",
      "Epoch 402 - loss: 1.8225492666906704 - val_loss: 1.6198325371073756 - mutation_sigma: 0.05432390026163654\n",
      "Epoch 415 - loss: 1.8223668040710084 - val_loss: 1.5079992643264213 - mutation_sigma: 0.05033667656789271\n",
      "Epoch 419 - loss: 1.8180591447725498 - val_loss: 1.5916522063030418 - mutation_sigma: 0.0491904574455904\n",
      "Epoch 425 - loss: 1.8151129171360803 - val_loss: 1.5854997139467533 - mutation_sigma: 0.04753801233339985\n",
      "Epoch 427 - loss: 1.8069069448466806 - val_loss: 1.5124144206687609 - mutation_sigma: 0.04700447052065555\n",
      "Epoch 431 - loss: 1.806438573789528 - val_loss: 1.6009515284457183 - mutation_sigma: 0.0459624797293419\n",
      "Epoch 440 - loss: 1.804257465046809 - val_loss: 1.5614015536611947 - mutation_sigma: 0.0437355537412688\n",
      "Epoch 442 - loss: 1.801706834485771 - val_loss: 1.6365161345341235 - mutation_sigma: 0.04326191759021416\n",
      "Epoch 446 - loss: 1.8009563614907615 - val_loss: 1.5822342711172328 - mutation_sigma: 0.04233690086306476\n",
      "Epoch 448 - loss: 1.7985136144097837 - val_loss: 1.6036817457436192 - mutation_sigma: 0.041885285430947085\n",
      "Epoch 449 - loss: 1.7959964813126112 - val_loss: 1.526056797692123 - mutation_sigma: 0.04166215049038336\n",
      "Epoch 450 - loss: 1.7897790210188556 - val_loss: 1.541225200561767 - mutation_sigma: 0.0414407784840066\n",
      "Epoch 453 - loss: 1.7886037343697552 - val_loss: 1.5767297696738587 - mutation_sigma: 0.040787100170129174\n",
      "Epoch 466 - loss: 1.7881156066846802 - val_loss: 1.5935755067461455 - mutation_sigma: 0.03812745897501384\n",
      "Epoch 470 - loss: 1.7799616539797742 - val_loss: 1.5998823158004296 - mutation_sigma: 0.03736271964351303\n",
      "Epoch 471 - loss: 1.7788361095724452 - val_loss: 1.5193445797981255 - mutation_sigma: 0.037175290677730455\n",
      "Epoch 479 - loss: 1.7783441329002132 - val_loss: 1.5657429105747838 - mutation_sigma: 0.035728105653352056\n",
      "Epoch 481 - loss: 1.7769695411887707 - val_loss: 1.5367208702342885 - mutation_sigma: 0.03538040317591349\n",
      "Epoch 488 - loss: 1.7768256413790464 - val_loss: 1.5169342520884654 - mutation_sigma: 0.034205871183812124\n",
      "Epoch 495 - loss: 1.776259104217378 - val_loss: 1.6172353922848757 - mutation_sigma: 0.03309458896676531\n",
      "Epoch 496 - loss: 1.7677635434774952 - val_loss: 1.5477715917280654 - mutation_sigma: 0.03294079439253083\n",
      "Epoch 511 - loss: 1.764155044742701 - val_loss: 1.5406418243586515 - mutation_sigma: 0.03077387107931785\n",
      "Epoch 515 - loss: 1.7631299917064658 - val_loss: 1.5631614281014312 - mutation_sigma: 0.030238059326832798\n",
      "Epoch 520 - loss: 1.7623378758170634 - val_loss: 1.570258071146122 - mutation_sigma: 0.02959163070501022\n",
      "Epoch 521 - loss: 1.7599696061276071 - val_loss: 1.5707671051647425 - mutation_sigma: 0.029465374433176244\n",
      "Epoch 523 - loss: 1.7582424283285596 - val_loss: 1.5680381689850313 - mutation_sigma: 0.029215827282581906\n",
      "Epoch 524 - loss: 1.75402615743999 - val_loss: 1.5680926674056992 - mutation_sigma: 0.029092520675272824\n",
      "Epoch 540 - loss: 1.7489373971185744 - val_loss: 1.5329714605588098 - mutation_sigma: 0.027246109625623186\n",
      "Epoch 554 - loss: 1.7466804076050904 - val_loss: 1.5439246282243797 - mutation_sigma: 0.02581045784175193\n",
      "Epoch 559 - loss: 1.7466118735383698 - val_loss: 1.5732276844934583 - mutation_sigma: 0.02533477804341309\n",
      "Epoch 560 - loss: 1.7460381342113067 - val_loss: 1.514554438737896 - mutation_sigma: 0.025241859729949776\n",
      "Epoch 561 - loss: 1.742950017805513 - val_loss: 1.5361445369192621 - mutation_sigma: 0.02514966692831948\n",
      "Epoch 564 - loss: 1.7412651519751683 - val_loss: 1.5111243194623012 - mutation_sigma: 0.024877384029169055\n",
      "Epoch 566 - loss: 1.7403107512094358 - val_loss: 1.559605891963243 - mutation_sigma: 0.024699384918770537\n",
      "Epoch 569 - loss: 1.739605461935401 - val_loss: 1.530268586265701 - mutation_sigma: 0.024437558978519253\n",
      "Epoch 573 - loss: 1.7393697104844608 - val_loss: 1.5512218096661392 - mutation_sigma: 0.02409788486037692\n",
      "Epoch 574 - loss: 1.739205797302013 - val_loss: 1.5453294240095399 - mutation_sigma: 0.02401461417810461\n",
      "Epoch 575 - loss: 1.7360637365025036 - val_loss: 1.5461656595121493 - mutation_sigma: 0.02393199215449855\n",
      "Epoch 578 - loss: 1.735630613300042 - val_loss: 1.4842882173835465 - mutation_sigma: 0.023687966569832613\n",
      "Epoch 580 - loss: 1.7333939767503346 - val_loss: 1.531364632105483 - mutation_sigma: 0.023528432500799234\n",
      "Epoch 582 - loss: 1.7323768692986115 - val_loss: 1.534465452545016 - mutation_sigma: 0.02337137153893803\n",
      "Epoch 583 - loss: 1.7321788366233208 - val_loss: 1.499531703454028 - mutation_sigma: 0.02329375618517539\n",
      "Epoch 585 - loss: 1.7307067744682159 - val_loss: 1.498881528236468 - mutation_sigma: 0.0231403314976806\n",
      "Epoch 587 - loss: 1.7294842420149017 - val_loss: 1.503549936772614 - mutation_sigma: 0.022989282973622068\n",
      "Epoch 591 - loss: 1.7292711298550967 - val_loss: 1.5176654613723755 - mutation_sigma: 0.022694164195629783\n",
      "Epoch 594 - loss: 1.726997768850053 - val_loss: 1.5492679058088765 - mutation_sigma: 0.022478786440017794\n",
      "Epoch 595 - loss: 1.7261082044570486 - val_loss: 1.5241542283617706 - mutation_sigma: 0.022408105338765676\n",
      "Epoch 599 - loss: 1.7256136755329385 - val_loss: 1.4974519908053685 - mutation_sigma: 0.02213082116586163\n",
      "Epoch 600 - loss: 1.7252279364529362 - val_loss: 1.5049623990735275 - mutation_sigma: 0.022062838600181354\n",
      "Epoch 601 - loss: 1.7243256058350935 - val_loss: 1.5053657713100879 - mutation_sigma: 0.02199538291340768\n",
      "Epoch 605 - loss: 1.7212225662129799 - val_loss: 1.5290890937073818 - mutation_sigma: 0.02173074551979345\n",
      "Epoch 610 - loss: 1.7209217416447282 - val_loss: 1.490496839719547 - mutation_sigma: 0.021411309727809417\n",
      "Epoch 611 - loss: 1.7199605413872066 - val_loss: 1.522133873930876 - mutation_sigma: 0.021348897437936925\n",
      "Epoch 619 - loss: 1.718658215655511 - val_loss: 1.4796551888575085 - mutation_sigma: 0.02086665048459224\n",
      "Epoch 620 - loss: 1.7158997173797115 - val_loss: 1.513453916780404 - mutation_sigma: 0.020808444972335093\n",
      "Epoch 625 - loss: 1.715151838834765 - val_loss: 1.5296853566272057 - mutation_sigma: 0.02052408136109617\n",
      "Epoch 626 - loss: 1.7143720658833508 - val_loss: 1.505212338636334 - mutation_sigma: 0.020468516791157118\n",
      "Epoch 635 - loss: 1.7109478138702108 - val_loss: 1.5238941688419954 - mutation_sigma: 0.019987290863024098\n",
      "Epoch 638 - loss: 1.7096352101859185 - val_loss: 1.4890912539036238 - mutation_sigma: 0.019834167777814767\n",
      "Epoch 642 - loss: 1.708440954604318 - val_loss: 1.510154624032653 - mutation_sigma: 0.01963543285216051\n",
      "Epoch 644 - loss: 1.7081257296126242 - val_loss: 1.4958803411039603 - mutation_sigma: 0.01953833699551024\n",
      "Epoch 647 - loss: 1.7063782658958269 - val_loss: 1.487920463658274 - mutation_sigma: 0.019395465369031677\n",
      "Epoch 648 - loss: 1.705653300556553 - val_loss: 1.4954504084561548 - mutation_sigma: 0.01934856900007133\n",
      "Epoch 651 - loss: 1.705333518700015 - val_loss: 1.5066961285634057 - mutation_sigma: 0.01921002207031661\n",
      "Epoch 653 - loss: 1.7039811355868277 - val_loss: 1.5104444961734165 - mutation_sigma: 0.01911941429618765\n",
      "Epoch 654 - loss: 1.7035523053925126 - val_loss: 1.512511728638961 - mutation_sigma: 0.019074629090329403\n",
      "Epoch 659 - loss: 1.7034174007308667 - val_loss: 1.539003342162316 - mutation_sigma: 0.018855780666425934\n",
      "Epoch 661 - loss: 1.7030351033931648 - val_loss: 1.5126744468832705 - mutation_sigma: 0.018770560891268735\n",
      "Epoch 662 - loss: 1.7027367548497458 - val_loss: 1.5060440586333737 - mutation_sigma: 0.018728437546110508\n",
      "Epoch 665 - loss: 1.698695239222831 - val_loss: 1.5155143808052176 - mutation_sigma: 0.018603982827019933\n",
      "Epoch 672 - loss: 1.6974853076129377 - val_loss: 1.4745067193513004 - mutation_sigma: 0.01832443964259058\n",
      "Epoch 676 - loss: 1.6972898747608398 - val_loss: 1.5467003670344692 - mutation_sigma: 0.018171271776579727\n",
      "Epoch 678 - loss: 1.697168381051914 - val_loss: 1.505829908194616 - mutation_sigma: 0.018096418735436857\n",
      "Epoch 680 - loss: 1.696914368875435 - val_loss: 1.5008701796143022 - mutation_sigma: 0.01802269524895053\n",
      "Epoch 683 - loss: 1.696352378010625 - val_loss: 1.5062800666303593 - mutation_sigma: 0.017914188817473573\n",
      "Epoch 685 - loss: 1.6960374554833368 - val_loss: 1.4962515551630509 - mutation_sigma: 0.017843211449281363\n",
      "Epoch 690 - loss: 1.6947684487633414 - val_loss: 1.4970305297276991 - mutation_sigma: 0.01767040388218275\n",
      "Epoch 692 - loss: 1.6946052096226003 - val_loss: 1.5462865639226144 - mutation_sigma: 0.017603091252803977\n",
      "Epoch 693 - loss: 1.6935893663757093 - val_loss: 1.5517177426951598 - mutation_sigma: 0.01756981467610096\n",
      "Epoch 701 - loss: 1.6930717643274076 - val_loss: 1.509371985665412 - mutation_sigma: 0.01731245354470898\n",
      "Epoch 706 - loss: 1.692434030765556 - val_loss: 1.5255420358341263 - mutation_sigma: 0.017159301919106376\n",
      "Epoch 710 - loss: 1.692142700935924 - val_loss: 1.5159898515246275 - mutation_sigma: 0.017040855386663206\n",
      "Epoch 712 - loss: 1.6912990209535448 - val_loss: 1.5087313671464395 - mutation_sigma: 0.016982951067119872\n",
      "Epoch 715 - loss: 1.6911185376158928 - val_loss: 1.5056467992673657 - mutation_sigma: 0.01689770418024372\n",
      "Epoch 716 - loss: 1.6910082086417622 - val_loss: 1.495912258256612 - mutation_sigma: 0.016869710964267835\n",
      "Epoch 718 - loss: 1.6892950900791364 - val_loss: 1.50273339224132 - mutation_sigma: 0.016814348096532466\n",
      "Epoch 720 - loss: 1.68875171111645 - val_loss: 1.4964446907206912 - mutation_sigma: 0.016759805652651893\n",
      "Epoch 726 - loss: 1.6872407691104763 - val_loss: 1.4929733915197025 - mutation_sigma: 0.0166009717854485\n",
      "Epoch 730 - loss: 1.6855010710272462 - val_loss: 1.4965040210249405 - mutation_sigma: 0.016498951572685366\n",
      "Epoch 737 - loss: 1.6850272143135603 - val_loss: 1.4911761178813705 - mutation_sigma: 0.016327534510095217\n",
      "Epoch 745 - loss: 1.6843038397447672 - val_loss: 1.4980112666756435 - mutation_sigma: 0.016142176660412812\n",
      "Epoch 747 - loss: 1.6837646923798686 - val_loss: 1.4920001984160052 - mutation_sigma: 0.016097517008078998\n",
      "Epoch 748 - loss: 1.6815400839026688 - val_loss: 1.5079980592456197 - mutation_sigma: 0.01607543184392674\n",
      "Epoch 754 - loss: 1.6791279835449475 - val_loss: 1.4929807109396844 - mutation_sigma: 0.01594626520299637\n",
      "Epoch 764 - loss: 1.678606209189609 - val_loss: 1.5009182064445405 - mutation_sigma: 0.015743167103918\n",
      "Epoch 769 - loss: 1.6775822478516285 - val_loss: 1.5160291855355896 - mutation_sigma: 0.015647016507831877\n",
      "Epoch 772 - loss: 1.6754733956587036 - val_loss: 1.5163537531580165 - mutation_sigma: 0.015590972426211946\n",
      "Epoch 781 - loss: 1.6751812120523104 - val_loss: 1.5171871909867927 - mutation_sigma: 0.015429921365728457\n",
      "Epoch 788 - loss: 1.675114926767903 - val_loss: 1.5182156300807688 - mutation_sigma: 0.015311641530773128\n",
      "Epoch 789 - loss: 1.673824369201371 - val_loss: 1.4987808326893184 - mutation_sigma: 0.015295220946124573\n",
      "Epoch 792 - loss: 1.6730280152906596 - val_loss: 1.4862613681490555 - mutation_sigma: 0.015246653308166064\n",
      "Epoch 794 - loss: 1.6709539137333858 - val_loss: 1.4939968609559666 - mutation_sigma: 0.015214844152080405\n",
      "Epoch 808 - loss: 1.6705289085350614 - val_loss: 1.4701504725041372 - mutation_sigma: 0.015004307176521705\n",
      "Epoch 809 - loss: 1.6704686532500106 - val_loss: 1.4720747509752061 - mutation_sigma: 0.01499004495223458\n",
      "Epoch 812 - loss: 1.6694183624147694 - val_loss: 1.4768696377773085 - mutation_sigma: 0.014947849926667606\n",
      "Epoch 821 - loss: 1.6670721445570622 - val_loss: 1.474692828512551 - mutation_sigma: 0.014826409523965009\n",
      "Epoch 823 - loss: 1.6658819020704358 - val_loss: 1.4741190159945885 - mutation_sigma: 0.014800432402400174\n",
      "Epoch 826 - loss: 1.6656589624389269 - val_loss: 1.4793079871249442 - mutation_sigma: 0.014762129831800377\n",
      "Epoch 837 - loss: 1.6648559849230902 - val_loss: 1.4755556077612546 - mutation_sigma: 0.014628212898870174\n",
      "Epoch 840 - loss: 1.6639619720958985 - val_loss: 1.4712610586091115 - mutation_sigma: 0.014593391160156986\n",
      "Epoch 841 - loss: 1.6638914869138612 - val_loss: 1.4884123433153642 - mutation_sigma: 0.014581939540008717\n",
      "Epoch 847 - loss: 1.6638781942467098 - val_loss: 1.4652805688444184 - mutation_sigma: 0.01451482107783781\n",
      "Epoch 849 - loss: 1.6613669182008606 - val_loss: 1.4943225371740951 - mutation_sigma: 0.01449304011937546\n",
      "Epoch 850 - loss: 1.6608108092817093 - val_loss: 1.4844783067472644 - mutation_sigma: 0.0144822579804051\n",
      "Epoch 861 - loss: 1.659905797090408 - val_loss: 1.4565844985810863 - mutation_sigma: 0.014368248195888535\n",
      "Epoch 864 - loss: 1.6590084626552613 - val_loss: 1.4701893675523645 - mutation_sigma: 0.014338559178370486\n",
      "Epoch 869 - loss: 1.6587444026080567 - val_loss: 1.468069603491669 - mutation_sigma: 0.014290348991362719\n",
      "Epoch 871 - loss: 1.6583405134688463 - val_loss: 1.4645905686395646 - mutation_sigma: 0.0142714982597059\n",
      "Epoch 875 - loss: 1.6568021067649628 - val_loss: 1.487832968400229 - mutation_sigma: 0.014234518244266258\n",
      "Epoch 880 - loss: 1.6560872098007888 - val_loss: 1.4688288529518012 - mutation_sigma: 0.014189607030501261\n",
      "Epoch 883 - loss: 1.655189336770723 - val_loss: 1.494473412397927 - mutation_sigma: 0.014163338984989744\n",
      "Epoch 893 - loss: 1.654787685576 - val_loss: 1.486376028497698 - mutation_sigma: 0.014079283846554307\n",
      "Epoch 899 - loss: 1.6535480784899257 - val_loss: 1.4943725310453781 - mutation_sigma: 0.014031311885722604\n",
      "Epoch 904 - loss: 1.6515206150841117 - val_loss: 1.4689475165166879 - mutation_sigma: 0.013992668499623419\n",
      "Epoch 927 - loss: 1.6514359646964234 - val_loss: 1.4753617425050969 - mutation_sigma: 0.013829186389930713\n",
      "Epoch 932 - loss: 1.651246206537777 - val_loss: 1.4598430881910627 - mutation_sigma: 0.013796490250640139\n",
      "Epoch 937 - loss: 1.651135997566617 - val_loss: 1.4942517331738099 - mutation_sigma: 0.013764723475059835\n",
      "Epoch 944 - loss: 1.6508516698489393 - val_loss: 1.4582764362354674 - mutation_sigma: 0.013721742543775648\n",
      "Epoch 945 - loss: 1.6497937807448522 - val_loss: 1.4757123482655345 - mutation_sigma: 0.01371573972730276\n",
      "Epoch 957 - loss: 1.6495275162810914 - val_loss: 1.4805903245828849 - mutation_sigma: 0.013646245706365315\n",
      "Epoch 958 - loss: 1.6489772616657223 - val_loss: 1.4899849088977717 - mutation_sigma: 0.013640658473338535\n",
      "Epoch 966 - loss: 1.6484735653856282 - val_loss: 1.4783408855616238 - mutation_sigma: 0.013597028775399662\n",
      "Epoch 970 - loss: 1.6478457331618466 - val_loss: 1.4461332716670796 - mutation_sigma: 0.01357590196808647\n",
      "Epoch 975 - loss: 1.6475169451201648 - val_loss: 1.4829689833208737 - mutation_sigma: 0.013550110031756053\n",
      "Epoch 978 - loss: 1.645181907660707 - val_loss: 1.45997006650999 - mutation_sigma: 0.013534953413423218\n",
      "Epoch 987 - loss: 1.6443724092825462 - val_loss: 1.4672106316173699 - mutation_sigma: 0.013490854063524276\n",
      "Epoch 999 - loss: 1.6432171060057628 - val_loss: 1.4412142395130065 - mutation_sigma: 0.013435057063585856\n",
      "Epoch 1005 - loss: 1.6423787383507042 - val_loss: 1.429135197763711 - mutation_sigma: 0.013408356312654635\n",
      "Epoch 1007 - loss: 1.641840988642151 - val_loss: 1.4411689886500796 - mutation_sigma: 0.013399624305504943\n",
      "Epoch 1020 - loss: 1.641001476442525 - val_loss: 1.4575943784928775 - mutation_sigma: 0.013344806498173978\n",
      "Epoch 1035 - loss: 1.640139760144246 - val_loss: 1.462198376644603 - mutation_sigma: 0.01328541839330347\n",
      "Epoch 1036 - loss: 1.6386647865005535 - val_loss: 1.4711522295487387 - mutation_sigma: 0.013281595427096476\n",
      "Epoch 1050 - loss: 1.6383806888283725 - val_loss: 1.4565238873397193 - mutation_sigma: 0.013229725921898808\n",
      "Epoch 1055 - loss: 1.6367983936898376 - val_loss: 1.451633576709871 - mutation_sigma: 0.01321191016863774\n",
      "Epoch 1057 - loss: 1.6354518655548598 - val_loss: 1.444876708338808 - mutation_sigma: 0.013204882694777102\n",
      "Epoch 1073 - loss: 1.6348436148887138 - val_loss: 1.4460860387417547 - mutation_sigma: 0.013150577344193577\n",
      "Epoch 1083 - loss: 1.6332801254750242 - val_loss: 1.4365001464036904 - mutation_sigma: 0.013118242764930808\n",
      "Epoch 1091 - loss: 1.6327434693733198 - val_loss: 1.4447669509949805 - mutation_sigma: 0.013093188331085973\n",
      "Epoch 1098 - loss: 1.6325507871019767 - val_loss: 1.4624848293839974 - mutation_sigma: 0.013071820964650639\n",
      "Epoch 1099 - loss: 1.6323963909816246 - val_loss: 1.4352683463359852 - mutation_sigma: 0.013068809044040102\n",
      "Epoch 1105 - loss: 1.6320523203536097 - val_loss: 1.442748436420223 - mutation_sigma: 0.013050942789699774\n",
      "Epoch 1112 - loss: 1.6310990515587733 - val_loss: 1.4348160708474755 - mutation_sigma: 0.013030527636103031\n",
      "Epoch 1117 - loss: 1.6309782387432177 - val_loss: 1.4450676056125766 - mutation_sigma: 0.013016215104671297\n",
      "Epoch 1122 - loss: 1.6309394872736378 - val_loss: 1.4402154068485826 - mutation_sigma: 0.013002117561733792\n",
      "Epoch 1124 - loss: 1.6308117877416752 - val_loss: 1.4469543973769583 - mutation_sigma: 0.012996536880786346\n",
      "Epoch 1126 - loss: 1.6302946005750518 - val_loss: 1.4542674355530585 - mutation_sigma: 0.012990988759510606\n",
      "Epoch 1129 - loss: 1.6286865310588001 - val_loss: 1.4531830846577525 - mutation_sigma: 0.012982726523150974\n",
      "Epoch 1135 - loss: 1.6282664885717237 - val_loss: 1.4463200020301596 - mutation_sigma: 0.012966411769711566\n",
      "Epoch 1145 - loss: 1.628251488607008 - val_loss: 1.4497247983174926 - mutation_sigma: 0.01293981203753684\n",
      "Epoch 1147 - loss: 1.6282268631856889 - val_loss: 1.4451317659005825 - mutation_sigma: 0.012934576749867008\n",
      "Epoch 1160 - loss: 1.6274908659627139 - val_loss: 1.463378420713023 - mutation_sigma: 0.012901192059498724\n",
      "Epoch 1164 - loss: 1.6274720500515067 - val_loss: 1.4335168875937137 - mutation_sigma: 0.012891133861854226\n",
      "Epoch 1166 - loss: 1.627306901284965 - val_loss: 1.4571554896967558 - mutation_sigma: 0.012886140703546739\n",
      "Epoch 1173 - loss: 1.6264614606280987 - val_loss: 1.4467500185857407 - mutation_sigma: 0.01286884710881767\n",
      "Epoch 1175 - loss: 1.6262930171581862 - val_loss: 1.457137721251174 - mutation_sigma: 0.012863956751731565\n",
      "Epoch 1179 - loss: 1.6251811909203329 - val_loss: 1.4580910320000475 - mutation_sigma: 0.0128542413629478\n",
      "Epoch 1188 - loss: 1.625127579994084 - val_loss: 1.439401800284108 - mutation_sigma: 0.012832688169937396\n",
      "Epoch 1191 - loss: 1.6248521605043231 - val_loss: 1.4417814853160995 - mutation_sigma: 0.012825594250136536\n",
      "Epoch 1197 - loss: 1.6236293096015841 - val_loss: 1.4303360765146633 - mutation_sigma: 0.012811535976500098\n",
      "Epoch 1204 - loss: 1.6233245589852225 - val_loss: 1.4389355433959061 - mutation_sigma: 0.012795344235722265\n",
      "Epoch 1209 - loss: 1.6227922534211265 - val_loss: 1.4363265762701938 - mutation_sigma: 0.012783910656181736\n",
      "Epoch 1217 - loss: 1.6201124649432055 - val_loss: 1.4275008378065004 - mutation_sigma: 0.012765834242878808\n",
      "Epoch 1243 - loss: 1.6200211047631083 - val_loss: 1.436108879588107 - mutation_sigma: 0.01270876054569801\n",
      "Epoch 1244 - loss: 1.619855971261856 - val_loss: 1.428986994191229 - mutation_sigma: 0.012706612186151395\n",
      "Epoch 1248 - loss: 1.6193780821707704 - val_loss: 1.437371191651899 - mutation_sigma: 0.012698050753598901\n",
      "Epoch 1258 - loss: 1.6185370437239284 - val_loss: 1.436711970050596 - mutation_sigma: 0.012676862953887359\n",
      "Epoch 1273 - loss: 1.6183982544676203 - val_loss: 1.4366507717559354 - mutation_sigma: 0.012645618942979057\n",
      "Epoch 1275 - loss: 1.6181907776349702 - val_loss: 1.4130167894093848 - mutation_sigma: 0.012641498627954616\n",
      "Epoch 1279 - loss: 1.617765484813033 - val_loss: 1.4266342654312276 - mutation_sigma: 0.01263328850370533\n",
      "Epoch 1282 - loss: 1.6176321458578218 - val_loss: 1.4223267805951552 - mutation_sigma: 0.012627157013750254\n",
      "Epoch 1283 - loss: 1.6157048372481142 - val_loss: 1.4256702966476502 - mutation_sigma: 0.01262511805800207\n",
      "Epoch 1319 - loss: 1.6150821890880886 - val_loss: 1.445944149722168 - mutation_sigma: 0.012553189385673378\n",
      "Epoch 1338 - loss: 1.614904506640372 - val_loss: 1.4349248220355644 - mutation_sigma: 0.012516237105358992\n",
      "Epoch 1340 - loss: 1.6144670567326984 - val_loss: 1.406961305451305 - mutation_sigma: 0.012512382665557716\n",
      "Epoch 1353 - loss: 1.6133852364182015 - val_loss: 1.429062068634201 - mutation_sigma: 0.012487479793246704\n",
      "Epoch 1356 - loss: 1.6124694867396714 - val_loss: 1.4370409118333178 - mutation_sigma: 0.012481768595814981\n",
      "Epoch 1368 - loss: 1.6115182974636515 - val_loss: 1.4165738770494756 - mutation_sigma: 0.012459048878826453\n",
      "Epoch 1394 - loss: 1.611393417969238 - val_loss: 1.4147221352571573 - mutation_sigma: 0.012410447942107428\n",
      "Epoch 1404 - loss: 1.61108893014879 - val_loss: 1.4133569434618303 - mutation_sigma: 0.01239195937753013\n",
      "Epoch 1408 - loss: 1.6100370515085605 - val_loss: 1.4248052841520882 - mutation_sigma: 0.012384592937248091\n",
      "Epoch 1409 - loss: 1.608086364130082 - val_loss: 1.4044837241798498 - mutation_sigma: 0.01238275383001582\n",
      "Epoch 1450 - loss: 1.607722106586941 - val_loss: 1.4074509710703471 - mutation_sigma: 0.01230813181865854\n",
      "Epoch 1451 - loss: 1.6070694531804026 - val_loss: 1.414723272242064 - mutation_sigma: 0.01230632915365721\n",
      "Epoch 1460 - loss: 1.6070412894063324 - val_loss: 1.4066603490026868 - mutation_sigma: 0.012290138585453119\n",
      "Epoch 1464 - loss: 1.6069294910239662 - val_loss: 1.4129183863061507 - mutation_sigma: 0.012282961541032613\n",
      "Epoch 1465 - loss: 1.6055211443740645 - val_loss: 1.4159526710953128 - mutation_sigma: 0.012281169033377289\n",
      "Epoch 1471 - loss: 1.6045739276717523 - val_loss: 1.4153331542996632 - mutation_sigma: 0.01227042839459164\n",
      "Epoch 1473 - loss: 1.604516315891886 - val_loss: 1.4217820026536305 - mutation_sigma: 0.012266853572353006\n",
      "Epoch 1480 - loss: 1.6044840917131065 - val_loss: 1.3992090312734642 - mutation_sigma: 0.012254362298958286\n",
      "Epoch 1482 - loss: 1.603666870503361 - val_loss: 1.4206456593829184 - mutation_sigma: 0.01225079912482687\n",
      "Epoch 1485 - loss: 1.6034587499871158 - val_loss: 1.4241802793164449 - mutation_sigma: 0.012245459053295913\n",
      "Epoch 1493 - loss: 1.6023167973320187 - val_loss: 1.4300243671520931 - mutation_sigma: 0.012231245695648894\n",
      "Epoch 1509 - loss: 1.6014957749380245 - val_loss: 1.4140323587790558 - mutation_sigma: 0.012202930253318877\n",
      "Epoch 1518 - loss: 1.5991551267515909 - val_loss: 1.4068589005018342 - mutation_sigma: 0.012187064136710241\n",
      "Epoch 1547 - loss: 1.5980713642715354 - val_loss: 1.3840300773448924 - mutation_sigma: 0.012136212791422102\n",
      "Epoch 1572 - loss: 1.597111293194125 - val_loss: 1.4104749087647837 - mutation_sigma: 0.012092675956640847\n",
      "Epoch 1587 - loss: 1.5962530980488563 - val_loss: 1.412116405493275 - mutation_sigma: 0.012066673379212082\n",
      "Epoch 1606 - loss: 1.5952481782406374 - val_loss: 1.4135322271993132 - mutation_sigma: 0.012033854621601989\n",
      "Epoch 1634 - loss: 1.594782902662078 - val_loss: 1.4034583696954228 - mutation_sigma: 0.011985710639848587\n",
      "Epoch 1646 - loss: 1.5946893369057102 - val_loss: 1.4008482790733097 - mutation_sigma: 0.011965152066497654\n",
      "Epoch 1657 - loss: 1.5942859059416725 - val_loss: 1.3882723505649213 - mutation_sigma: 0.011946343682561832\n",
      "Epoch 1666 - loss: 1.5935303032250272 - val_loss: 1.4055526184665659 - mutation_sigma: 0.011930980389927213\n",
      "Epoch 1671 - loss: 1.5930591643789664 - val_loss: 1.411103491493929 - mutation_sigma: 0.011922454839179714\n",
      "Epoch 1679 - loss: 1.5921838656688483 - val_loss: 1.4127802192663867 - mutation_sigma: 0.011908827919633935\n",
      "Epoch 1685 - loss: 1.5915696852003385 - val_loss: 1.402539713195365 - mutation_sigma: 0.011898618796209355\n",
      "Epoch 1701 - loss: 1.5905656169437652 - val_loss: 1.4025104815412899 - mutation_sigma: 0.011871439538823577\n",
      "Epoch 1712 - loss: 1.5902763229877228 - val_loss: 1.4044477775034891 - mutation_sigma: 0.011852790713706013\n",
      "Epoch 1716 - loss: 1.5901276282991317 - val_loss: 1.3749002354507798 - mutation_sigma: 0.011846016583358092\n",
      "Epoch 1717 - loss: 1.5897342958489857 - val_loss: 1.3987692244928955 - mutation_sigma: 0.011844323647871587\n",
      "Epoch 1726 - loss: 1.5882287969133588 - val_loss: 1.3942791602441922 - mutation_sigma: 0.011829097857038447\n",
      "Epoch 1728 - loss: 1.5877148353146349 - val_loss: 1.3963206044069998 - mutation_sigma: 0.01182571692182844\n",
      "Epoch 1785 - loss: 1.587364776806824 - val_loss: 1.3965704843945712 - mutation_sigma: 0.01172973528085168\n",
      "Epoch 1794 - loss: 1.5872908109002555 - val_loss: 1.405288577911579 - mutation_sigma: 0.011714643710036072\n",
      "Epoch 1796 - loss: 1.5867100506575598 - val_loss: 1.3938844128442083 - mutation_sigma: 0.011711292285716015\n",
      "Epoch 1818 - loss: 1.5865965442565646 - val_loss: 1.3958038407687527 - mutation_sigma: 0.01167447992257397\n",
      "Epoch 1820 - loss: 1.5864597096736035 - val_loss: 1.391565936310267 - mutation_sigma: 0.011671138130315974\n",
      "Epoch 1822 - loss: 1.5862017395467638 - val_loss: 1.3924337237143682 - mutation_sigma: 0.011667797126331217\n",
      "Epoch 1826 - loss: 1.585954787361674 - val_loss: 1.3810002157590735 - mutation_sigma: 0.011661117474959577\n",
      "Epoch 1837 - loss: 1.5858652674646028 - val_loss: 1.393871992174292 - mutation_sigma: 0.011642764519349709\n",
      "Epoch 1844 - loss: 1.584520582251103 - val_loss: 1.4031067610714203 - mutation_sigma: 0.011631097539784876\n",
      "Epoch 1871 - loss: 1.58420216043585 - val_loss: 1.4134689494418264 - mutation_sigma: 0.011586183536188687\n",
      "Epoch 1873 - loss: 1.5834533558429873 - val_loss: 1.4012033178630017 - mutation_sigma: 0.011582862013336793\n",
      "Epoch 1897 - loss: 1.5809171113227212 - val_loss: 1.3948385266069354 - mutation_sigma: 0.01154306128719261\n",
      "Epoch 1952 - loss: 1.5800992610312556 - val_loss: 1.3947638620959 - mutation_sigma: 0.011452243357466245\n",
      "Epoch 1964 - loss: 1.5796429133793373 - val_loss: 1.382650600873259 - mutation_sigma: 0.011432499555124289\n",
      "Epoch 1990 - loss: 1.578875380900198 - val_loss: 1.3786316446811473 - mutation_sigma: 0.011389807018409262\n",
      "Epoch 2006 - loss: 1.577515469377503 - val_loss: 1.3820262076058842 - mutation_sigma: 0.01136359246326727\n",
      "Epoch 2042 - loss: 1.576582822600041 - val_loss: 1.379375140348381 - mutation_sigma: 0.011304768857411673\n",
      "Epoch 2073 - loss: 1.5760448099181064 - val_loss: 1.3685220815235426 - mutation_sigma: 0.011254289964111572\n",
      "Epoch 2089 - loss: 1.5757596803440024 - val_loss: 1.3710288775113657 - mutation_sigma: 0.01122829905241151\n",
      "Epoch 2093 - loss: 1.5745429566747737 - val_loss: 1.3727699961498507 - mutation_sigma: 0.011221807963098556\n",
      "Epoch 2096 - loss: 1.5745146283496647 - val_loss: 1.367702533132055 - mutation_sigma: 0.011216941385143855\n",
      "Epoch 2110 - loss: 1.5738603999103171 - val_loss: 1.3763714163767315 - mutation_sigma: 0.011194250361430215\n",
      "Epoch 2138 - loss: 1.573180087814976 - val_loss: 1.3515296257120644 - mutation_sigma: 0.011148965168533927\n",
      "Epoch 2141 - loss: 1.5729154436153998 - val_loss: 1.3942633574091736 - mutation_sigma: 0.011144120816448212\n",
      "Epoch 2150 - loss: 1.57288294976354 - val_loss: 1.3664410720075442 - mutation_sigma: 0.011129596598404068\n",
      "Epoch 2153 - loss: 1.5723427867801285 - val_loss: 1.3602193086317906 - mutation_sigma: 0.011124758135389188\n",
      "Epoch 2154 - loss: 1.5707965243041837 - val_loss: 1.367916348048245 - mutation_sigma: 0.011123145641093454\n",
      "Epoch 2163 - loss: 1.570563230911838 - val_loss: 1.3512575740668005 - mutation_sigma: 0.011108640536995121\n",
      "Epoch 2187 - loss: 1.5704727135564183 - val_loss: 1.3651934374320638 - mutation_sigma: 0.01107002474972614\n",
      "Epoch 2197 - loss: 1.5696537633620546 - val_loss: 1.3614142895800385 - mutation_sigma: 0.01105396244980007\n",
      "Epoch 2222 - loss: 1.5694723349524686 - val_loss: 1.3619969428617993 - mutation_sigma: 0.011013877513890775\n",
      "Epoch 2225 - loss: 1.5691582326305606 - val_loss: 1.3518920660606217 - mutation_sigma: 0.011009074105362178\n",
      "Epoch 2238 - loss: 1.5679719856885452 - val_loss: 1.357517687390613 - mutation_sigma: 0.010988276096195737\n",
      "Epoch 2242 - loss: 1.5678387535322005 - val_loss: 1.3562706899495809 - mutation_sigma: 0.010981882182687065\n",
      "Epoch 2256 - loss: 1.567509202280568 - val_loss: 1.3665762104205987 - mutation_sigma: 0.010959523739347894\n",
      "Epoch 2269 - loss: 1.5673013392915758 - val_loss: 1.3415608126225949 - mutation_sigma: 0.010938790495563216\n",
      "Epoch 2286 - loss: 1.567221932423831 - val_loss: 1.3599917622127142 - mutation_sigma: 0.010911718633954338\n",
      "Epoch 2290 - loss: 1.5656952338544103 - val_loss: 1.3624315278317585 - mutation_sigma: 0.010905355500433228\n",
      "Epoch 2304 - loss: 1.565435453775313 - val_loss: 1.3472290376902827 - mutation_sigma: 0.01088310465002085\n",
      "Epoch 2324 - loss: 1.5649595238834075 - val_loss: 1.3739336176147945 - mutation_sigma: 0.010851371913827415\n",
      "Epoch 2347 - loss: 1.564618393989928 - val_loss: 1.3705626597647815 - mutation_sigma: 0.010814957893848613\n",
      "Epoch 2362 - loss: 1.5645828100656556 - val_loss: 1.3634595494291455 - mutation_sigma: 0.010791254826420068\n",
      "Epoch 2368 - loss: 1.5641625365543275 - val_loss: 1.361848630328406 - mutation_sigma: 0.01078178357461761\n",
      "Epoch 2371 - loss: 1.5621032173642004 - val_loss: 1.3724389771220895 - mutation_sigma: 0.010777050084203741\n",
      "Epoch 2409 - loss: 1.5617618107336222 - val_loss: 1.3591983412957955 - mutation_sigma: 0.010717215537741742\n",
      "Epoch 2410 - loss: 1.5615659570619882 - val_loss: 1.3410703818692735 - mutation_sigma: 0.01071564401833371\n",
      "Epoch 2415 - loss: 1.5614439455453963 - val_loss: 1.365460092874347 - mutation_sigma: 0.010707788781875411\n",
      "Epoch 2436 - loss: 1.5609246493925848 - val_loss: 1.3294379724925294 - mutation_sigma: 0.010674839707747782\n",
      "Epoch 2441 - loss: 1.5599133830263825 - val_loss: 1.3359290882803763 - mutation_sigma: 0.01066700489708344\n",
      "Epoch 2459 - loss: 1.5597270103048169 - val_loss: 1.3522647415014457 - mutation_sigma: 0.010638832031168152\n",
      "Epoch 2482 - loss: 1.5594848194698567 - val_loss: 1.350115356188767 - mutation_sigma: 0.010602907164614057\n",
      "Epoch 2499 - loss: 1.558473087764596 - val_loss: 1.3613298436081642 - mutation_sigma: 0.010576407105008127\n",
      "Epoch 2504 - loss: 1.5584244993394047 - val_loss: 1.3499486654276631 - mutation_sigma: 0.010568621546122214\n",
      "Epoch 2540 - loss: 1.5584002023043626 - val_loss: 1.3688439154932524 - mutation_sigma: 0.010512680364015863\n",
      "Epoch 2549 - loss: 1.557905464325448 - val_loss: 1.3701267708649914 - mutation_sigma: 0.010498726524578395\n",
      "Epoch 2555 - loss: 1.5569057246740912 - val_loss: 1.3310911695559118 - mutation_sigma: 0.010489430942726353\n",
      "Epoch 2580 - loss: 1.5563961744694315 - val_loss: 1.3563560728764141 - mutation_sigma: 0.01045075935456971\n",
      "Epoch 2595 - loss: 1.5552105820665192 - val_loss: 1.351794120345545 - mutation_sigma: 0.010427602781619645\n",
      "Epoch 2635 - loss: 1.5550931974759377 - val_loss: 1.3515422641632662 - mutation_sigma: 0.010366021512641689\n",
      "Epoch 2640 - loss: 1.554777994940189 - val_loss: 1.361606326160278 - mutation_sigma: 0.010358341163068226\n",
      "Epoch 2657 - loss: 1.5546114879558162 - val_loss: 1.3403047028027593 - mutation_sigma: 0.010332256685191925\n",
      "Epoch 2658 - loss: 1.5537020935593076 - val_loss: 1.343732385692427 - mutation_sigma: 0.010330723684856498\n",
      "Epoch 2660 - loss: 1.553456589492552 - val_loss: 1.3592594974186003 - mutation_sigma: 0.010327658144111532\n",
      "Epoch 2671 - loss: 1.5534287597153176 - val_loss: 1.349661232605175 - mutation_sigma: 0.01031080862637232\n",
      "Epoch 2674 - loss: 1.5526415955436637 - val_loss: 1.3544433209266433 - mutation_sigma: 0.01030621651955729\n",
      "Epoch 2689 - loss: 1.5524552672687926 - val_loss: 1.3418883766063174 - mutation_sigma: 0.010283276640770618\n",
      "Epoch 2694 - loss: 1.5512892988306577 - val_loss: 1.3433662090012952 - mutation_sigma: 0.010275637658659943\n",
      "Epoch 2719 - loss: 1.5511113930844524 - val_loss: 1.3268620191680884 - mutation_sigma: 0.010237499992084235\n",
      "Epoch 2725 - loss: 1.5509583686378243 - val_loss: 1.3755210492106762 - mutation_sigma: 0.010228361131366156\n",
      "Epoch 2737 - loss: 1.5508092854129407 - val_loss: 1.347663042607589 - mutation_sigma: 0.01021009985230208\n",
      "Epoch 2750 - loss: 1.5503684777176796 - val_loss: 1.3592550818887978 - mutation_sigma: 0.010190341513843893\n",
      "Epoch 2781 - loss: 1.5501830224513404 - val_loss: 1.3657874298516333 - mutation_sigma: 0.010143329002948929\n",
      "Epoch 2799 - loss: 1.5498719501433857 - val_loss: 1.3598796253800658 - mutation_sigma: 0.010116098220053972\n",
      "Epoch 2815 - loss: 1.5494832467618311 - val_loss: 1.3401901755194499 - mutation_sigma: 0.010091934193320922\n",
      "Epoch 2817 - loss: 1.5488820945679156 - val_loss: 1.3498621088106617 - mutation_sigma: 0.010088916407450417\n",
      "Epoch 2858 - loss: 1.5480410687756607 - val_loss: 1.3595238959602014 - mutation_sigma: 0.01002718461494382\n",
      "Epoch 2877 - loss: 1.5480200665634962 - val_loss: 1.3478229909913135 - mutation_sigma: 0.009998662904182477\n",
      "Epoch 2881 - loss: 1.5472726440601234 - val_loss: 1.361534530544385 - mutation_sigma: 0.009992665235261114\n",
      "Epoch 2905 - loss: 1.5470738747735755 - val_loss: 1.3472224616177744 - mutation_sigma: 0.00995672955545757\n",
      "Epoch 2907 - loss: 1.5467655286946798 - val_loss: 1.3587130611676121 - mutation_sigma: 0.009953738806441157\n",
      "Epoch 2909 - loss: 1.5465063883981667 - val_loss: 1.3431249188959222 - mutation_sigma: 0.009950748655475013\n",
      "Epoch 2910 - loss: 1.5454527136628116 - val_loss: 1.3560004980108187 - mutation_sigma: 0.009949253804223322\n",
      "Epoch 2954 - loss: 1.545050455142064 - val_loss: 1.3482194369955756 - mutation_sigma: 0.009883628109728513\n",
      "Epoch 2955 - loss: 1.5450065697787307 - val_loss: 1.3394859615901162 - mutation_sigma: 0.00988213996971066\n",
      "Epoch 2967 - loss: 1.5445940020658602 - val_loss: 1.3635868123305865 - mutation_sigma: 0.00986429389125289\n",
      "Epoch 2975 - loss: 1.5443913256851485 - val_loss: 1.3401448115996846 - mutation_sigma: 0.009852408396512139\n",
      "Epoch 2982 - loss: 1.5443310784465387 - val_loss: 1.3466216330643277 - mutation_sigma: 0.009842016384984657\n",
      "Epoch 2988 - loss: 1.5440519291121937 - val_loss: 1.322900116698893 - mutation_sigma: 0.009833114734063605\n",
      "Epoch 3000 - loss: 1.5433577749164704 - val_loss: 1.3412658900669032 - mutation_sigma: 0.009815327445863081\n",
      "Epoch 3006 - loss: 1.5430200438076243 - val_loss: 1.323385008232178 - mutation_sigma: 0.009806441802172699\n",
      "Epoch 3031 - loss: 1.5428267052781703 - val_loss: 1.3398620515160897 - mutation_sigma: 0.009769475614831793\n",
      "Epoch 3035 - loss: 1.5426259231716604 - val_loss: 1.3453130411301708 - mutation_sigma: 0.009763569595554331\n",
      "Epoch 3046 - loss: 1.5425326670773387 - val_loss: 1.3428476678750014 - mutation_sigma: 0.009747340217358274\n",
      "Epoch 3070 - loss: 1.5420186039470125 - val_loss: 1.3348140151035488 - mutation_sigma: 0.009711992565243015\n",
      "Epoch 3074 - loss: 1.5417683748236246 - val_loss: 1.3406302800142376 - mutation_sigma: 0.009706109532534635\n",
      "Epoch 3083 - loss: 1.5404130982398399 - val_loss: 1.3239773005881847 - mutation_sigma: 0.009692881308938663\n",
      "Epoch 3130 - loss: 1.540097868114943 - val_loss: 1.348944858849143 - mutation_sigma: 0.009623993662306263\n",
      "Epoch 3136 - loss: 1.5400052678056442 - val_loss: 1.3290895926669821 - mutation_sigma: 0.009615222774109821\n",
      "Epoch 3145 - loss: 1.538906056093952 - val_loss: 1.3341935443537016 - mutation_sigma: 0.009602076304186555\n",
      "Epoch 3172 - loss: 1.538825812803906 - val_loss: 1.3388292322709812 - mutation_sigma: 0.009562707804134481\n",
      "Epoch 3173 - loss: 1.5381828062771037 - val_loss: 1.3432432023593404 - mutation_sigma: 0.009561251751688132\n",
      "Epoch 3200 - loss: 1.5378941038523504 - val_loss: 1.3219851270094405 - mutation_sigma: 0.009521993318719196\n",
      "Epoch 3213 - loss: 1.5372762532117008 - val_loss: 1.336156176223909 - mutation_sigma: 0.009503128877648835\n",
      "Epoch 3230 - loss: 1.5366378782630943 - val_loss: 1.3269456203130479 - mutation_sigma: 0.009478496963944077\n",
      "Epoch 3231 - loss: 1.5364623991683408 - val_loss: 1.3141104503132552 - mutation_sigma: 0.009477049331346611\n",
      "Epoch 3234 - loss: 1.5358648069487544 - val_loss: 1.311986694021345 - mutation_sigma: 0.009472707301947856\n",
      "Epoch 3269 - loss: 1.534926668314523 - val_loss: 1.327199462063306 - mutation_sigma: 0.009422146414242338\n",
      "Epoch 3304 - loss: 1.5342528528081902 - val_loss: 1.30738835574831 - mutation_sigma: 0.009371762163027892\n",
      "Epoch 3317 - loss: 1.5335719680800164 - val_loss: 1.3228671685760371 - mutation_sigma: 0.009353092876486394\n",
      "Epoch 3358 - loss: 1.5331022066760174 - val_loss: 1.3102283529771568 - mutation_sigma: 0.009294371528017365\n",
      "Epoch 3368 - loss: 1.5325887709842072 - val_loss: 1.3123486864137948 - mutation_sigma: 0.00928008572899567\n",
      "Epoch 3391 - loss: 1.5322558143447993 - val_loss: 1.3170734681222287 - mutation_sigma: 0.009247282549922277\n",
      "Epoch 3427 - loss: 1.532170640374761 - val_loss: 1.3088773334041348 - mutation_sigma: 0.009196089654118833\n",
      "Epoch 3433 - loss: 1.5315528956576163 - val_loss: 1.3242612179778404 - mutation_sigma: 0.009187575406223551\n",
      "Epoch 3462 - loss: 1.5313908809471382 - val_loss: 1.31008886880172 - mutation_sigma: 0.009146495140503332\n",
      "Epoch 3472 - loss: 1.53067074969383 - val_loss: 1.315879124302905 - mutation_sigma: 0.009132357129276483\n",
      "Epoch 3484 - loss: 1.530370074402697 - val_loss: 1.3207790992797501 - mutation_sigma: 0.009115410165551213\n",
      "Epoch 3493 - loss: 1.5299584098680732 - val_loss: 1.3344207562235884 - mutation_sigma: 0.009102713280492055\n",
      "Epoch 3522 - loss: 1.5292479037526214 - val_loss: 1.3164782571912574 - mutation_sigma: 0.009061878733991568\n",
      "Epoch 3554 - loss: 1.5284014393563063 - val_loss: 1.3186999695547805 - mutation_sigma: 0.009016957126974694\n",
      "Epoch 3640 - loss: 1.5275594407380195 - val_loss: 1.3001249197408475 - mutation_sigma: 0.008896940109967208\n",
      "Epoch 3652 - loss: 1.5264058532547264 - val_loss: 1.311494474461461 - mutation_sigma: 0.008880275449077043\n",
      "Epoch 3758 - loss: 1.5255292733831547 - val_loss: 1.3051115737204175 - mutation_sigma: 0.008733936131599833\n",
      "Epoch 3806 - loss: 1.5247093601131163 - val_loss: 1.3175388457761998 - mutation_sigma: 0.008668177760312146\n",
      "Epoch 3835 - loss: 1.5239959636703462 - val_loss: 1.3126530712527273 - mutation_sigma: 0.00862860141586256\n",
      "Epoch 3857 - loss: 1.5239927181913642 - val_loss: 1.2997100577225424 - mutation_sigma: 0.008598654441187126\n",
      "Epoch 3869 - loss: 1.5237842412108817 - val_loss: 1.2940101053435522 - mutation_sigma: 0.008582347472688694\n",
      "Epoch 3878 - loss: 1.52356401259396 - val_loss: 1.3002191317782756 - mutation_sigma: 0.008570130080351176\n",
      "Epoch 3894 - loss: 1.5231685556638184 - val_loss: 1.3006588463072515 - mutation_sigma: 0.00854843740025802\n",
      "Epoch 3947 - loss: 1.5225105755410904 - val_loss: 1.30735245729511 - mutation_sigma: 0.008476827776102352\n",
      "Epoch 3963 - loss: 1.5221793421142755 - val_loss: 1.2992418905430654 - mutation_sigma: 0.008455284245431387\n",
      "Epoch 3973 - loss: 1.5220082132768606 - val_loss: 1.3042830884535657 - mutation_sigma: 0.00844183703063403\n",
      "Epoch 3974 - loss: 1.521854410358196 - val_loss: 1.296375638133106 - mutation_sigma: 0.008440493048529278\n",
      "Epoch 3987 - loss: 1.521676187138161 - val_loss: 1.299705876528199 - mutation_sigma: 0.0084230335046812\n",
      "Epoch 3996 - loss: 1.5214498907035865 - val_loss: 1.2958707280956148 - mutation_sigma: 0.008410959416089665\n",
      "Epoch 4002 - loss: 1.5210581242360743 - val_loss: 1.2997611027917175 - mutation_sigma: 0.008402916058023912\n",
      "Epoch 4036 - loss: 1.5191746451317272 - val_loss: 1.3068333212091443 - mutation_sigma: 0.008357428065625724\n",
      "Epoch 4099 - loss: 1.5191457619815516 - val_loss: 1.3028193793607739 - mutation_sigma: 0.008273549152677237\n",
      "Epoch 4114 - loss: 1.5185037806662043 - val_loss: 1.2980556150709521 - mutation_sigma: 0.008253655742076232\n",
      "Epoch 4117 - loss: 1.5181972254280898 - val_loss: 1.3017513804688978 - mutation_sigma: 0.00824968063915903\n",
      "Epoch 4134 - loss: 1.5174934102837228 - val_loss: 1.302223129966856 - mutation_sigma: 0.008227177565410845\n",
      "Epoch 4151 - loss: 1.516925945664128 - val_loss: 1.2971435214958875 - mutation_sigma: 0.008204712710570945\n",
      "Epoch 4175 - loss: 1.516832342710414 - val_loss: 1.2929701543790513 - mutation_sigma: 0.008173062560450632\n",
      "Epoch 4189 - loss: 1.5165436894027138 - val_loss: 1.2991930550444182 - mutation_sigma: 0.008154635017909237\n",
      "Epoch 4196 - loss: 1.5164900718418224 - val_loss: 1.2994433031208916 - mutation_sigma: 0.008145430915618375\n",
      "Epoch 4204 - loss: 1.51579698542343 - val_loss: 1.2988034783434448 - mutation_sigma: 0.00813491982699057\n",
      "Epoch 4213 - loss: 1.5156848448057927 - val_loss: 1.3011599745485305 - mutation_sigma: 0.008123104898154397\n",
      "Epoch 4228 - loss: 1.5153456642810426 - val_loss: 1.3013792678330507 - mutation_sigma: 0.008103436962239807\n",
      "Epoch 4263 - loss: 1.5152437421558547 - val_loss: 1.3080832720925255 - mutation_sigma: 0.008057659667592875\n",
      "Epoch 4268 - loss: 1.515242670143588 - val_loss: 1.3092840476949146 - mutation_sigma: 0.008051133122185868\n",
      "Epoch 4283 - loss: 1.5148942393023568 - val_loss: 1.3065510102969256 - mutation_sigma: 0.00803157305222903\n",
      "Epoch 4286 - loss: 1.5145850236623843 - val_loss: 1.299028914466481 - mutation_sigma: 0.008027664557466334\n",
      "Epoch 4295 - loss: 1.5139961060932385 - val_loss: 1.2957420932572017 - mutation_sigma: 0.008015946105304133\n",
      "Epoch 4315 - loss: 1.513891937956392 - val_loss: 1.298137156801345 - mutation_sigma: 0.007989942825368173\n",
      "Epoch 4316 - loss: 1.5134471512593723 - val_loss: 1.2928967481100222 - mutation_sigma: 0.007988644025906627\n",
      "Epoch 4330 - loss: 1.513382280229381 - val_loss: 1.300556329033807 - mutation_sigma: 0.007970474462888066\n",
      "Epoch 4335 - loss: 1.5133642616806178 - val_loss: 1.2916160586917733 - mutation_sigma: 0.007963991494830455\n",
      "Epoch 4347 - loss: 1.512580323942462 - val_loss: 1.3089504556825093 - mutation_sigma: 0.00794844558903596\n",
      "Epoch 4369 - loss: 1.5125366500400963 - val_loss: 1.2931841025641357 - mutation_sigma: 0.007919993163125711\n",
      "Epoch 4371 - loss: 1.5125217931843176 - val_loss: 1.3008091703251827 - mutation_sigma: 0.00791740968119807\n",
      "Epoch 4376 - loss: 1.5121335510237215 - val_loss: 1.3015006564554483 - mutation_sigma: 0.007910953236247673\n",
      "Epoch 4378 - loss: 1.5119658458277694 - val_loss: 1.2900086225192033 - mutation_sigma: 0.007908371561943881\n",
      "Epoch 4380 - loss: 1.5115192720413546 - val_loss: 1.2980950111816867 - mutation_sigma: 0.0079057904038717\n",
      "Epoch 4398 - loss: 1.5114496678468028 - val_loss: 1.297034011870862 - mutation_sigma: 0.007882583194619222\n",
      "Epoch 4409 - loss: 1.511186739605909 - val_loss: 1.2988388980998415 - mutation_sigma: 0.007868421559595686\n",
      "Epoch 4434 - loss: 1.5108297096039132 - val_loss: 1.2962030832760063 - mutation_sigma: 0.007836293894775602\n",
      "Epoch 4448 - loss: 1.5107704102709076 - val_loss: 1.297934057752117 - mutation_sigma: 0.00781833745141016\n",
      "Epoch 4449 - loss: 1.5104334021193058 - val_loss: 1.2977668228632968 - mutation_sigma: 0.007817055809912307\n",
      "Epoch 4456 - loss: 1.5102090745586767 - val_loss: 1.291476997404381 - mutation_sigma: 0.007808087906767919\n",
      "Epoch 4466 - loss: 1.5100613526521611 - val_loss: 1.291202396889054 - mutation_sigma: 0.007795287500171677\n",
      "Epoch 4479 - loss: 1.5099036950776097 - val_loss: 1.2945810632929693 - mutation_sigma: 0.007778666094816055\n",
      "Epoch 4486 - loss: 1.509853642010719 - val_loss: 1.2913891725156106 - mutation_sigma: 0.0077697250523838185\n",
      "Epoch 4512 - loss: 1.5098363393918206 - val_loss: 1.2962152874291264 - mutation_sigma: 0.007736570202712476\n",
      "Epoch 4525 - loss: 1.5090542040130326 - val_loss: 1.2947575803667841 - mutation_sigma: 0.007720025072626368\n",
      "Epoch 4551 - loss: 1.508909006649633 - val_loss: 1.2791477621236889 - mutation_sigma: 0.007686999262174814\n",
      "Epoch 4552 - loss: 1.508865963767768 - val_loss: 1.3106510429499445 - mutation_sigma: 0.007685730752526098\n",
      "Epoch 4555 - loss: 1.5081931893822755 - val_loss: 1.3006985727156593 - mutation_sigma: 0.007681925984520871\n",
      "Epoch 4570 - loss: 1.5081687463813689 - val_loss: 1.2845910762198764 - mutation_sigma: 0.007662919254827716\n",
      "Epoch 4572 - loss: 1.5080982097189148 - val_loss: 1.283732370635203 - mutation_sigma: 0.007660387177400676\n",
      "Epoch 4575 - loss: 1.5080871613938927 - val_loss: 1.283296489881322 - mutation_sigma: 0.007656590010567634\n",
      "Epoch 4580 - loss: 1.5073057507144547 - val_loss: 1.2893112008414345 - mutation_sigma: 0.0076502639298223875\n",
      "Epoch 4586 - loss: 1.5072098196960784 - val_loss: 1.2926466911640748 - mutation_sigma: 0.007642676806541391\n",
      "Epoch 4591 - loss: 1.507003036716309 - val_loss: 1.2851665934640484 - mutation_sigma: 0.0076363576799640545\n",
      "Epoch 4630 - loss: 1.5068094991537544 - val_loss: 1.2919428799833415 - mutation_sigma: 0.007587176768211814\n",
      "Epoch 4658 - loss: 1.5066375899154258 - val_loss: 1.2879122577230846 - mutation_sigma: 0.007551985483177694\n",
      "Epoch 4703 - loss: 1.5062359777994059 - val_loss: 1.2872495781753723 - mutation_sigma: 0.007495634069409199\n",
      "Epoch 4707 - loss: 1.5061095174006922 - val_loss: 1.3037762680506721 - mutation_sigma: 0.007490637314874412\n",
      "Epoch 4717 - loss: 1.5059598561485006 - val_loss: 1.282721320635646 - mutation_sigma: 0.007478154168487524\n",
      "Epoch 4732 - loss: 1.5055297222377355 - val_loss: 1.2911719620383522 - mutation_sigma: 0.00745945283687281\n",
      "Epoch 4733 - loss: 1.505272576247615 - val_loss: 1.2840301306034452 - mutation_sigma: 0.007458207078453923\n",
      "Epoch 4779 - loss: 1.5051810914713823 - val_loss: 1.2940151466980343 - mutation_sigma: 0.007401036635755931\n",
      "Epoch 4788 - loss: 1.5051765567366318 - val_loss: 1.2988164881251318 - mutation_sigma: 0.007389881838675017\n",
      "Epoch 4792 - loss: 1.5046960061511951 - val_loss: 1.279217641354839 - mutation_sigma: 0.0073849273723454985\n",
      "Epoch 4806 - loss: 1.5044983289245635 - val_loss: 1.2885104590934886 - mutation_sigma: 0.007367602336882057\n",
      "Epoch 4821 - loss: 1.5044722248690192 - val_loss: 1.2928468016675196 - mutation_sigma: 0.00734906669214984\n",
      "Epoch 4822 - loss: 1.5042224924780383 - val_loss: 1.2927753868040095 - mutation_sigma: 0.0073478319706898705\n",
      "Epoch 4836 - loss: 1.5041046576164734 - val_loss: 1.289013941158037 - mutation_sigma: 0.007330558827266641\n",
      "Epoch 4874 - loss: 1.5040781901023623 - val_loss: 1.2814930029061262 - mutation_sigma: 0.007283796285072698\n",
      "Epoch 4885 - loss: 1.5033684040972342 - val_loss: 1.284231924016209 - mutation_sigma: 0.0072702928877288455\n",
      "Epoch 4902 - loss: 1.5032476521506928 - val_loss: 1.2854602429585003 - mutation_sigma: 0.007249453192548516\n",
      "Epoch 4910 - loss: 1.5030883292275052 - val_loss: 1.27869750305631 - mutation_sigma: 0.007239658527849157\n",
      "Epoch 4919 - loss: 1.5030496280049621 - val_loss: 1.2692527206152497 - mutation_sigma: 0.007228648891217317\n",
      "Epoch 4933 - loss: 1.5027202802082984 - val_loss: 1.2821519235393695 - mutation_sigma: 0.007211542470699796\n",
      "Epoch 4942 - loss: 1.502581005147861 - val_loss: 1.2835676685294335 - mutation_sigma: 0.007200558124607883\n",
      "Epoch 4946 - loss: 1.5022849527070081 - val_loss: 1.287627649924723 - mutation_sigma: 0.007195679365050952\n",
      "Epoch 4970 - loss: 1.5018120713155578 - val_loss: 1.2861409120403433 - mutation_sigma: 0.007166447749704549\n",
      "Epoch 4987 - loss: 1.501795179042218 - val_loss: 1.2785904462949464 - mutation_sigma: 0.007145784423665708\n",
      "Epoch 4993 - loss: 1.5016102550300652 - val_loss: 1.272660181326898 - mutation_sigma: 0.007138499867052668\n",
      "Epoch 4996 - loss: 1.5015418648169507 - val_loss: 1.2872474208768367 - mutation_sigma: 0.007134859227279799\n",
      "Epoch 5000 - loss: 1.501196309814991 - val_loss: 1.2741443882820158 - mutation_sigma: 0.007130006739399943\n",
      "Epoch 5018 - loss: 1.5010228723180163 - val_loss: 1.2807667033862433 - mutation_sigma: 0.007108194545351775\n",
      "Epoch 5032 - loss: 1.5008514154413137 - val_loss: 1.2888859887381652 - mutation_sigma: 0.00709125662608994\n",
      "Epoch 5035 - loss: 1.500805393211421 - val_loss: 1.282649837005486 - mutation_sigma: 0.007087630155746895\n",
      "Epoch 5042 - loss: 1.5004736876061073 - val_loss: 1.2755180428946449 - mutation_sigma: 0.007079172620873881\n",
      "Epoch 5056 - loss: 1.5002979797580764 - val_loss: 1.2650779099077214 - mutation_sigma: 0.007062275299821118\n",
      "Epoch 5062 - loss: 1.5002314722179597 - val_loss: 1.2840488332606865 - mutation_sigma: 0.007055040828646706\n",
      "Epoch 5066 - loss: 1.500181513270051 - val_loss: 1.2888854086609605 - mutation_sigma: 0.007050220258550546\n",
      "Epoch 5074 - loss: 1.4997184080840422 - val_loss: 1.2836411383062234 - mutation_sigma: 0.007040584900536546\n",
      "Epoch 5099 - loss: 1.4996359988055088 - val_loss: 1.2816148271062549 - mutation_sigma: 0.0070105240361073165\n",
      "Epoch 5103 - loss: 1.499210111974627 - val_loss: 1.2775283562938307 - mutation_sigma: 0.007005721267387519\n",
      "Epoch 5144 - loss: 1.4991932124319747 - val_loss: 1.2820290405083903 - mutation_sigma: 0.006956603482228253\n",
      "Epoch 5149 - loss: 1.4991253460005922 - val_loss: 1.2783070079731527 - mutation_sigma: 0.006950627272285113\n",
      "Epoch 5164 - loss: 1.4988076971381181 - val_loss: 1.278216310349523 - mutation_sigma: 0.006932716558841034\n",
      "Epoch 5185 - loss: 1.4986866700297619 - val_loss: 1.2780612374841887 - mutation_sigma: 0.006907686647662896\n",
      "Epoch 5198 - loss: 1.49866103200409 - val_loss: 1.2776518049522625 - mutation_sigma: 0.00689221825849106\n",
      "Epoch 5204 - loss: 1.4978695892354414 - val_loss: 1.2732750112198776 - mutation_sigma: 0.006885085780741017\n",
      "Epoch 5240 - loss: 1.4963890926472199 - val_loss: 1.2848576137746726 - mutation_sigma: 0.006842380657780885\n",
      "Epoch 5326 - loss: 1.4963776081194793 - val_loss: 1.2816482023105216 - mutation_sigma: 0.006740982958883678\n",
      "Epoch 5363 - loss: 1.4963152584364467 - val_loss: 1.270320744097884 - mutation_sigma: 0.006697625917624462\n",
      "Epoch 5379 - loss: 1.4960588767470848 - val_loss: 1.2771688202634066 - mutation_sigma: 0.006678926549576307\n",
      "Epoch 5394 - loss: 1.4957058536568655 - val_loss: 1.2751744289441 - mutation_sigma: 0.006661423041015797\n",
      "Epoch 5428 - loss: 1.4951561142458676 - val_loss: 1.2661294500836662 - mutation_sigma: 0.006621845480408002\n",
      "Epoch 5464 - loss: 1.4948015477764829 - val_loss: 1.2749284131882912 - mutation_sigma: 0.0065800862243605\n",
      "Epoch 5479 - loss: 1.4946285288325023 - val_loss: 1.279334029268152 - mutation_sigma: 0.006562730850345679\n",
      "Epoch 5507 - loss: 1.494430116780131 - val_loss: 1.2682605216090237 - mutation_sigma: 0.006530403715784834\n",
      "Epoch 5544 - loss: 1.493798591259605 - val_loss: 1.2697328656494486 - mutation_sigma: 0.006487824300467447\n",
      "Epoch 5557 - loss: 1.4935385478205103 - val_loss: 1.2699178887176656 - mutation_sigma: 0.006472901323211222\n",
      "Epoch 5621 - loss: 1.4933719354027788 - val_loss: 1.2640891657950608 - mutation_sigma: 0.006399716514390779\n",
      "Epoch 5624 - loss: 1.4929995791489263 - val_loss: 1.269367257165318 - mutation_sigma: 0.00639629745422714\n",
      "Epoch 5640 - loss: 1.49264926926856 - val_loss: 1.272248423281841 - mutation_sigma: 0.006378079778094947\n",
      "Epoch 5653 - loss: 1.4925149745024322 - val_loss: 1.2747594368764266 - mutation_sigma: 0.006363299361777027\n",
      "Epoch 5690 - loss: 1.4922584750343144 - val_loss: 1.2583256557327602 - mutation_sigma: 0.006321337028555464\n",
      "Epoch 5714 - loss: 1.492099675261437 - val_loss: 1.2580157619848262 - mutation_sigma: 0.006294201109405766\n",
      "Epoch 5728 - loss: 1.491785141519088 - val_loss: 1.2674778902012185 - mutation_sigma: 0.006278401869824546\n",
      "Epoch 5732 - loss: 1.4910684524268492 - val_loss: 1.2647893731592394 - mutation_sigma: 0.006273891862139049\n",
      "Epoch 5761 - loss: 1.4910105110474055 - val_loss: 1.2689457535274016 - mutation_sigma: 0.006241248196296381\n",
      "Epoch 5797 - loss: 1.4905906844103365 - val_loss: 1.2669432934568952 - mutation_sigma: 0.006200856490649114\n",
      "Epoch 5826 - loss: 1.490150129460951 - val_loss: 1.2624765774103095 - mutation_sigma: 0.006168424299449876\n",
      "Epoch 5850 - loss: 1.4896779277602046 - val_loss: 1.2698350594619634 - mutation_sigma: 0.0061416548942060415\n",
      "Epoch 5852 - loss: 1.4892567590114896 - val_loss: 1.2604117810143074 - mutation_sigma: 0.006139427008811705\n",
      "Epoch 5916 - loss: 1.4892539094450652 - val_loss: 1.2586303125075269 - mutation_sigma: 0.006068369408555014\n",
      "Epoch 5924 - loss: 1.4886303669430194 - val_loss: 1.2665352701115844 - mutation_sigma: 0.006059519138635097\n",
      "Epoch 5957 - loss: 1.4884333129445895 - val_loss: 1.2605915464216064 - mutation_sigma: 0.006023086515627159\n",
      "Epoch 6008 - loss: 1.488175158814361 - val_loss: 1.2613678438614138 - mutation_sigma: 0.00596701747886232\n",
      "Epoch 6014 - loss: 1.4878357362992205 - val_loss: 1.2723715205952864 - mutation_sigma: 0.005960439899604013\n",
      "Epoch 6046 - loss: 1.487445856613044 - val_loss: 1.2614896548875967 - mutation_sigma: 0.0059254260453530045\n",
      "Epoch 6072 - loss: 1.4871824525430277 - val_loss: 1.2585186494829452 - mutation_sigma: 0.0058970596665435466\n",
      "Epoch 6119 - loss: 1.4870384906102312 - val_loss: 1.2653351049541768 - mutation_sigma: 0.005845968752889661\n",
      "Epoch 6122 - loss: 1.4866209625693334 - val_loss: 1.269241198699485 - mutation_sigma: 0.00584271577553253\n",
      "Epoch 6129 - loss: 1.4863231644062105 - val_loss: 1.2596555841405306 - mutation_sigma: 0.005835129288718475\n",
      "Epoch 6208 - loss: 1.4862681235275805 - val_loss: 1.2600821612854556 - mutation_sigma: 0.005749877480496548\n",
      "Epoch 6212 - loss: 1.4858231298994389 - val_loss: 1.2487751311731254 - mutation_sigma: 0.0057455788191600595\n",
      "Epoch 6227 - loss: 1.4857939030442637 - val_loss: 1.260741010983042 - mutation_sigma: 0.005729474142925443\n",
      "Epoch 6231 - loss: 1.485700347236984 - val_loss: 1.258045821535246 - mutation_sigma: 0.005725183640476211\n",
      "Epoch 6236 - loss: 1.4853606720077355 - val_loss: 1.2632172973582798 - mutation_sigma: 0.005719822925018052\n",
      "Epoch 6246 - loss: 1.4853265225842114 - val_loss: 1.2463038172729555 - mutation_sigma: 0.005709109531022125\n",
      "Epoch 6279 - loss: 1.4849563947221607 - val_loss: 1.251250988017374 - mutation_sigma: 0.005673831238593346\n",
      "Epoch 6301 - loss: 1.4847156374814678 - val_loss: 1.2529241836838019 - mutation_sigma: 0.005650376964456348\n",
      "Epoch 6303 - loss: 1.484602711506318 - val_loss: 1.26206999620929 - mutation_sigma: 0.005648247315000447\n",
      "Epoch 6329 - loss: 1.4836579387296518 - val_loss: 1.2635487999718862 - mutation_sigma: 0.005620600592965058\n",
      "Epoch 6444 - loss: 1.4836364680146477 - val_loss: 1.260835673968235 - mutation_sigma: 0.005499175361818644\n",
      "Epoch 6464 - loss: 1.4836304354184013 - val_loss: 1.2526582590968127 - mutation_sigma: 0.005478200090884092\n",
      "Epoch 6469 - loss: 1.4835500192437772 - val_loss: 1.257906615968178 - mutation_sigma: 0.0054729628239911605\n",
      "Epoch 6470 - loss: 1.483420783892503 - val_loss: 1.2486516631486249 - mutation_sigma: 0.005471915684780517\n",
      "Epoch 6476 - loss: 1.4830867700858181 - val_loss: 1.2587798549765263 - mutation_sigma: 0.005465635047812836\n",
      "Epoch 6494 - loss: 1.4824283058535783 - val_loss: 1.2566467790591527 - mutation_sigma: 0.005446815729126167\n",
      "Epoch 6498 - loss: 1.481898698297152 - val_loss: 1.2433329851032071 - mutation_sigma: 0.005442638256132109\n",
      "Epoch 6574 - loss: 1.4818860502537496 - val_loss: 1.258945044880992 - mutation_sigma: 0.005363582901748956\n",
      "Epoch 6581 - loss: 1.481024053973441 - val_loss: 1.261563480733174 - mutation_sigma: 0.0053563316570738985\n",
      "Epoch 6617 - loss: 1.4805951337109031 - val_loss: 1.2486610050887748 - mutation_sigma: 0.0053191196061901\n",
      "Epoch 6639 - loss: 1.4805276660743802 - val_loss: 1.251613473068183 - mutation_sigma: 0.005296444762014099\n",
      "Epoch 6648 - loss: 1.4804723027478688 - val_loss: 1.2587288926814182 - mutation_sigma: 0.005287183056291536\n",
      "Epoch 6649 - loss: 1.4804493522436153 - val_loss: 1.2537419613640182 - mutation_sigma: 0.005286154492271366\n",
      "Epoch 6661 - loss: 1.4802916687581793 - val_loss: 1.2520058984867453 - mutation_sigma: 0.005273819742685373\n",
      "Epoch 6662 - loss: 1.4799181874441314 - val_loss: 1.2640662323200567 - mutation_sigma: 0.0052727925147961425\n",
      "Epoch 6685 - loss: 1.4796055131022696 - val_loss: 1.256177703631446 - mutation_sigma: 0.005249194599807554\n",
      "Epoch 6691 - loss: 1.4793681605337479 - val_loss: 1.2480119550927955 - mutation_sigma: 0.0052430475420551645\n",
      "Epoch 6712 - loss: 1.4791538103001154 - val_loss: 1.2502823462779713 - mutation_sigma: 0.005221561858647966\n",
      "Epoch 6721 - loss: 1.4790388081150136 - val_loss: 1.252721201942059 - mutation_sigma: 0.005212367510487225\n",
      "Epoch 6733 - loss: 1.4788853306142171 - val_loss: 1.253124309550036 - mutation_sigma: 0.005200121243331058\n",
      "Epoch 6741 - loss: 1.4787264883093063 - val_loss: 1.2561613256220774 - mutation_sigma: 0.00519196522478061\n",
      "Epoch 6755 - loss: 1.4785719078017279 - val_loss: 1.2612470775100282 - mutation_sigma: 0.005177707881668874\n",
      "Epoch 6772 - loss: 1.478507365428562 - val_loss: 1.2558384197469021 - mutation_sigma: 0.005160422203826538\n",
      "Epoch 6776 - loss: 1.4783453623983085 - val_loss: 1.2587747613496276 - mutation_sigma: 0.005156359253884157\n",
      "Epoch 6793 - loss: 1.4777691601558105 - val_loss: 1.2515109952121755 - mutation_sigma: 0.005139109834255023\n",
      "Epoch 6799 - loss: 1.4774220401812757 - val_loss: 1.2529132655603898 - mutation_sigma: 0.005133028800950204\n",
      "Epoch 6806 - loss: 1.477393455444324 - val_loss: 1.2546628404535598 - mutation_sigma: 0.005125938871547247\n",
      "Epoch 6820 - loss: 1.4772582684591433 - val_loss: 1.2530169327698737 - mutation_sigma: 0.00511177389142456\n",
      "Epoch 6861 - loss: 1.4772452914543055 - val_loss: 1.2519273598541971 - mutation_sigma: 0.005070404620348177\n",
      "Epoch 6870 - loss: 1.476566884116717 - val_loss: 1.2580959341587945 - mutation_sigma: 0.005061346238911034\n",
      "Epoch 6879 - loss: 1.4765278437749945 - val_loss: 1.2469955094802863 - mutation_sigma: 0.005052296005535199\n",
      "Epoch 6900 - loss: 1.4762581917929547 - val_loss: 1.2508448739772706 - mutation_sigma: 0.0050312104400720295\n",
      "Epoch 6906 - loss: 1.4760136035899813 - val_loss: 1.255754199173026 - mutation_sigma: 0.005025194120516279\n",
      "Epoch 6942 - loss: 1.475897724151448 - val_loss: 1.2479906104542358 - mutation_sigma: 0.0049891719027961865\n",
      "Epoch 6953 - loss: 1.4755585865193164 - val_loss: 1.2464479041635375 - mutation_sigma: 0.004978190952427985\n",
      "Epoch 6966 - loss: 1.475542004887329 - val_loss: 1.25208948970155 - mutation_sigma: 0.004965229027458843\n",
      "Epoch 6979 - loss: 1.4749340147127583 - val_loss: 1.2509937388389687 - mutation_sigma: 0.004952283940361387\n",
      "Epoch 6987 - loss: 1.474810243339331 - val_loss: 1.252249847160468 - mutation_sigma: 0.0049443260925573764\n",
      "Epoch 7001 - loss: 1.4747904452618468 - val_loss: 1.2498538756569153 - mutation_sigma: 0.0049304151670401\n",
      "Epoch 7009 - loss: 1.4746790273675023 - val_loss: 1.2522287384931081 - mutation_sigma: 0.004922474805510676\n",
      "Epoch 7022 - loss: 1.4746596570312098 - val_loss: 1.2517007725990041 - mutation_sigma: 0.004909585257239813\n",
      "Epoch 7027 - loss: 1.4744489408597985 - val_loss: 1.2450102022109362 - mutation_sigma: 0.004904632198285005\n",
      "Epoch 7036 - loss: 1.474135546497918 - val_loss: 1.2440803177658082 - mutation_sigma: 0.004895722930005417\n",
      "Epoch 7045 - loss: 1.4739501815813503 - val_loss: 1.2552955616646286 - mutation_sigma: 0.004886821675659078\n",
      "Epoch 7078 - loss: 1.4732441404240832 - val_loss: 1.2512445244730777 - mutation_sigma: 0.00485425219028266\n",
      "Epoch 7110 - loss: 1.4730569285852226 - val_loss: 1.2485917116822665 - mutation_sigma: 0.004822772126242919\n",
      "Epoch 7145 - loss: 1.473011429557416 - val_loss: 1.2579883340760598 - mutation_sigma: 0.004788455943766245\n",
      "Epoch 7146 - loss: 1.4728304204645564 - val_loss: 1.25760577426447 - mutation_sigma: 0.004787477244977501\n",
      "Epoch 7153 - loss: 1.4723994395857125 - val_loss: 1.2554518258617713 - mutation_sigma: 0.0047806290928540775\n",
      "Epoch 7174 - loss: 1.471834765224373 - val_loss: 1.2597405159056454 - mutation_sigma: 0.0047601133723764065\n",
      "Epoch 7227 - loss: 1.4716252183717609 - val_loss: 1.2472808269525761 - mutation_sigma: 0.00470852675544292\n",
      "Epoch 7245 - loss: 1.4716196686039873 - val_loss: 1.2554037343053444 - mutation_sigma: 0.004691068869881842\n",
      "Epoch 7268 - loss: 1.4712020374582064 - val_loss: 1.2509412012947103 - mutation_sigma: 0.004668807248320747\n",
      "Epoch 7301 - loss: 1.4709313252132767 - val_loss: 1.2410590067473883 - mutation_sigma: 0.004636955953070132\n",
      "Epoch 7328 - loss: 1.4708253355308665 - val_loss: 1.253088441603009 - mutation_sigma: 0.004610973861811709\n",
      "Epoch 7340 - loss: 1.4705562322161618 - val_loss: 1.2522812315754674 - mutation_sigma: 0.004599448762130185\n",
      "Epoch 7363 - loss: 1.4703409421190634 - val_loss: 1.2513505827851208 - mutation_sigma: 0.004577397603645303\n",
      "Epoch 7384 - loss: 1.4701714339620668 - val_loss: 1.2497721730341005 - mutation_sigma: 0.004557308178898232\n",
      "Epoch 7392 - loss: 1.4700819264758138 - val_loss: 1.2469529518791795 - mutation_sigma: 0.004549666153775211\n",
      "Epoch 7402 - loss: 1.4699783981711365 - val_loss: 1.2583510141877021 - mutation_sigma: 0.004540122214780081\n",
      "Epoch 7417 - loss: 1.4698037522961365 - val_loss: 1.2498754597115527 - mutation_sigma: 0.004525824187461743\n",
      "Epoch 7435 - loss: 1.469692970440176 - val_loss: 1.2475151193820921 - mutation_sigma: 0.0045086948378983355\n",
      "Epoch 7459 - loss: 1.4691006332949657 - val_loss: 1.246224181053335 - mutation_sigma: 0.004485903609822573\n",
      "Epoch 7520 - loss: 1.4688863615391285 - val_loss: 1.2534558039634793 - mutation_sigma: 0.004428221475370868\n",
      "Epoch 7564 - loss: 1.468872976179937 - val_loss: 1.2476277335735766 - mutation_sigma: 0.004386832562147066\n",
      "Epoch 7568 - loss: 1.4687588422580797 - val_loss: 1.2545361720414765 - mutation_sigma: 0.004383078955254318\n",
      "Epoch 7574 - loss: 1.468133299642567 - val_loss: 1.2614612762095343 - mutation_sigma: 0.004377451359088443\n",
      "Epoch 7603 - loss: 1.4679581585549961 - val_loss: 1.2474882852590425 - mutation_sigma: 0.004350298855555036\n",
      "Epoch 7623 - loss: 1.467947996988755 - val_loss: 1.2528160864659634 - mutation_sigma: 0.004331618812117767\n",
      "Epoch 7629 - loss: 1.4679248068875588 - val_loss: 1.2517365842950756 - mutation_sigma: 0.004326022079691352\n",
      "Epoch 7630 - loss: 1.4678523994802348 - val_loss: 1.2573363414758487 - mutation_sigma: 0.00432508961735351\n",
      "Epoch 7646 - loss: 1.467682448396118 - val_loss: 1.2468634935448566 - mutation_sigma: 0.004310182893197268\n",
      "Epoch 7674 - loss: 1.4675579369546259 - val_loss: 1.2446245732871677 - mutation_sigma: 0.004284153442276902\n",
      "Epoch 7680 - loss: 1.4674347852429976 - val_loss: 1.252471575987131 - mutation_sigma: 0.004278585177684404\n",
      "Epoch 7687 - loss: 1.4672036610634374 - val_loss: 1.2543171873118528 - mutation_sigma: 0.004272093089764656\n",
      "Epoch 7702 - loss: 1.4671553116908884 - val_loss: 1.24648648639911 - mutation_sigma: 0.00425819676461169\n",
      "Epoch 7723 - loss: 1.467129310689095 - val_loss: 1.261389174137212 - mutation_sigma: 0.004238776891396292\n",
      "Epoch 7737 - loss: 1.466883807078504 - val_loss: 1.2561188454965015 - mutation_sigma: 0.004225852945016141\n",
      "Epoch 7741 - loss: 1.4664519290312104 - val_loss: 1.2651734882835672 - mutation_sigma: 0.004222163710657627\n",
      "Epoch 7743 - loss: 1.466313558623826 - val_loss: 1.2634702893001273 - mutation_sigma: 0.004220319646734426\n",
      "Epoch 7782 - loss: 1.466163400832144 - val_loss: 1.2584104029094432 - mutation_sigma: 0.004184434011145602\n",
      "Epoch 7783 - loss: 1.4661404773355657 - val_loss: 1.2513381662751446 - mutation_sigma: 0.004183515705491101\n",
      "Epoch 7794 - loss: 1.4659437370906003 - val_loss: 1.2528410294660242 - mutation_sigma: 0.004173420401180705\n",
      "Epoch 7800 - loss: 1.465788812236337 - val_loss: 1.2581776614181504 - mutation_sigma: 0.004167918549845569\n",
      "Epoch 7835 - loss: 1.4657330707664877 - val_loss: 1.2447019472860672 - mutation_sigma: 0.004135890120208351\n",
      "Epoch 7842 - loss: 1.465649986051515 - val_loss: 1.2494411707784152 - mutation_sigma: 0.004129497873896037\n",
      "Epoch 7847 - loss: 1.4652955113324515 - val_loss: 1.2590800729503262 - mutation_sigma: 0.0041249347221572535\n",
      "Epoch 7915 - loss: 1.4651427809355935 - val_loss: 1.2555351870583125 - mutation_sigma: 0.004063101819444856\n",
      "Epoch 7917 - loss: 1.4650241192070674 - val_loss: 1.2647280693654568 - mutation_sigma: 0.0040612895615385865\n",
      "Epoch 7922 - loss: 1.4649550358344645 - val_loss: 1.255494715228838 - mutation_sigma: 0.004056760502022995\n",
      "Epoch 7957 - loss: 1.4648262171730269 - val_loss: 1.2583684387454683 - mutation_sigma: 0.004025120406738219\n",
      "Epoch 7982 - loss: 1.4648062753472508 - val_loss: 1.2514673304184338 - mutation_sigma: 0.004002588036156094\n",
      "Epoch 7998 - loss: 1.4647050668314021 - val_loss: 1.2549650421967309 - mutation_sigma: 0.003988196850435205\n",
      "Epoch 8034 - loss: 1.4643003096735945 - val_loss: 1.253537106696657 - mutation_sigma: 0.003955900739261174\n",
      "Epoch 8075 - loss: 1.464110919289541 - val_loss: 1.253048073055666 - mutation_sigma: 0.00391926037433669\n",
      "Epoch 8086 - loss: 1.4639537622732774 - val_loss: 1.2598536125214284 - mutation_sigma: 0.003909455562041464\n",
      "Epoch 8146 - loss: 1.4638198116276793 - val_loss: 1.2650890488440683 - mutation_sigma: 0.0038561641917750694\n",
      "Epoch 8147 - loss: 1.4636230733872584 - val_loss: 1.2564549960438967 - mutation_sigma: 0.003855278708179169\n",
      "Epoch 8195 - loss: 1.463360705857805 - val_loss: 1.2576881403662683 - mutation_sigma: 0.003812879449923953\n",
      "Epoch 8231 - loss: 1.4631709042913092 - val_loss: 1.2554555407903514 - mutation_sigma: 0.0037812132838158643\n",
      "Epoch 8239 - loss: 1.4629975177485792 - val_loss: 1.2541451514665782 - mutation_sigma: 0.003774191824293013\n",
      "Epoch 8241 - loss: 1.4629230038059422 - val_loss: 1.251319983733881 - mutation_sigma: 0.003772437336831494\n",
      "Epoch 8269 - loss: 1.4628878721957725 - val_loss: 1.2613605580832368 - mutation_sigma: 0.0037479113173396654\n",
      "Epoch 8274 - loss: 1.4627207241402977 - val_loss: 1.2625045689318743 - mutation_sigma: 0.003743538892120905\n",
      "Epoch 8287 - loss: 1.4626757492863611 - val_loss: 1.262062442048671 - mutation_sigma: 0.003732180811720884\n",
      "Epoch 8292 - loss: 1.4626334837914976 - val_loss: 1.2535757917883998 - mutation_sigma: 0.003727816249002894\n",
      "Epoch 8295 - loss: 1.4625674877297405 - val_loss: 1.2549694114232786 - mutation_sigma: 0.003725198558570433\n",
      "Epoch 8305 - loss: 1.4624909212710144 - val_loss: 1.2654322832620826 - mutation_sigma: 0.0037164785927179646\n",
      "Epoch 8310 - loss: 1.4624181318965113 - val_loss: 1.251057914987765 - mutation_sigma: 0.003712121878362386\n",
      "Epoch 8318 - loss: 1.4622769939963787 - val_loss: 1.263714510076754 - mutation_sigma: 0.0037051556643382033\n",
      "Epoch 8326 - loss: 1.461962034764587 - val_loss: 1.2543889258277594 - mutation_sigma: 0.003698195020499849\n",
      "Epoch 8332 - loss: 1.4617689630703596 - val_loss: 1.257360597481568 - mutation_sigma: 0.003692978190376087\n",
      "Epoch 8363 - loss: 1.4614424908132608 - val_loss: 1.26110824899517 - mutation_sigma: 0.003666074370831488\n",
      "Epoch 8416 - loss: 1.4613618455529513 - val_loss: 1.255437960790094 - mutation_sigma: 0.0036202702452205636\n",
      "Epoch 8420 - loss: 1.4612360407905334 - val_loss: 1.2574396882051802 - mutation_sigma: 0.003616823171290611\n",
      "Epoch 8446 - loss: 1.4611014923392858 - val_loss: 1.263246147625337 - mutation_sigma: 0.0035944507650159012\n",
      "Epoch 8451 - loss: 1.4608956586069621 - val_loss: 1.2628769475233879 - mutation_sigma: 0.003590155043225505\n",
      "Epoch 8468 - loss: 1.460786441585939 - val_loss: 1.2543145749248623 - mutation_sigma: 0.003575565643095407\n",
      "Epoch 8471 - loss: 1.460784375457062 - val_loss: 1.2628216197695135 - mutation_sigma: 0.0035729936164284335\n",
      "Epoch 8482 - loss: 1.4606356060439114 - val_loss: 1.2568861993822331 - mutation_sigma: 0.0035635694501086465\n",
      "Epoch 8485 - loss: 1.460473288836986 - val_loss: 1.256553023909183 - mutation_sigma: 0.0035610010214000504\n",
      "Epoch 8523 - loss: 1.4597686133321053 - val_loss: 1.2604638884836 - mutation_sigma: 0.003528534190245544\n",
      "Epoch 8597 - loss: 1.4595628839504549 - val_loss: 1.2578171619989145 - mutation_sigma: 0.003465662237534756\n",
      "Epoch 8617 - loss: 1.4593026407952505 - val_loss: 1.2550412798064081 - mutation_sigma: 0.0034487495226830306\n",
      "Epoch 8664 - loss: 1.459150597626344 - val_loss: 1.2701255935435656 - mutation_sigma: 0.003409137522239282\n",
      "Epoch 8675 - loss: 1.4589249376410123 - val_loss: 1.2604641486319077 - mutation_sigma: 0.003399893480523947\n",
      "Epoch 8739 - loss: 1.4588769338087142 - val_loss: 1.2685340394265088 - mutation_sigma: 0.0033463111667598995\n",
      "Epoch 8751 - loss: 1.4587895654913483 - val_loss: 1.2715613274401047 - mutation_sigma: 0.0033363026005570383\n",
      "Epoch 8755 - loss: 1.4586348364550552 - val_loss: 1.2635105899666006 - mutation_sigma: 0.003332969079617539\n",
      "Epoch 8766 - loss: 1.458634395645247 - val_loss: 1.2652708926066274 - mutation_sigma: 0.0033238087687555923\n",
      "Epoch 8767 - loss: 1.458436147691491 - val_loss: 1.2644273790478642 - mutation_sigma: 0.003322976512717817\n",
      "Epoch 8780 - loss: 1.4582444244912005 - val_loss: 1.2636566072510855 - mutation_sigma: 0.003312164753593131\n",
      "Epoch 8789 - loss: 1.4581793171817947 - val_loss: 1.2692673499059652 - mutation_sigma: 0.0033046879180790656\n",
      "Epoch 8805 - loss: 1.4581101746095337 - val_loss: 1.2653724406949298 - mutation_sigma: 0.0032914123682368313\n",
      "Epoch 8815 - loss: 1.4577698830830006 - val_loss: 1.2722279808491792 - mutation_sigma: 0.00328312592842295\n",
      "Epoch 8866 - loss: 1.4576665177266865 - val_loss: 1.2702538349198818 - mutation_sigma: 0.003240993727831934\n",
      "Epoch 8886 - loss: 1.4575769809241927 - val_loss: 1.2710669751270398 - mutation_sigma: 0.0032245298561225577\n",
      "Epoch 8903 - loss: 1.4575057884686498 - val_loss: 1.267888809558722 - mutation_sigma: 0.003210561428736893\n",
      "Epoch 8911 - loss: 1.4571647380645267 - val_loss: 1.2584435215294614 - mutation_sigma: 0.003203996262527116\n",
      "Epoch 8913 - loss: 1.4571498168087997 - val_loss: 1.264199585214308 - mutation_sigma: 0.0032023557913743077\n",
      "Epoch 8959 - loss: 1.4570923734343184 - val_loss: 1.2702732182368892 - mutation_sigma: 0.0031647153581397186\n",
      "Epoch 8960 - loss: 1.4570628823680194 - val_loss: 1.2623917435674663 - mutation_sigma: 0.0031638990090569483\n",
      "Epoch 8967 - loss: 1.4568156021496332 - val_loss: 1.2654909554615272 - mutation_sigma: 0.0031581868504552022\n",
      "Epoch 8968 - loss: 1.456792840726397 - val_loss: 1.2663642932412804 - mutation_sigma: 0.0031573711541252854\n",
      "Epoch 8969 - loss: 1.4566764297722343 - val_loss: 1.266043009509765 - mutation_sigma: 0.0031565555393527683\n",
      "Epoch 8982 - loss: 1.4565965076614016 - val_loss: 1.2665410653048788 - mutation_sigma: 0.0031459599653241915\n",
      "Epoch 9028 - loss: 1.4564703412707258 - val_loss: 1.268318679933858 - mutation_sigma: 0.003108578331314991\n",
      "Epoch 9029 - loss: 1.4564370117354823 - val_loss: 1.2624758863371273 - mutation_sigma: 0.003107767595092968\n",
      "Epoch 9032 - loss: 1.4563137476554258 - val_loss: 1.26638830329814 - mutation_sigma: 0.003105335872763259\n",
      "Epoch 9040 - loss: 1.456264753273992 - val_loss: 1.2660882854796478 - mutation_sigma: 0.0030988548449245876\n",
      "Epoch 9047 - loss: 1.456239126157423 - val_loss: 1.2695099649943973 - mutation_sigma: 0.003093188196756182\n",
      "Epoch 9050 - loss: 1.4561306990521736 - val_loss: 1.2661148098300141 - mutation_sigma: 0.0030907608471527582\n",
      "Epoch 9074 - loss: 1.4561131595664096 - val_loss: 1.2707518589054394 - mutation_sigma: 0.003071368240815349\n",
      "Epoch 9093 - loss: 1.456060055486124 - val_loss: 1.2675780759378823 - mutation_sigma: 0.0030560487312510003\n",
      "Epoch 9116 - loss: 1.4560491285342714 - val_loss: 1.262877951795979 - mutation_sigma: 0.003037542959540905\n",
      "Epoch 9121 - loss: 1.4558628984973614 - val_loss: 1.2708817387617404 - mutation_sigma: 0.003033525594222683\n",
      "Epoch 9135 - loss: 1.4557025126834362 - val_loss: 1.2639891302106097 - mutation_sigma: 0.003022287650581033\n",
      "Epoch 9151 - loss: 1.455696244203747 - val_loss: 1.2681584635387573 - mutation_sigma: 0.0030094635347798623\n",
      "Epoch 9152 - loss: 1.4554625051938184 - val_loss: 1.265658513050937 - mutation_sigma: 0.003008662708550985\n",
      "Epoch 9179 - loss: 1.4553856150162825 - val_loss: 1.2636686170224654 - mutation_sigma: 0.0029870706408519293\n",
      "Epoch 9204 - loss: 1.4551976937266937 - val_loss: 1.2688425605260731 - mutation_sigma: 0.002967129894641947\n",
      "Epoch 9208 - loss: 1.455145576625207 - val_loss: 1.2624926666736218 - mutation_sigma: 0.002963943998495409\n",
      "Epoch 9230 - loss: 1.4551136922574601 - val_loss: 1.2664822291154763 - mutation_sigma: 0.0029464443283603757\n",
      "Epoch 9290 - loss: 1.4550568795078338 - val_loss: 1.2648854554020628 - mutation_sigma: 0.002898913151617304\n",
      "Epoch 9305 - loss: 1.4549139391732506 - val_loss: 1.2663315745111146 - mutation_sigma: 0.002887074846668545\n",
      "Epoch 9329 - loss: 1.4548198768945908 - val_loss: 1.2630288891621357 - mutation_sigma: 0.0028681704518230414\n",
      "Epoch 9356 - loss: 1.4547995524460728 - val_loss: 1.2711661686125897 - mutation_sigma: 0.0028469571637565966\n",
      "Epoch 9374 - loss: 1.454795727773736 - val_loss: 1.2683058162771927 - mutation_sigma: 0.0028328467550799404\n",
      "Epoch 9375 - loss: 1.454735741275274 - val_loss: 1.2694165450877055 - mutation_sigma: 0.0028320635878801665\n",
      "Epoch 9380 - loss: 1.4544278804179336 - val_loss: 1.265497346051344 - mutation_sigma: 0.002828148926299327\n",
      "Epoch 9387 - loss: 1.4543494987239627 - val_loss: 1.2658442633093188 - mutation_sigma: 0.002822671687032219\n",
      "Epoch 9408 - loss: 1.4543197196895525 - val_loss: 1.2717973300905865 - mutation_sigma: 0.0028062629525636967\n",
      "Epoch 9413 - loss: 1.454183515658343 - val_loss: 1.2663926463830726 - mutation_sigma: 0.002802361186786715\n",
      "Epoch 9430 - loss: 1.4541101416693973 - val_loss: 1.2687014621415975 - mutation_sigma: 0.0027891097648111186\n",
      "Epoch 9445 - loss: 1.454020165106509 - val_loss: 1.268792541666699 - mutation_sigma: 0.0027774360250313237\n",
      "Epoch 9457 - loss: 1.4539430334199008 - val_loss: 1.2692012496195786 - mutation_sigma: 0.0027681096313958856\n",
      "Epoch 9464 - loss: 1.453889785432081 - val_loss: 1.2696584952953127 - mutation_sigma: 0.0027626744007296245\n",
      "Epoch 9474 - loss: 1.4538612683863967 - val_loss: 1.2647808090706443 - mutation_sigma: 0.0027549163817866974\n",
      "Epoch 9483 - loss: 1.4537946505220345 - val_loss: 1.2728088896265768 - mutation_sigma: 0.0027479407940871293\n",
      "Epoch 9498 - loss: 1.453638491516716 - val_loss: 1.2690075089166453 - mutation_sigma: 0.0027363287553060014\n",
      "Epoch 9516 - loss: 1.4536367488421338 - val_loss: 1.2653649315859419 - mutation_sigma: 0.002722417278778702\n",
      "Epoch 9518 - loss: 1.4535731701502939 - val_loss: 1.2711649954116406 - mutation_sigma: 0.0027208731041630154\n",
      "Epoch 9525 - loss: 1.453560379686139 - val_loss: 1.2721493533093593 - mutation_sigma: 0.0027154709241916557\n",
      "Epoch 9532 - loss: 1.4535215898139684 - val_loss: 1.2692088579119827 - mutation_sigma: 0.0027100725240452005\n",
      "Epoch 9535 - loss: 1.4534718355458487 - val_loss: 1.2721641921642515 - mutation_sigma: 0.0027077600804162353\n",
      "Epoch 9547 - loss: 1.4531288000010685 - val_loss: 1.2717793985945198 - mutation_sigma: 0.002698517239417465\n",
      "Epoch 9560 - loss: 1.4530633454568072 - val_loss: 1.2690739863258393 - mutation_sigma: 0.002688516668842427\n",
      "Epoch 9584 - loss: 1.4530311912567486 - val_loss: 1.2723404687630064 - mutation_sigma: 0.002670088194698374\n",
      "Epoch 9599 - loss: 1.452986853306886 - val_loss: 1.275115580842589 - mutation_sigma: 0.00265859283561671\n",
      "Epoch 9621 - loss: 1.4528552380729127 - val_loss: 1.2719348550852536 - mutation_sigma: 0.00264176413260886\n",
      "Epoch 9626 - loss: 1.4528316994547925 - val_loss: 1.2702006313593912 - mutation_sigma: 0.0026379445874629226\n",
      "Epoch 9635 - loss: 1.452626291390787 - val_loss: 1.2697462459988293 - mutation_sigma: 0.0026310742165019385\n",
      "Epoch 9643 - loss: 1.4525688421102223 - val_loss: 1.2711750318000816 - mutation_sigma: 0.0026249724083582063\n",
      "Epoch 9650 - loss: 1.4525315394216352 - val_loss: 1.278398661009897 - mutation_sigma: 0.002619637328676039\n",
      "Epoch 9657 - loss: 1.4525051055304044 - val_loss: 1.268936612953278 - mutation_sigma: 0.0026143059818697035\n",
      "Epoch 9661 - loss: 1.4524693110398887 - val_loss: 1.276963145078171 - mutation_sigma: 0.002611261172960222\n",
      "Epoch 9663 - loss: 1.4521947292271966 - val_loss: 1.2747154983938978 - mutation_sigma: 0.002609739225120271\n",
      "Epoch 9684 - loss: 1.4521408818855681 - val_loss: 1.2743153403221372 - mutation_sigma: 0.0025937771350166188\n",
      "Epoch 9691 - loss: 1.4520722220636855 - val_loss: 1.2713402589894685 - mutation_sigma: 0.002588463882202809\n",
      "Epoch 9695 - loss: 1.451916880378828 - val_loss: 1.277870701014077 - mutation_sigma: 0.0025854294070329353\n",
      "Epoch 9709 - loss: 1.4514417440124276 - val_loss: 1.2723826073724942 - mutation_sigma: 0.0025748182964845023\n",
      "Epoch 9731 - loss: 1.4512366465574498 - val_loss: 1.281829526352631 - mutation_sigma: 0.0025581736764888887\n",
      "Epoch 9754 - loss: 1.4511688660984376 - val_loss: 1.2714488281596066 - mutation_sigma: 0.002540811587297476\n",
      "Epoch 9775 - loss: 1.4511263318887089 - val_loss: 1.2770543739240285 - mutation_sigma: 0.0025249940789111485\n",
      "Epoch 9777 - loss: 1.4508894670822658 - val_loss: 1.276399249485962 - mutation_sigma: 0.002523489381039955\n",
      "Epoch 9792 - loss: 1.450860113400768 - val_loss: 1.282934797788122 - mutation_sigma: 0.0025122137333828754\n",
      "Epoch 9794 - loss: 1.4507366626289273 - val_loss: 1.27720391020462 - mutation_sigma: 0.0025107115910696686\n",
      "Epoch 9809 - loss: 1.4505422873404072 - val_loss: 1.276292338598044 - mutation_sigma: 0.0024994550938161122\n",
      "Epoch 9841 - loss: 1.4505238133129412 - val_loss: 1.2790597463317757 - mutation_sigma: 0.0024754975857213543\n",
      "Epoch 9844 - loss: 1.4504537174713945 - val_loss: 1.2774130081334838 - mutation_sigma: 0.0024732554969846355\n",
      "Epoch 9850 - loss: 1.4503898098514183 - val_loss: 1.279990096777203 - mutation_sigma: 0.0024687733366850163\n",
      "Epoch 9858 - loss: 1.4503855458445598 - val_loss: 1.283909078951417 - mutation_sigma: 0.002462801304350312\n",
      "Epoch 9865 - loss: 1.4502811209476891 - val_loss: 1.279557120942712 - mutation_sigma: 0.0024575796933754\n",
      "Epoch 9868 - loss: 1.450256437214313 - val_loss: 1.2816302787216334 - mutation_sigma: 0.0024553429786628393\n",
      "Epoch 9881 - loss: 1.4501817465951718 - val_loss: 1.2789899016972939 - mutation_sigma: 0.0024456582976651155\n",
      "Epoch 9888 - loss: 1.4501614588232328 - val_loss: 1.2805288502801235 - mutation_sigma: 0.0024404486813967754\n",
      "Epoch 9891 - loss: 1.4499512504887841 - val_loss: 1.2796660118010696 - mutation_sigma: 0.0024382171047032638\n",
      "Epoch 9892 - loss: 1.4498012334795032 - val_loss: 1.2791961718127542 - mutation_sigma: 0.002437473394549937\n",
      "Epoch 9909 - loss: 1.4496909518838945 - val_loss: 1.2794263077807257 - mutation_sigma: 0.0024248416929375646\n",
      "Epoch 9932 - loss: 1.4496832851507189 - val_loss: 1.2792257441736319 - mutation_sigma: 0.0024077858843225934\n",
      "Epoch 9934 - loss: 1.4495025273581161 - val_loss: 1.2684052565150536 - mutation_sigma: 0.00240630462340285\n",
      "Epoch 9949 - loss: 1.4494926188391657 - val_loss: 1.278805500526476 - mutation_sigma: 0.002395204603565653\n",
      "Epoch 9957 - loss: 1.4494556609470288 - val_loss: 1.2684694323316266 - mutation_sigma: 0.0023892913968015213\n",
      "Epoch 9976 - loss: 1.4493934997651632 - val_loss: 1.274127130198042 - mutation_sigma: 0.0023752664735357696\n",
      "Epoch 9979 - loss: 1.4493162475286983 - val_loss: 1.2789465455128135 - mutation_sigma: 0.0023730544466170275\n",
      "Epoch 9980 - loss: 1.449230757877797 - val_loss: 1.280669558146814 - mutation_sigma: 0.0023723172517522107\n",
      "Epoch 9998 - loss: 1.4490757463063113 - val_loss: 1.2851728639742785 - mutation_sigma: 0.0023590603411886265\n"
     ]
    }
   ],
   "source": [
    "regressor = EvoMLPRegressor3(n = 480 // 2, hidden_layers = [16], activation = \"relu\", random_state = 42)\n",
    "regressor.fit(scaled_X_train, y_train, epochs = 10000, validation_data = (scaled_X_val, y_val), verbose = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "lr_target = 0.002\n",
    "lr_initial_decay = 20\n",
    "lr_final_decay = 0.02\n",
    "epochs = 10000\n",
    "\n",
    "mutation_sigma_new = lambda epoch: math.exp(-epoch / (epochs / (lr_initial_decay * math.log10(epochs + 1)))) + lr_final_decay * math.exp(-(epoch + 1) * (1 / (epochs))) + lr_target + (-0.035 * 10 * lr_final_decay)\n",
    "mutation_sigma_old = lambda epoch: math.exp(-epoch / (epochs / (20 * math.log10(epochs + 1)))) + 0.02 * math.exp(-(epoch + 1) * (1 / (epochs + 1))) - 0.005\n",
    "\n",
    "new = list(range(epochs))\n",
    "old = list(range(epochs))\n",
    "\n",
    "new = [mutation_sigma_new(i) for i in new]\n",
    "old = [mutation_sigma_old(i) for i in old]\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.plot(new[700:], label = \"new\")\n",
    "ax.plot(old[700:], label = \"old\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "can't multiply sequence by non-int of type 'float'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb Cell 13'\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000013?line=0'>1</a>\u001b[0m mutation_sigma_new(\u001b[39m1\u001b[39;49m)\n",
      "\u001b[1;32m/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb Cell 12'\u001b[0m in \u001b[0;36m<lambda>\u001b[0;34m(epoch)\u001b[0m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000012?line=2'>3</a>\u001b[0m lr_final_decay \u001b[39m=\u001b[39m \u001b[39m0.02\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000012?line=3'>4</a>\u001b[0m epochs \u001b[39m=\u001b[39m \u001b[39m10000\u001b[39m\n\u001b[0;32m----> <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000012?line=5'>6</a>\u001b[0m mutation_sigma_new \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m epoch: math\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mepoch \u001b[39m/\u001b[39m (epochs \u001b[39m/\u001b[39m (lr_initial_decay \u001b[39m*\u001b[39;49m math\u001b[39m.\u001b[39;49mlog10(epochs \u001b[39m+\u001b[39;49m \u001b[39m1\u001b[39;49m)))) \u001b[39m+\u001b[39m lr_final_decay \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (epochs))) \u001b[39m+\u001b[39m lr_target \u001b[39m+\u001b[39m (\u001b[39m-\u001b[39m\u001b[39m0.036\u001b[39m \u001b[39m*\u001b[39m \u001b[39m10\u001b[39m \u001b[39m*\u001b[39m lr_final_decay)\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000012?line=6'>7</a>\u001b[0m mutation_sigma_old \u001b[39m=\u001b[39m \u001b[39mlambda\u001b[39;00m epoch: math\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39mepoch \u001b[39m/\u001b[39m (epochs \u001b[39m/\u001b[39m (\u001b[39m20\u001b[39m \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mlog10(epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m)))) \u001b[39m+\u001b[39m \u001b[39m0.02\u001b[39m \u001b[39m*\u001b[39m math\u001b[39m.\u001b[39mexp(\u001b[39m-\u001b[39m(epoch \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m) \u001b[39m*\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m/\u001b[39m (epochs \u001b[39m+\u001b[39m \u001b[39m1\u001b[39m))) \u001b[39m-\u001b[39m \u001b[39m0.005\u001b[39m\n\u001b[1;32m      <a href='vscode-notebook-cell:/Users/dp/Documents/GitHub/EvoMLP/evo_regressor_test.ipynb#ch0000012?line=8'>9</a>\u001b[0m new \u001b[39m=\u001b[39m \u001b[39mlist\u001b[39m(\u001b[39mrange\u001b[39m(epochs))\n",
      "\u001b[0;31mTypeError\u001b[0m: can't multiply sequence by non-int of type 'float'"
     ]
    }
   ],
   "source": [
    "mutation_sigma_new(1)"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "1a5361d98b3513d72cb4620654a80a0bb40d1ea53adf5703cbac5d5174886d6d"
  },
  "kernelspec": {
   "display_name": "Python 3.9.12 ('EvoMLP-1oh6nhp_')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
